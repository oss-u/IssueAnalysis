[
  {
    "url": "https://api.github.com/repos/jupyter/notebook/issues/comments/682679378",
    "html_url": "https://github.com/jupyter/notebook/issues/5705#issuecomment-682679378",
    "issue_url": "https://api.github.com/repos/jupyter/notebook/issues/5705",
    "id": 682679378,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY4MjY3OTM3OA==",
    "user": {
      "login": "Zsailer",
      "id": 2791223,
      "node_id": "MDQ6VXNlcjI3OTEyMjM=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2791223?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Zsailer",
      "html_url": "https://github.com/Zsailer",
      "followers_url": "https://api.github.com/users/Zsailer/followers",
      "following_url": "https://api.github.com/users/Zsailer/following{/other_user}",
      "gists_url": "https://api.github.com/users/Zsailer/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Zsailer/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Zsailer/subscriptions",
      "organizations_url": "https://api.github.com/users/Zsailer/orgs",
      "repos_url": "https://api.github.com/users/Zsailer/repos",
      "events_url": "https://api.github.com/users/Zsailer/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Zsailer/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-08-28T15:07:01Z",
    "updated_at": "2020-08-28T15:07:01Z",
    "author_association": "MEMBER",
    "body": "Have you looked at using [the `\"chunk\"` field](https://jupyter-notebook.readthedocs.io/en/stable/extending/contents.html#chunked-saving) in the model you're passing to the contents API? \r\n\r\nThis allows you to break up a large file into chunks and save each chunk one at a time. That way, you don't have to load the entire file first and send one large JSON blog to the server. ",
    "performed_via_github_app": null
  }
]
