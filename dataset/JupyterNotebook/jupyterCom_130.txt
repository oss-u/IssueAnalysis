[
  {
    "url": "https://api.github.com/repos/jupyter/notebook/issues/comments/108472195",
    "html_url": "https://github.com/jupyter/notebook/issues/130#issuecomment-108472195",
    "issue_url": "https://api.github.com/repos/jupyter/notebook/issues/130",
    "id": 108472195,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwODQ3MjE5NQ==",
    "user": {
      "login": "Carreau",
      "id": 335567,
      "node_id": "MDQ6VXNlcjMzNTU2Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/335567?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Carreau",
      "html_url": "https://github.com/Carreau",
      "followers_url": "https://api.github.com/users/Carreau/followers",
      "following_url": "https://api.github.com/users/Carreau/following{/other_user}",
      "gists_url": "https://api.github.com/users/Carreau/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Carreau/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Carreau/subscriptions",
      "organizations_url": "https://api.github.com/users/Carreau/orgs",
      "repos_url": "https://api.github.com/users/Carreau/repos",
      "events_url": "https://api.github.com/users/Carreau/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Carreau/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-06-03T15:03:25Z",
    "updated_at": "2015-06-03T15:03:25Z",
    "author_association": "MEMBER",
    "body": "That would require to open/read/parse each file in a directory which we don't want to do. \nTry-jupyter is \"just\" the notebook app, we probably won't add features that don't apply to local server.\n",
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/jupyter/notebook/issues/comments/108476845",
    "html_url": "https://github.com/jupyter/notebook/issues/130#issuecomment-108476845",
    "issue_url": "https://api.github.com/repos/jupyter/notebook/issues/130",
    "id": 108476845,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwODQ3Njg0NQ==",
    "user": {
      "login": "minad",
      "id": 50754,
      "node_id": "MDQ6VXNlcjUwNzU0",
      "avatar_url": "https://avatars.githubusercontent.com/u/50754?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minad",
      "html_url": "https://github.com/minad",
      "followers_url": "https://api.github.com/users/minad/followers",
      "following_url": "https://api.github.com/users/minad/following{/other_user}",
      "gists_url": "https://api.github.com/users/minad/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minad/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minad/subscriptions",
      "organizations_url": "https://api.github.com/users/minad/orgs",
      "repos_url": "https://api.github.com/users/minad/repos",
      "events_url": "https://api.github.com/users/minad/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minad/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-06-03T15:09:09Z",
    "updated_at": "2015-06-03T15:09:09Z",
    "author_association": "NONE",
    "body": "Well, it needs some effort. But I think it could also be done in a performant way using ajax and by only parsing the first <n> bytes of the ipynb file, which is not a problem unless you have like thousands of ipynb files in a directory, which nobody should have ;)\n\nI would also like to have it for the local server/containers and not only for try.jupyter.org.\n",
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/jupyter/notebook/issues/comments/108477390",
    "html_url": "https://github.com/jupyter/notebook/issues/130#issuecomment-108477390",
    "issue_url": "https://api.github.com/repos/jupyter/notebook/issues/130",
    "id": 108477390,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwODQ3NzM5MA==",
    "user": {
      "login": "minad",
      "id": 50754,
      "node_id": "MDQ6VXNlcjUwNzU0",
      "avatar_url": "https://avatars.githubusercontent.com/u/50754?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minad",
      "html_url": "https://github.com/minad",
      "followers_url": "https://api.github.com/users/minad/followers",
      "following_url": "https://api.github.com/users/minad/following{/other_user}",
      "gists_url": "https://api.github.com/users/minad/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minad/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minad/subscriptions",
      "organizations_url": "https://api.github.com/users/minad/orgs",
      "repos_url": "https://api.github.com/users/minad/repos",
      "events_url": "https://api.github.com/users/minad/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minad/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-06-03T15:10:24Z",
    "updated_at": "2015-06-03T15:10:24Z",
    "author_association": "NONE",
    "body": "I think for like a few hundred files you wouldn't even notice if the information is loaded with all the other data. Also you could cache the data in the server.\n",
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/jupyter/notebook/issues/comments/108486249",
    "html_url": "https://github.com/jupyter/notebook/issues/130#issuecomment-108486249",
    "issue_url": "https://api.github.com/repos/jupyter/notebook/issues/130",
    "id": 108486249,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwODQ4NjI0OQ==",
    "user": {
      "login": "Carreau",
      "id": 335567,
      "node_id": "MDQ6VXNlcjMzNTU2Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/335567?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Carreau",
      "html_url": "https://github.com/Carreau",
      "followers_url": "https://api.github.com/users/Carreau/followers",
      "following_url": "https://api.github.com/users/Carreau/following{/other_user}",
      "gists_url": "https://api.github.com/users/Carreau/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Carreau/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Carreau/subscriptions",
      "organizations_url": "https://api.github.com/users/Carreau/orgs",
      "repos_url": "https://api.github.com/users/Carreau/repos",
      "events_url": "https://api.github.com/users/Carreau/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Carreau/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-06-03T15:33:48Z",
    "updated_at": "2015-06-03T15:33:48Z",
    "author_association": "MEMBER",
    "body": "1) Problem is many people do run notebooks on NFS filesystems, so you would notice the difference. People already have issues. \n\n2) The data ordering in json is not guarantied, the bytes would be the last bytes of the fileformat curently, unless people add keys in which case the metadata could be somewhere else. \nIt also break the storage layer as we don't assume things are on disk. \n\nCache is not a solution,  people do use other application to modify `.ipynb` and you don't get notified when the file change. \n\nWe did speak about things like that for several hours, like in person at our dev meeting. \nThere is no reasonable solution as far as we can tell, or the complexity and requirement are probably not worth the gain. \n",
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/jupyter/notebook/issues/comments/108494078",
    "html_url": "https://github.com/jupyter/notebook/issues/130#issuecomment-108494078",
    "issue_url": "https://api.github.com/repos/jupyter/notebook/issues/130",
    "id": 108494078,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwODQ5NDA3OA==",
    "user": {
      "login": "minad",
      "id": 50754,
      "node_id": "MDQ6VXNlcjUwNzU0",
      "avatar_url": "https://avatars.githubusercontent.com/u/50754?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minad",
      "html_url": "https://github.com/minad",
      "followers_url": "https://api.github.com/users/minad/followers",
      "following_url": "https://api.github.com/users/minad/following{/other_user}",
      "gists_url": "https://api.github.com/users/minad/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minad/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minad/subscriptions",
      "organizations_url": "https://api.github.com/users/minad/orgs",
      "repos_url": "https://api.github.com/users/minad/repos",
      "events_url": "https://api.github.com/users/minad/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minad/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-06-03T15:51:06Z",
    "updated_at": "2015-06-03T15:51:06Z",
    "author_association": "NONE",
    "body": "Maybe low-priority-background thread or caching+mtime-checking could be the way to handle this. I mean this is not a critical thing it wouldn't be a problem if it loads slowly or shows obsolete information from time to time.\n\nThe question about complexity vs. gain is always the question :)\n",
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/jupyter/notebook/issues/comments/108558101",
    "html_url": "https://github.com/jupyter/notebook/issues/130#issuecomment-108558101",
    "issue_url": "https://api.github.com/repos/jupyter/notebook/issues/130",
    "id": 108558101,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwODU1ODEwMQ==",
    "user": {
      "login": "takluyver",
      "id": 327925,
      "node_id": "MDQ6VXNlcjMyNzkyNQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/327925?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/takluyver",
      "html_url": "https://github.com/takluyver",
      "followers_url": "https://api.github.com/users/takluyver/followers",
      "following_url": "https://api.github.com/users/takluyver/following{/other_user}",
      "gists_url": "https://api.github.com/users/takluyver/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/takluyver/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/takluyver/subscriptions",
      "organizations_url": "https://api.github.com/users/takluyver/orgs",
      "repos_url": "https://api.github.com/users/takluyver/repos",
      "events_url": "https://api.github.com/users/takluyver/events{/privacy}",
      "received_events_url": "https://api.github.com/users/takluyver/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-06-03T18:21:50Z",
    "updated_at": "2015-06-03T18:21:50Z",
    "author_association": "MEMBER",
    "body": "For now we want to steer clear of anything putting anything from the files' contents in the directory listing.\n\nIt's too slow to do synchronously - I just clocked reading+parsing 10 fairly small ipynb files at 0.5s, and even just opening and reading 500 bytes from each with no parsing at 0.3s. It definitely wouldn't be unreasonable to have several dozen notebooks in one directory, and listings should work into the hundreds of notebooks, whether or not we think it's a good idea to arrange things like that.\n\nAs you say, it's possible to do this with smart caching and background threads or processes to update the cache. And we might want to do that at some point. But that sounds like significant extra complexity, and I think there are other things we want to focus on for now.\n",
    "performed_via_github_app": null
  }
]
