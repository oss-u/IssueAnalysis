This discussion bring some insights about adding a multivariate imputer in scikit learn. Because of release time constraint, the development was moved into a specific branch FIXME: give a specific name see 11600. Decide good default for the IterativeImputer ChaindedImputer 11350. Modify the example for imputation to show a compelling example 11350. Add an example to illustrate how to make multiple imputation 11370. Add a meta estimator which does multiple imputations? From the discussion in 8478, we have to deal with the following issues in CODESCODES: We have the following things to do: Determine the most appropriate way to use individual imputation samples in predictive modelling, clustering, etc, which are Scikit learn's focus. is using a single draw acceptable? is averaging over multiple draws from the final fit appropriate? is ensembling multiple predictive estimators each trained on a different imputation most appropriate? Perhaps determine if, in a predictive modelling context, it is necessary to have the sophistication of MICE in sampling each imputation value rather than just using point predictions. Provide an example illustrating the inferential capabilities due to multiple imputation. I don't think there's anything limiting about our current interface, but it deserves an example. Rename MICEImputer to de emphasise multiple imputation because it only performs a single one at a time. Minor things: The documentation refer to CODESCODES instead of CODESCODES. CODESCODES should be improved length of the list mainly. glemaitre can you clarify what you mean by your point about CODESCODES? Currently the documentation is not explicit regarding the size of the list. I find that it should be CODESCODES. Ah, thanks! One comment about the to do list. I don't think this is necessary to do: Perhaps determine if, in a predictive modelling context, it is necessary to have the sophistication of MICE in sampling each imputation value rather than just using point predictions. I think stats people already know that they want to understand uncertainty due to missing values. Why do we need to if it is necessary? The answer will inevitably be sometimes and depend on what dataset is chosen to run some basic experiments on. I'll try to carve some time out next week to tackle at least some of these, but it would be great to have someone else contribute as it's a busy work season for me. I am willing to help, but I don't totally get what's been decided on the n imputations and m? We keep as it is for n imputations. For m, it will be an example to show how to make inference.  Sent from my phone sorry to be brief and potential misspell. My understanding is that concrete action about CODESCODES and CODESCODES are not being taken now, but instead we can demonstrate how to perform MICE as the stats world intended with something like this: CODELCODEL. This can be accompanied by comments that discuss why the various values are set as they are. In addition, we will also need a ML example that looks like this: CODELCODEL. I think we may have something like this already actually here: LINKLINK. glemaitre jnothman I ran a quick experiment with Boston to demonstrate that averaging a longer set of the last CODESCODES is helpful for downstream ML tasks. See this gist for the code: LINKLINK. The result: LINKLINK. Here Boston is missing data in 75 of the rows. I kept CODESCODES fixed to CODESCODES and swept over CODESCODES. At the very left of the plot is the average held out MSE of a CODESCODES after the data is imputed with MICE with CODESCODES and CODESCODES. It's the highest MSE and the most variable. As we go to the right, the mean MSE goes down until about 50 and then flattens out. This entire experiment was re run 100 times to get the bars. I think this provides at least a partial answer to: CODELCODEL Ah great! I can spend some time making a draft for the first example. I am also thinking that we should remove CODESCODES initial strategy CODESCODES to CODESCODES initial imputer CODESCODES since that it will be a pain to update the docstring, each time that the CODESCODES SimpleImputer CODESCODES get updated. Also it allows to accept several imputer and the future CODESCODES RandomImputer CODESCODES for instance. jnothman WDYT? glemaitre I'll try to get this one done today: Rename MICEImputer to de emphasise multiple imputation because it only performs a single one at a time. I'm going to use CODESCODES, unless anyone has any objections. Here is the renaming PR: 11314 sergeyf possibly naive question of somebody who has no statistics background: When the goal is to have a single best imputation and not multiple imputations to do inference on the results, is it then really needed to do the many iterations and take the average? In your experiment above you showed that with the current code it ensures a more stable result which is indeed good if the goal is the single best imputation, and you are not interested in the variation on this. But, the current implementation also introduces random noise based on the sigma of the model in each iteration: LINKLINK. So naively I would think doing this many times and averaging in the end will lead to almost the same results as just doing it once and not adding this random noise after the initial burn in, so relying on the mean of the model, but the latter would be much more efficient 1 instead of 100 imputations. jorisvandenbossche I'm not a MICE expert, really. I just happened to meet Stef in real life, tried MICE out on some problems I had, and noticed it worked very well in ML contexts. So I wanted to make it available to ML people. That is to say: I'm not the best person to answer. stefvanbuuren would know better, however. But, my intuition is that it would not be the same. It's basically a MCMC sampling process: LINKLINK. I think of the individual imputed values as jumping around as in the upper left image. If we freeze the process at any point, it may not be near the mean of that plot around By taking the average across the last CODESCODES, we are hedging against this and getting closer to the mean. I don't think simply not sampling at the very end would get you the mean because refitting at each iteration is also part of the sampling process. I could be wrong though. Maybe you could make a fork, quickly modify MICE and check your hypothesis, using the gist I posted as a base? Yes, I was planning, and took now the time to do it, see figure below. The figure is the same as the one above using the same seeds as you, but the added red line gives the experiment of using the imputed values of the last iteration + using the model mean without adding noise in each iteration. I did this for a CODESCODES of and with CODESCODES. So you don't really need to look at the x values, as the number of iterations do not match with the green line the green line always has a total of 100 iterations divided between n burn in and n imputations. I also added error bars for the results with the original data without missing values by running this also 100 times like the other experiments. LINKLINK. The mean value is slightly lower whether this is good or bad I don't know, the variation is clearly lower, and already stable after 10 burn in iterations for this example. Yes, but, when we don't add noise to the iterations but only use the mean of the model it will not be jumping around but converge to a stable single best imputation. So that is the reason that doing it like this gives a lower variance and with much less iterations, so more efficient. Of course, directly taking the output prediction of the imputation model without adding noise in each iteration will affect the next model in the chain, but whether this is a problem I don't know. The quick and dirty patch to MICEImputer click to expand. CODELCODEL. Thanks for the experiment. I think I had the wrong interpretation of what you were suggesting. It looks like not sampling during the chained process helps the downstream model quite a bit and reduces variance, which is a cool finding! Have you by chance looked at whether the MICE tests that have to do with empirical correctness still pass? They're in CODESCODES: CODESCODES, CODESCODES. Without any kind of sampling, this is somewhat far from the original MICE algorithm. The empirical evidence you've provided is positive, but it's limited to this example and has not been thoroughly explored like MICE has in the literature. This makes me a bit hesitant about adding it to sklearn. But maybe we can put the sampling behind a flag and set it to True by default? This would at least allow an end user to try to version you're suggesting. We would probably also need an example of how to use it with the flag set to False and why one might want to. Anyone else have thoughts here? This is what I meant by Perhaps determine if, in a predictive modelling. context, it is necessary to have the sophistication of MICE in sampling. each imputation value rather than just using point predictions. I think there is literature on chaining without sampling from a. distribution around the candidate imputation, but I've not explored the. references in the MICE paper. Before seeing this conversation, I was thinking it would be nice to support. regressors or indeed classifiers that do not give a predictive. distribution. Yes, I think we could control this with a parameter. 'sample'? The question is: which behaviour should we offer by default? Also I believe that this is the right way to go if we're to make it a more. generic ChainingImputer. But we can still support, and illustrate, the. inferences possible with Multiple Imputation and sampling. On 22 June 2018 at 14:13, Joel Nothman wrote: In my opinion, the default one should be the one that is empirically shown to work best with a variety of examples based on already available ones. Those two still pass only CODESCODES is failing, for good reason as CODESCODES no longer adds the stochastic noise Great, thank you for checking. Definitely would be good to have a flag to enable this. It would also mean we can easily toss in other regressors, It would be great to have a reference for the non sampled version of MICE if anyone happens to know a good one. Hi, I'm very new here but after talking with jorisvandenbossche, I would like to support him on the fact that generating a variable's missing data in the conditional distribution is perhaps not necessary with an aim of prediction we can just take the regression instead. I'm not sure about such an implementation of MICE, but missForest does just that with random forests. julierennes would know more than me on that. The example to show how ChainedImputer can be used as a MICE Imputer would include something like this: CODELCODEL. However, I have done some simulations testing the procedure and comparing the ChainedImputer with m 1 and m 5 and with n 1 and n 100 and the results are not as I would expect. I will post the script in a github repo later, but need some more time to think about these results. LINKLINK. RianneSchouten I haven't had time to think about this yet, but you probably want to set a different seed in this line for each of the imputations in this line: CODESCODES. It should instead be: CODESCODES jorisvandenbossche nprost. Indeed, if your aim is to impute and predict as well as possible the missing entries then using single imputation is enough and using multiple imputation for that is not required and you could impute by taking the conditional expectation and not by drawing from the conditional distribution. As far as prediction is concerned, there are not yet many results on this problem. Common practice and few papers tend more to suggest the following approach: Perform multiple imputation and on each imputed data set, apply your predictive algorithm to estimate the response say Then aggregate the different predictions. Best, JJ sergeyf. If the default random state is None, and that means it picks a random number, than the imputer will be different for every CODESCODES, don't you think? Because I delete the imputer after round i is finished with CODESCODES. Maybe changing it to CODESCODES means you don't have to delete the imputer? You shouldn't need to delete the imputer in any case. fitting again will. reset things. Using random state None is fine for this. random state i for. i 0, will only make sure the result is reproducible. julierennes thank you for confirming the suspicion that you could impute. by taking the conditional expectation and not by drawing from the. conditional distribution. When you suggest that we nonetheless predict. with an ensemble of predictive models fitted to different imputations, is. this common practice with something like missForest? In that case do. multiple draws come from different randomisation of the forest. construction, rather than samples from the gaussian posterior of. BayesianRidge used here in MICE?​. I also suspect that the current n imputations is not a feature we should be. providing. ​. You're probably right jnothman. I can make a PR that: a removes CODESCODES. b add a CODESCODES flag that requires the imputation model to have CODESCODES. On by default? With those two we should be able to replicate CODESCODES trivially as well as still do CODESCODES as designed. Let me know how that sounds and I can get started next week. Yes, thanks! I'm tempted to say sampling False by default given the results. cited above. But not sure. I should probably rename CODESCODES to CODESCODES, eh? n iter would be wonderful, actually. PR is here: 11350. It's nearly done, just waiting for some input. Or, a CODESCODES might also be an option? although it might be more work to change the current code. And then have a convergence criterion. This is similar to what missForest does LINKLINK. I suppose it should be possible to test convergence. ​ We can call it. max iter, but not have early stopping for now. The work in this thread is heading towards a method that does single imputation. This is very different from the philosophy and theory that underlies the MICE chained equations approach. I would like to ask the developers to document and advertise their new method as doing single imputation, so as to avoid any confusion with multiple imputation methodology. stefvanbuuren, the work in this thread is heading towards a tool that is capable of either single or multiple imputation, for example to engineer a solution that provides a common basis for multivariate imputation with chaining. We hope to include an example of multiple imputation and its application. However, in the context of a Scikit learn pipeline, and the needs of our users, single imputation is both most compatible, and appears to be at least as useful for predictive performance. At the moment we do not mention single imputation explicitly in the documentation and for many of our users this is not an especially meaningful term except in contrast to a definition of multiple imputation; otherwise it is mere imputation. The only place either term is used is LINKLINK. To quote: Does this seem acceptable? If you make single imputation the default, then this creates an association between the name MICE and single imputation. Users will say that they are doing MICE imputation, while in fact they are not properly accounting for the uncertainty of the synthetic values. So either break the association with MICE and advertise the method as single, or set the default to multiple and users to need to explicitly set m 1 when they want to do single. it is no longer called MICEImputer for this reason. In the section just above the one you quoted, we read: A more sophisticated approach is to use the ChainedImputer class, which implements. the imputation technique from MICE Multivariate Imputation by Chained Equations. MICE. models each feature with missing values as a function of other features. It's fairly easy for the user to mix up multivariate and multiple, and think they're doing MICE, which they do not under the proposed default. agreed that it's easy to confuse. let's reword it to: Focus on describing the algorithm. explain the parameters and application setting in which it is equivalent. to MICE, referring to the example we have not yet coded up. perhaps also note that the implementation was inspired by your R package. reference other literature on chained multivariate imputation. could you. please recommend seminal work in this space? thanks. OK, thanks. Here’s what I would do: Choose your default algorithm, and describe that as clearly as possible;. Buck 1960 was to first to suggest mutual regressions to find replacement values in multivariate missing data. MICE uses an iterated version of Buck’s method, and extended it to multiple imputation;. Refer to Rubin 1987, Ch 1 or Little and Rubin 2002, Ch. 4 for the limitations of single imputation;. If your default is single imputation, indicate that it is fairly easy to make it multiple: take m draws from the posterior for the missing value instead of finding one “best” value, analyze in parallel, and pool the results;. Create an example of the full multiple imputation cycle, including the pooling, for a classification prediction problem;. Indicate that it is still an open problem how useful single versus multiple imputation is in context of prediction classification. I haven’t seen any research that properly evaluates out of sample predictions from incomplete data;. Say that your method does single imputation if that's the default. Thanks stefvanbuuren and jnothman. I'll integrate the fruits of this discussion into the most recent open PR. Some additional questions, given the discussed re purposing of the imputer: Is CODESCODES a good name? We still do the chained equations part of MICE, but if I quickly google for it no thorough search though, I only find that term in context of MICE. For example, the missForest package paper which also does this iterative version, and is actually much closer to the future implementation in scikit learn does not speak at all about chained equations. One description of itself in the paper is iterative imputation method. Raghunathan et al. 2001 LINKLINK describes a similar idea as multivariate imputation using a sequence of regression models sequential regression multivariate imputation. It's mainly that the name ChainedImputer would not directly give many people a clue what it is doing I think. But MultivariateImputer is probably also too generic? Should CODESCODES be the default estimator? If the default settings are optimized for the single prediction setting, it may make sense to choose a different default estimator? I am OK changing name to CODESCODES or CODESCODES. In terms of the default: CODESCODES is best in my opinion: it's really fast. it supports CODESCODES if we want to do MICE. it has no params to tune. I tried to swap in CODESCODES into the predictor in the current tests and both CODESCODES and CODESCODES failed. I don't really agree: CODELCODEL. True but this is not the primary use case, isn't it. However, we should support it and offer an option to return the std when needing it. There is actually 4 parameters alpha 1, alpha 2, lambda 1, lambda 2. That's why I would go for a CODESCODES which has a single parameter, optimized using a CV, under consideration that a linear model is enough as default. sergeyf CODESCODES sounds good and neutral. CODESCODES might be too close to IVEware. glemaitre Thanks for the timings. I was thinking fast compared to RF, which I was fooling around with just a few minutes prior to writing that. It's true that RidgeCV is faster, but I still feel that supporting more use cases with less parameter fiddling out of the box is more important here. I'm open to being convinced however. stefvanbuuren Thanks. If glemaitre and jnothman agree, I'll change the name to CODESCODES. The naming is fine with me thumbs up. Guillaume Lemaitre. INRIA Saclay Parietal team. Center for Data Science Paris Saclay. LINKLINK. Iterative imputer does sound reasonably clear, and is probably clearer. In. comparison to ClassifierChain, this is much less chain like, since it uses. round robin rather than adding a link in each step. I would be happy with the default regression being dependent on whether or. not we are in sampling mode. Which could be fill 'mean' 'sample'. Or mode 'single' 'multiple'? My. main problem with that is that it might be possible to do multiple. imputation by randomising the regressor rather than using return std. I like the idea of the default being dependent on the mode. I think keeping the flag more function oriented CODESCODES rather than usage oriented CODESCODES or CODESCODES is probably better. I'm fine with posterior sample True False. OK, I made the changes as discussed. And I last minute went with CODESCODES I didn't actually look at Guillaume's benchmarks until now. I'd be. comfortable using BayesianRidge as the default in both cases. ​ but I. might be under informed on that. There is one thing that is not mentioned in this discussion yet and it is. important: the methodology of mice implies using the whole dataset including the. output variable in the imputation iterations. In other words: x1 is imputed. with x2, x3, and Then x2 is imputed with x1, x3, and y, etcetera! This is because the imputation model should include at least the. analysis model. stefvanbuuren am I saying this correctly? As said earlier, it is yet unknown what the difference is between the. outcome of a prediction model when the output variable will not be used. and a model built for statistical inference. Would it be possible to include the possibility to add y in the imputation. process? Something like CODESCODES by default and X np. column stack X, y CODESCODES include y True? I will show the difference between the two in the example then. This won't work because during validation test time we don't have CODESCODES. Your option would only work in transductive settings. Good point Rianne. Potentially you could add CODESCODES as a predictor and impute. Whether that's OK to do depends on how you want to impute. If you want to impute a single best value, then DO NOT include CODESCODES as now implemented. If you draw from the posterior you must include CODESCODES. See Little 1992 for more details. so. should we refuse to draw from the posterior and refuse to do multiple. imputation? surely in a clustering context needing to have y as a predictor. is meaningless. Sure, in a clustering context there is no observed CODESCODES, so then there is no issue whether you should include it or not. But for prediction, there is an observed CODESCODES, and we could go down two routes. It's then up to the software designer to decide which routes to support. So maybe we can find a way to make this an option, or to illustrate it in. an example. eventually. I don't see it as a priority here. My interest is mostly in making the API as stable, useful, well informed. and well documented as possible. Before a release in coming weeks, I should add. I just used this and noticed that a the model is called CODESCODES predictor CODESCODES not CODESCODES estimator CODESCODES or CODESCODES base estimator CODESCODES as is usual for a meta estimator, and b that it's not the first argument. Ideally I'd prohibit positional arguments but since we don't for now, I'd rather have the base estimator be the first argument. If you do CODESCODES IterativeImputer RandomForestRegressor n estimators 100 CODESCODES you get a hard to debug error about nans in training data. Thanks for the feedback. I'm happy to rename to CODESCODES. Regarding position: this is tricky because CODESCODES doesn't have a. meta estimator so it will have a different first parameter from. CODESCODES if we make the ordering change. Is that acceptable? That's the reason we haven't made CODESCODES first already. On Mon, Feb 11, 2019, 8:35 AM Andreas Mueller. wrote: I'm okay with renaming predictor to estimator. I think the point of. predictor was that there might be other underlying estimators, for example an. initial imputer. Thanks for the feedback, amueller. Most of the discussion is happening at 11977 rather than here. Should we consider making CODESCODES the first input parameter? It is the most important parameter. but I don't mind the current. consistency with SimpleImputer. I'll defer to sklearn full timers. Let me know if you reach a consensus to move it. I'll change the name to CODESCODES in the most recent PR.