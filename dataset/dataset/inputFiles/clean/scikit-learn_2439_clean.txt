The Mean Square Error returned by sklearn. cross validation. cross val score is always a negative. While being a designed decision so that the output of this function can be used for maximization given some hyperparameters, it's extremely confusing when using cross val score directly. At least I asked myself how a the mean of a square can possibly be negative and thought that cross val score was not working correctly or did not use the supplied metric. Only after digging in the sklearn source code I realized that the sign was flipped.

This behavior is mentioned in make scorer in scorer. py, however it's not mentioned in cross val score and I think it should be, because otherwise it makes people think that cross val score is not working correctly.
You're referring to 

 CODELCODEL 

in LINKLINK ? just for reference's sake 

I agree that that it can be more clear in cross val score docs

Thanks for reporting
Indeed we overlooked that issue when doing the Scorer refactoring. The following is very counter intuitive:

 CODELCODEL 

 cc larsmans 
BTW I don't agree that it's a documentation issue. It's CODESCODES should return the value with the sign that matches the scoring name. Ideally the CODESCODES should be consistent too. Otherwise the API is very confusing.
I also agree that a change to return the actual MSE without the sign switched would be the way better option. 

The scorer object could just store the CODESCODES flag and whenever the scorer is used the sign could be flipped in case it's needed, for example in CODESCODES.
I agree that we have a usability issue here, but I don't fully agree with ogrisel's solution that we should



because that's an unreliable hack in the long run. What if someone defines a custom scorer with a name such as CODESCODES? What if they do follow the naming pattern but wrap the scorer in a decorator that changes the name?



This is what scorers originally did, during development between the 0.13 and 0.14 releases and it made their definition a lot harder. It also made the code hard to follow because the CODESCODES attribute seemed to disappear in the scorer code, only to reappear in the middle of the grid search code. A special CODESCODES class was needed to do something that ideally, a simple function would do.

I believe that if we want to optimize scores, then they should be maximized. For the sake of user friendlyness, I think we might introduce a parameter CODESCODES ∈ CODESCODES that only changes the display of scores and can use a heuristic based on the built in names.
That was a hurried response because I had to get off the train. What I meant by display is really the return value from CODESCODES. I think scorers should be simple and uniform and the algorithms should always maximize.

This does introduce an asymmetry between built in and custom scorers.

Ping GaelVaroquaux.
I like the score is loss solution, or something to that effect. the sign change to match the scoring name seems hard to maintain could cause problems as larsmans mentioned
what's the conclusion, which solution should we go for? thumbs up 
 tdomhan jaquesgrobler larsmans Do you know if this applies to CODESCODES as well? I am noticing that the CODESCODES scores returned by CODESCODES are also mostly negative for CODESCODES, CODESCODES and CODESCODES. 
R² can be either positive or negative, and negative simply means your model is performing very poorly.
IIRC, GaelVaroquaux was a proponent of returning a negative number when CODESCODES.
 CODESCODES is a score function greater is better, so that should be positive if your model is any good but it's one of the few performance metrics that can actually be negative, meaning worse than What is the consensus on this issue? In my opinion, CODESCODES is an evaluation tool, not a model selection one. It should thus return the original values.

I can fix it in my PR 2759, since the changes I made make it really easy to fix. The trick is to not flip the sign upfront but, instead, to access the CODESCODES attribute on the scorer when doing grid search.




Special case are varying behaviors are a source of problems in software.

I simply think that we should rename mse to negated mse in the list
of acceptable scoring strings.


I don't think that ogrisel was suggesting to use name matching, just to be consistent with the original metric. Correct me if I'm wrong ogrisel.


That's completely unintuitive if you don't know the internals of scikit learn. If you have to bend the system like that, I think it's a sign that there's a design problem.




I disagree. Humans understand things with a lot of prior knowledge and
context. They are all but systematic. Trying to embed this in software
gives shopping list like set of special cases. Not only does it make
software hard to maintain, but also it means that people who do not have
in mind those exceptions run into surprising behaviors and write buggy
code using the library.
What special case do you have in mind?

To be clear, I think that the cross validation scores stored in the CODESCODES object should also be the original values not with sign flipped. 

AFAIK, flipping the sign was introduced so as to make the grid search implementation a little simpler but was not supposed to affect usability.


Well, the fact that for some metrics bigger is better, whereas for others
it is the opposite.





It's not about grid search, it's about separation of concerns: scores
need to be useable without knowing anything about them, or else code to
deal with their specificities will spread to the whole codebase. There is
already a lot of scoring code.
But that's somewhat postponing the problem to user code. Nobody wants to plot negated MSE so users will have to flip signs back in their code. This is inconvenient, especially for multiple metric cross validation reports PR 2759, as you need to handle each metric individually. I wonder if we can have the best of both worlds: generic code and intuitive results.




Certainly not the end of the world. Note that when reading papers or
looking at presentations I have the same problem: when the graph is not
well done, I loose a little bit of time and mental bandwidth trying to
figure if bigger is better or not.




Why. If you just accept that its always bigger is better, it makes
everything easier, including the interpretation of results.




The risk is to have very complex code that slows us down for maintainance
and development. Scikit learn is picking up weight.


That's what she said thumbs up 

More seriously, I think one reason this is confusing people is because the output of CODESCODES is not consistent with the metrics. If we follow your logic, all metrics in sklearn. metrics should follow bigger is better.


Nice one!






Agreed. That's why I like the idea of changing the name: it would pop up
to people's eyes.


And this in turn makes CODESCODES seem more mysterious than it is.


I'd suggest to make it all consistent and return the non sign flipped score for linear models as well.

Example:

 CODELCODEL 

This gives:

 CODELCODEL 
Cross validation flips all signs of models where greater is better. I still disagree with this decision. I think the main proponent of it were GaelVaroquaux and maybe mblondel.
Oh never mind, all the discussion is above.
I feel flipping the sign by default in mse and r2 is even less intuitive thumbs down 
 Huitzilo GaussianNB is a classifier and uses accuracy as default scorer. LinearRegression is a regressor and uses r2 score as default scorer. The second score is negative but remember that the r2 score can be negative. Also, iris is a multiclass dataset. Hence the targets are categorical. You can't use a regressor.
right, I was a bit confused about what happens, r2 is not flipped. only mse would be.
Maybe a solution to the whole problem is rename the thing CODESCODES?
 mblondel of course you are right, sorry. I was just quickly slapping together an example for a regression, and in my overconfidence on the iris data I thought predicting feature 4 from the others would work with positive R2. But it didn't, hence, negative R2. No sign flipping here. OK. My bad.

Still, the sign is flipped in the MSE I get from CODESCODES. 

Maybe it's just me, but I find this inconsistency vastly confusing which is what got me into this issue. Why should MSE be sign flipped, but not R2? 


Because the semantic of score is higher is better. High MSE is bad. 
maybe negmse would solve the problem
 amueller I agree, making the sign flipping explicit in the name of the scoring parameter would definitely help to avoid confusion. 

Maybe the documentation at could also be even more explicit about how signs are flipping for some scores. In my case, I needed information quickly and only looked at the table under 3.1. but didn't read the text which explains the bigger is better principle. IMHO, adding a comment for mse, median and mean absolute error in the table under 3.1. indicating their negation, would already help a lot, without any changes to the actual code.

 LINKLINK 
I've come across a very interesting case:

 CODELCODEL 

Results in 

 CODELCODEL 

For the same dataset the following code 

 CODELCODEL 

results in a reasonable value

 CODELCODEL 

AFAIK for linear regression model with intercept one can't obtain R 2 1 or R 2 0

Thus, the cv result doesn't look like R 2 with a flipped sign. Am I wrong at some point?
r2 can be negative for bad models. It cannot be larger than 
You are probably overfitting. try:

 CODELCODEL 

Try with different values for the CODESCODES integer seed that controls the random split.


 thumbs up for 'neg mse' I think that underscore makes things more readable.
Does that solve all problems? Are there other scores were greater is not better?
There are:
 CODESCODES 
 CODESCODES 
 CODESCODES 

According to CODESCODES, that should be all of them.
And CODESCODES I guess?
Adding the CODESCODES prefix to all those losses feels awkward.

An idea would be to return the original scores without sign flip but instead of returning an ndarray, we return a class which extends ndarray with methods like CODESCODES, CODESCODES, CODESCODES. This way the results are unsurprising and we have convenience methods for retrieving the best results.
There's no scorer for hinge loss and I've never seen it being used for evaluation.
The scorer doesn't return a numpy array, it returns a float, right?
we could return a score object that has a custom but looks like a float.
That feels more contrived to me than the previous solution, which was tagging the scorer with a bool lower is better which was then used in GridSearchCV.
 CODESCODES returns an array.
Actually the scores returned by CODESCODES usually don't need to be sorted, just averaged.

Another idea is to add a CODESCODES method to CODESCODES.

 CODELCODEL 
 CODESCODES returns an array, but the scorers return a float. I feel it would be odd to have specific logic in CODESCODES because you'd like to have the same behavior in GridSearchCV and in all other CV objects.

You'd also need an argsort method, because in GridSearchCV you want the best score and the best index.
 How to implement estimate the means and variances of the workers' errors from the control questions, then compute the weighted average after removing the estimated bias for the predictions by scikit learn?
IIRC we discussed this in the sprint last summer? and decided to go with CODESCODES or was it CODESCODES and deprecate all scorers strings where we have a negative sign now.
Is this still the consensus? We should do that before 0.18 then.
Ping GaelVaroquaux agramfort jnothman ogrisel raghavrv
yes we agreed on neg mse AFAIK
It was CODESCODES 
We also need:
 CODESCODES 
 CODESCODES 
 CODESCODES 


model. add Dense 11, input dim 3, kernel initializer keras. initializers. he normal seed 2,
 kernel regularizer regularizers. l2 2 
keras. layers. LeakyReLU alpha 0.1 
model. add Dense 8, kernel initializer keras. initializers. he normal seed 2 
keras. layers. LeakyReLU alpha 0.1 
model. add Dense 4, kernel initializer keras. initializers. he normal seed 2 
keras. layers. LeakyReLU alpha 0.1 
model. add Dense 1, kernel initializer keras. initializers. he normal seed 2 
keras. layers. LeakyReLU alpha 0.2 
adag RMSprop lr 0.0002 
model. compile loss losses. mean squared error,
 optimizer adag
 
history model. fit X train, Y train, epochs 2000,
 batch size 20, shuffle True 
 
How to cross validate the above code? I want leave one out cross validation method to be used in this. shreyassks this isn't the correct place for your question but I would check this out: LINKLINK. Wrap your network in a CODESCODES estimator then use w CODESCODES Yes. I totally agree! This also happened to Brier score loss, it works perfectly fine using Brier score loss, but it gets confusing when it comes from the GridSearchCV, the negative Brier score loss returns. At least, it would be better output something like, because Brier score loss is a loss the lower the better, the scoring function here flip the sign to make it negative. The idea is that cross val score should entirely focus on the absolute value of the result. In my knowledge, importance of negative sign obtained for MSE mean squared error in cross val score is not predefined. Let's wait for the updated version of sklearn where this issue is taken care of.For Regression usecase:
model score cross val score model, df input, df target, scoring 'neg mean squared error', cv 3 
I am getting the values as:

SVR:
 
 3.183982080147279

Linear Regression:
 
 5.4719685646934275

Lasso: 
 
 6.6363521107522345

Ridge: 
 
 3.8360764993832004

So which one is best?
SVR?
For Regression usecase:
I am getting different results when I use
 1 cross val score with scoring 'neg mean squared error'
and
 2 For the same inputs when when I use GridSearchCV and check the 'best score '

For Regression models which one is better?
 cross val score with scoring 'neg mean squared error'
 OR 
 use GridSearchCV and check the 'best score ' 



 pritishban 
You're asking a usage question. The issue tracker is mainly for bugs and new features. For usage questions, it is recommended to try LINKLINK or LINKLINK.