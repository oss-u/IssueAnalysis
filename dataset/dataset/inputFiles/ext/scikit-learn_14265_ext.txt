Before adding OpenMP based parallelism we need to decide how to control the number of threads and how to expose it in the public API [CLS] [SEP] I've seen several proposition from different people:&nbsp;&nbsp; 1 Use the existing CODESCODES public parameter with CODESCODES means 1 same a for joblib parallelism.&nbsp;&nbsp; 2 Use the existing CODESCODES public parameter with CODESCODES means thumbs down like numpy lets BLAS use as many threads as possible.&nbsp;&nbsp; 3 Add a new public parameter CODESCODES when underlying parallelism is handled by OpenMP, with CODESCODES means.&nbsp;&nbsp; 4 Add a new public parameter CODESCODES when underlying parallelism is handled by OpenMP, with CODESCODES means thumbs down [CLS] [SEP] &nbsp;&nbsp; 5 Do not expose that in the public API [CLS] [SEP] Use as many threads as possible [CLS] [SEP] The user can still have some control with CODESCODES before runtime or using threadpoolctl at runtime [CLS] [SEP] 1 or 2 will require improving documentation of CODESCODES for each estimator: what's the default, what kind of parallelism, what is done in parallel [CLS] [SEP] see 14228. scikit learn core devs, which solution do you prefer [CLS] [SEP] If it's none of the previous ones, what's your solution [CLS] [SEP] My preference is 2 [CLS] [SEP] I opened 14196 in that directionThanks for opening the issue [CLS] [SEP] I have a preference towards 4 [CLS] [SEP] The default for CODESCODES could be 'auto', which would mean use as many threads as possible, avoiding over subscription. I don't like 2 because: the same parameter is used for different things OpenMP vs joblib [CLS] [SEP] the default does something different depending on the underlying implementation [CLS] [SEP] If we're going to expose the implementation to the user, I prefer 4 because it is more explicit [CLS] [SEP] it is going to be hard to explain to users the glossary for n jobs is already intense, especially for estimators that would use both joblib and openmp if any [CLS] [SEP] That being said, I wouldn't vote thumbs down [CLS] [SEP] I'm leaning towards 3 or I'm not sure which one's better, but as NicolasHug says, I too rather have it different than CODESCODES [CLS] [SEP] I think that naming openMP parallelism differently from Python level. parallelism is a very technical nuance that we be lost to most of our. users [CLS] [SEP] We need to be able to give a simple message in terms of how to. control parallelism [CLS] [SEP] With regards to the default, I would also love to have the right default. that prevents oversubscription [CLS] [SEP] Do we have the technical tool needed for. that [CLS] [SEP] I know that Jeremie and Olivier have been struggling with. oversubscription [CLS] [SEP] I have reviewed what is done in some other ML libraries that use OpenMP, xbgoost has nthread but deprecated it in favor of CODESCODES in their scikit learn API [CLS] [SEP] lighgbm uses CODESCODES. in recsys lightfm and implicit use CODESCODES. I guess CODESCODES would be semantically more correct, however my main concern for it are, additional parameters to a lot of models. how we handle parallelism change [CLS] [SEP] Say that now we have a pure python code that uses multiprocessing, we re write it to Cython + OpenMP, if we go with 4 that would require a deprecation warning, with a change of parameter name, and I don't think that asking that from the user would be justified or that we should spend efforts on deprecations for this [CLS] [SEP] similarly, joblib with threading assuming that it operates on functions that release GIL is fairly similar to using OpenMP [CLS] [SEP] Having a different parameter name in those case is then not very logical, but at the same time we don't want to rename parameter names in lots of estimators either [CLS] [SEP] So I don't there is a perfect solution here, but I would be also more in favor of. I don't think that should be documented [CLS] [SEP] It's an implementation detail that can change at any moment, mostly people only care that when one increases n jobs it makes their code faster and are disappointed if it doesn't [CLS] [SEP] To be more precise, I think we should have a documentation section glossary discussing different possible levels of parallelism, but that it shouldn't be specified in each estimator [CLS] [SEP] I know they can be changed and we don't give users a guarantee that they'll stay the same, but in terms of distributing jobs on a cluster, or to use dask's backend sometimes for instance, it's much easier for the user and developers to know which parts are on MPI and which parts on processes [CLS] [SEP] If one leaves, CODESCODES sets CODESCODES in the dask worker and uses CODESCODES to let dask handle the number or processes nodes shouldn't that work independently of the implementation [CLS] [SEP] We already have BLAS parallelism happening a bit everywhere, so when one adds OpenMP and processes on top, in such complex systems the easiest is just to benchmark and see what works best in terms of number of threads processes [CLS] [SEP] Say we add OpenMP somewhere, that may or may not be noteworthy depending on the amount of linalg operations happening before in BLAS that ratio may also scale in some unknown power with the data size [CLS] [SEP] the idea is to call CODESCODES on the estimator attribute CODESCODES or CODESCODES, just before the prange [CLS] [SEP] It will return a value which depends on CODESCODES [CLS] [SEP] We can influence that at the python level with threadpoolctl to prevent oversubscription for instance we could use it in grid search [CLS] [SEP] It works fine for all sklearn use cases thumbs up I would like to understand the oversubscription more [CLS] [SEP] jeremiedbb why does it work fine and what does that mean [CLS] [SEP] If I run CODESCODES GridSearchCV CODESCODES with CODESCODES n jobs thumbs down CODESCODES and inside I have an estimator that uses OpenMP and I set CODESCODES n jobs thumbs down CODESCODES, what happens [CLS] [SEP] Grid Search was not a good example because joblib should already handle this case [CLS] [SEP] It does it by setting the number of threads to 1 for openmp [CLS] [SEP] jeremiedbb Why is that not a good example [CLS] [SEP] And if the user sets CODESCODES n job 2 CODESCODES in CODESCODES GridSearchCV CODESCODES and CODESCODES n jobs thumbs down CODESCODES in and OpenMP estimator [CLS] [SEP] Or CODESCODES n jobs 4 CODESCODES in both with only 4 cores [CLS] [SEP] Sorry I'm not familiar with how joblib changes the openmp threads [CLS] [SEP] Assuming that we can ensure a good protection against subscription in joblib which I believe will soon be the case, I would be in favor setting the defaults to let our OpenMP loops use the currently maximum number of threads as OpenBLAS and MKL do for numpy scipy [CLS] [SEP] thumbs up for letting the user pass an explicit number of threads using the CODESCODES argument if they want [CLS] [SEP] It's just that for CODESCODES the default I would let OpenMP what it wants to do [CLS] [SEP] For process based parallelism for example with the loky backend of joblib I think keeping the default CODESCODES mean sequential by default is safer as there can be a non trivial communication and memory overhead [CLS] [SEP] For joblib with the threading backend I would be in favor of keeping the current behavior but reexploring this choice later if necessary [CLS] [SEP] For reference the over subscription protection in joblib will be tracked by LINKLINK [CLS] [SEP] My pocket sent this before i finished: D. It was not a good example for threadpoolctl [CLS] [SEP] But it is for joblib [CLS] [SEP] Alrhough we found that it doesn't work in previous joblib versions but it should be fixes on next release [CLS] [SEP] Threadpoolctl would be use when doing nested parallelism openmp blas [CLS] [SEP] It allows to prevent oversubscription by limiting the number of threads for blas [CLS] [SEP] This situation only happens for kmeans right now [CLS] [SEP] Overall we prevent oversubscription by disabling parallelism for non top level parallelism [CLS] [SEP] In summary, any default is fine for n jobs since it would be forced to one when already in a parallel call. Just for me to get this straight: If I'm running a grid search over HistGradientBoosting or something with OpenMP, we would probably want to default it to use all available cores [CLS] [SEP] Now the user sets CODESCODES n jobs 2 CODESCODES in CODESCODES GridSearchCV CODESCODES to make things run faster, but say they have more than 2 cores [CLS] [SEP] CODESCODES GridSearchCV CODESCODES would use joblib to parallelize, but joblib would prevent OpenMP parallelism inside the parallel call, so less threads would be active and things will likely be slower [CLS] [SEP] Separate scenario: Someone set's CODESCODES n jobs CODESCODES in both CODESCODES GridSearchCV CODESCODES and CODESCODES HistGradientBoosting CODESCODES [CLS] [SEP] No matter what, the CODESCODES HistGradientBoosting CODESCODES one that the user explicitly set would be ignored, right [CLS] [SEP] That's right [CLS] [SEP] In that case, he should set n jobs thumbs down for the grid search to benefit from all cores [CLS] [SEP] In many situations personal observations it's better to enable parallelism in the most outer loop [CLS] [SEP] Note that it's the case with BLAS [CLS] [SEP] If you set n jobs 2 for an estimator, joblib parallelized, BLAS will be limited to use only 1 core [CLS] [SEP] I think that behavior is ok if we document it properly [CLS] [SEP] OK, but if they go from n jobs 1 to 2, they will get worse results which may be a bit unexpected [CLS] [SEP] Isn't that set via CODESCODES in the created processes [CLS] [SEP] Which means that potentially it could be anything, including CODESCODES [CLS] [SEP] Note that slight oversubscription may not be bad at least last time I looked into it in HPC [CLS] [SEP] for example if you have 4 CPU cores, 2 processes with 4 threads each is not necessarily worse than 2 process with 2 threads each if the task use heterogeneous compute units; the problem is really on 40 core CPU with 40Â² threads [CLS] [SEP] So basically, it's about priority between CODESCODES and user provided CODESCODES or CODESCODES that corresponds to threading [CLS] [SEP] If one provided a CODESCODES that is not CODESCODES, maybe it should have higher priority than CODESCODES [CLS] [SEP] I agree [CLS] [SEP] That's a drawback of having default None means thumbs down [CLS] [SEP] But I think it's also a matter of documentation, because if I know my estimator already uses all cores, why would I increase the number of workers for my grid search [CLS] [SEP] Or maybe I'd like to disable parallelism of my estimator and use all cores for the grid search which is your next point [CLS] [SEP] Yes [CLS] [SEP] I guess it's possible [CLS] [SEP] Indeed, but it's not good either at least I've never experienced getting better performance with oversubcription [CLS] [SEP] In the implementation I propose, n jobs num threads has priority over environment variables [CLS] [SEP] Only the default None would be influenced by the environment variables [CLS] [SEP] If you have a grid search with n jobs 2 and you set n jobs n threads n cores 2, you'll get full saturation of your cores [CLS] [SEP] I'll add that having good default is important, but it shouldn't prevent users from thinking about what are those default, why are they good, and when are they good [CLS] [SEP] The case of multiprocessing where each process starts some number of threads makes me think of hybrid MPI OpenMP programming in HPC cf for example LINKLINK [CLS] [SEP] The analogy is maybe a bit partial as we don't use MPI nor run on multiple nodes, but the data serialization cost when starting new processes for now does have a somewhat similar effect to communication cost with MPI [CLS] [SEP] The conclusion in the HPC field, as far as I am aware, is that the optimal number of processes nested threads is application and hardware dependent [CLS] [SEP] So my point is that is might be useful to be able to control both the number or parallel processes and the number of nested threads separately, but to control the latter it's preferable to have some global mechanism such as CODESCODES for example set via joblib that would also apply to BLAS, as opposed to a CODESCODES parameters, that would only apply to scikit learn specific OpenMP loops [CLS] [SEP] I don't think that None should mean thumbs down or 1 [CLS] [SEP] It should mean best. guess to be efficient given the global constraint, and the semantics of. that can evolve and depend on the context [CLS] [SEP] What I am saying is that when users specify None, we should, as time. goes add dynamic scheduling logic to be more efficient, and we should. tell the user explicitly that the details implementation of the job. scheduling will evolve [CLS] [SEP] Yes, it matches my experience, as long as we don't blow the memory [CLS] [SEP] I was not aware of that [CLS] [SEP] That is [CLS] [SEP] an interesting side effect [CLS] [SEP] Was that always the case [CLS] [SEP] Is that documented anywhere [CLS] [SEP] It has been introduced in joblib 0 [CLS] [SEP] 12, LINKLINK. It is documented LINKLINK, under Avoiding over subscription of CPU ressources. However, it was bugged until now and should be fixed in the next joblib release [CLS] [SEP] So that was a change introduced in scikit learn 0 [CLS] [SEP] 20 but not documented in the changelog [CLS] [SEP] Did we communicate this change to scikit learn users in some way [CLS] [SEP] Anyway, it sounds like we want a different strategy by default, like CODESCODES available cpu cores n jobs CODESCODES right [CLS] [SEP] Well now that we unvendored joblib, I'm not sure where to communicate about joblib parallelism changes in scikit learn release notes [CLS] [SEP] Nevermind, as discussed above this feature was added but actually had no effect due to a bug, so there is nothing to worry about [CLS] [SEP] Change proposed in LINKLINK. Also I think we should still merge LINKLINK: it's a private function to be able to use CODESCODES safely and is a bit orthogonal to the present discussion [CLS] [SEP] It does not work because you can have nested CODESCODES calls [CLS] [SEP] Besides I don't think it's such a bad behavior [CLS] [SEP] Currently CODESCODES does not match the number of cores you're actually using because it does not affect BLAS [CLS] [SEP] Users are often surprised because you ask for 4 jobs and your htop is full on your 40 cores machine [CLS] [SEP] The discussion has derived to oversubscription questions in sklearn which is not exactly the initial purpose and which I think can be treated separately, assuming that we have the right tools to deal with oversubscription independently of the default [CLS] [SEP] Thus I propose to refocus the discussion on the initial question [CLS] [SEP] Let me try to summarize the discussion [CLS] [SEP] We are currently divided, almost evenly between use n jobs with default let OpenMP use as many as possible and add a new parameter CODESCODES with same default [CLS] [SEP] One good thing is we seem to agree on the default thumbs up. Comments about the choice for the defaults: This choice for the default is supported by the fact that there are ways to tell OpenMP what use as many as possible means [CLS] [SEP] Through CODESCODES env var or through CODESCODES, which we can use in nested parallelism situations in sklearn [CLS] [SEP] The fact that joblib sets openmp max threads to 1, isn't entirely satisfactory [CLS] [SEP] We could make it possible to pass the max number of threads as a parameter to CODESCODES [CLS] [SEP] Comments about the name of the parameter: pros to keeping CODESCODES: keep a single parameter for parallelism. changing the underlying implementation moving from joblib to OpenMP based parallelism causes change of parameter name and deprecation cycle [CLS] [SEP] pros to change name: the same parameter name having different defaults depending on the estimator is confusing [CLS] [SEP] use different names for different things. I propose to try to discuss a little bit more see if one side manage to convince the other side thumbs up. Then if we still don't agree I guess I'll have to make a slep [CLS] [SEP] I would argue that this is a bug that was introduced in joblib when I wasn't looking [CLS] [SEP] It has horrifyingly terrible performance consequences for anything using BLAS, and not also for gradient boosting [CLS] [SEP] Also, it was a backward incompatible change made at some point without even a mention in whatsnew, for example many people's code got drastically slower [CLS] [SEP] I find your name of parameter summary hard to understand having not followed this thread entirely [CLS] [SEP] Do you mean using the CODESCODES n jobs CODESCODES name for setting the number of threads [CLS] [SEP] That seems terrible [CLS] [SEP] And having different defaults also seems terrible [CLS] [SEP] There is estimators I assume kmeans [CLS] [SEP] that do both, multiprocessing and multithreading [CLS] [SEP] What would the parameter do there [CLS] [SEP] I don't see why we would have the user handle the threadpool via a parameter [CLS] [SEP] Why should the user care whether we use multithreading via Cython or BLAS [CLS] [SEP] We don't have a parameter to set CODESCODES OMP MAX THREADS CODESCODES every time we call CODESCODES np [CLS] [SEP] dot CODESCODES [CLS] [SEP] So why should we have it in other places [CLS] [SEP] We thought that we had implemented oversubscription protection in joblib 0 [CLS] [SEP] 12 in the case of Python worker processes with nested OpenMP BLAS but it's actually not the case because of the way the worker processes are started and their runtime initialized more details in joblib joblib 880 [CLS] [SEP] tomMoral has recently been working in improving the way loky starts its worker processes to have more control tomMoral loky 217 and this was released yesterday in loky 2 [CLS] [SEP] 0 [CLS] [SEP] The next step is to make joblib use this infrastructure to set the default number of threads allowed in each worker process to cpu count number of worker processes: joblib joblib 913 [CLS] [SEP] This default behavior will be overrideable by the user with something such as: CODELCODEL. It's actually mentioned in joblib's changelog LINKLINK but I agree it's not explicit enough [CLS] [SEP] We will make sure to be more explicit in the changelog for the next joblib release [CLS] [SEP] Why should the user care whether we use multithreading via Cython or BLAS [CLS] [SEP] We don't have a parameter to set OMP MAX THREADS every time we call np [CLS] [SEP] dot [CLS] [SEP] So why should we have it in other places [CLS] [SEP] I would be fine in not exposing any parameter in the scikt learn API to control OpenMP BLAS threads instead of overloading the meaning of n jobs [CLS] [SEP] Why should the user care whether we use multithreading via Cython or BLAS [CLS] [SEP] We don't have a parameter to set OMP MAX THREADS every time we call np [CLS] [SEP] dot [CLS] [SEP] So why should we have it in other places [CLS] [SEP] That was exactly the 5th solution proposed at the top of this thread, but no one seemed to prefer it so I did not mention it in the summary [CLS] [SEP] I agree that it's a legit solution which maybe deserved more attention thumbs up jeremiedbb sorry I should have started from the top thumbs up. Option 5 is the only one that seems consistent with current behavior [CLS] [SEP] I still don't understand your summary, though [CLS] [SEP] ogrisel the point is that sklearn behavior drastically changed, so that should be documented in the sklearn changelog, right [CLS] [SEP] At least as long as we were vendoring joblib [CLS] [SEP] I guess my question for 1 4 would be what when underlying parallelism is handled by OpenMP means [CLS] [SEP] Are we inspecting whether the blas we use was built with OpenMP and if so check if there's any call to the blas in the function [CLS] [SEP] No it means our OpenMP based cython code [CLS] [SEP] Let's say you have an estimator with a parallel implementation based on OpenMP, for example HGBT [CLS] [SEP] The question is which one of the following solutions do we want [CLS] [SEP] HGBT should have a CODESCODES parameter, which defaults to use as many cores as possible. HGBT should have a CODESCODES parameter, which defaults to single threaded. HGBT should have a CODESCODES parameter, which defaults to use as many cores as possible. HGBT should have a CODESCODES parameter, which defaults to single threaded. HGBT should not have any parameter, it uses as many cores as possible and that's all. We all agree that single threaded by default is not a good choice [CLS] [SEP] So we must decide between 1, 3 and I hope this summary is more clear thumbs up. But why does the user need to care if we wrote cython code or we're calling blas [CLS] [SEP] How is that relevant to the user [CLS] [SEP] If we replaced a call to CODESCODES dot CODESCODES with an OpenMP based implementation of our own the behavior in terms of multithreading wouldn't change or I might be missing something [CLS] [SEP] but because we wrote the code we would provide a different interface [CLS] [SEP] I agree that the 5th solution is the more consistent [CLS] [SEP] We let BLAS do what it wants, we should do the same for OpenMP based multithreading [CLS] [SEP] But having the possibility to get some control on the number of threads is nice, especially for estimators for which the OpenMP parallelism is not in a small part of the algo but at the outermost loop like in my KMeans PR [CLS] [SEP] Again, you looked at this way more than I did so I might be missing something [CLS] [SEP] But OpenBLAS optionally [CLS] [SEP] uses OpenMP under the hood, right [CLS] [SEP] the OpenBLAS shipped with numpy and scipy does not but you can build it to use OpenMP and link numpy against that [CLS] [SEP] MKL uses OpenMP [CLS] [SEP] I totally agree with that [CLS] [SEP] For small bits of code using OpenMP it's what makes the most sense [CLS] [SEP] My motivation is for estimators like KMeans for which the parallelism happens at the outermost loop [CLS] [SEP] Ok [CLS] [SEP] So that's why I find the phrase OpenMP based multithreading quite confusing [CLS] [SEP] Because what you really mean is OpenMP base multithreading where we wrote the call into OpenMP in Cython [CLS] [SEP] And maybe there's a qualitative difference in how you use OpenMP in KMeans vs how it might be used in Nystroem but at least to me that is not obvious sorry if that has been discussed above [CLS] [SEP] Can you maybe say a bit more about that difference [CLS] [SEP] I can't I don't know how it would be used in Nystroem: the idea is if it's a bit of code that is parallel with OpenMP in our cython code, we'd like it to behave as BLAS, i [CLS] [SEP] e use as many cores as possible [CLS] [SEP] On the other hand if it's the whole algorithm which is parallel still OpenMP in our cython code, at the outermost loop, maybe we'd like to provide some control [CLS] [SEP] Nystroem is just a call to SVD which I assume is handled entirely by blas thumbs up I also think that CODESCODES without exposing thread control in scikit learn would be a good idea [CLS] [SEP] That would sidestep this whole CODESCODES CODESCODES discussion, and leave us the possibility to change it later if needed, while still exposing a mechanism for controlling n threads for advanced users [CLS] [SEP] As a side note, about using all threads by default for example in BLAS that logic probably originated 10 20 years ago when computers had 2 4 cores [CLS] [SEP] Now you can get a desktop with 10 20 CPU cores and servers with up to 100 [CLS] [SEP] In that context using all cores by default kind of assumes that our library is alone in the world and can use all resources with no cost [CLS] [SEP] There is a lot of cases where using all CPU cores is not ideal shared servers, even desktops laptops with other resource intensive applications running for example rendering all that JS in a browser [CLS] [SEP] On a server under load spanning all threads will actually slow down both the running applications and other applications due to oversubsription [CLS] [SEP] Besides, not that many applications have a good scaling beyond 10 CPU cores or use big enough data where it makes sense Yes, using threads is almost always faster, but also at the cost of significantly higher CPU time and electricity consumption [CLS] [SEP] Maybe we don't care so much for the scikit learn use cases, but it could still be something worth considering [CLS] [SEP] Not saying we should change anything in the proposed approach now, but to leave things flexible enough so we can change this default later if needed [CLS] [SEP] The decorator seems reasonable [CLS] [SEP] Though if that doesn't apply to BLAS, I'm sure we'll get questions by confused users, and it'll basically mean there's two ways to control the number of threads, right [CLS] [SEP] I agree [CLS] [SEP] In our experience, the default scaling to all CPUs is a bad idea on large computers for multiple reasons: When multiple processes runs like this which often happens in Python, it leads to huge oversubscription, which can even freeze the box [CLS] [SEP] One examples is n job thumbs down + openMP using all the threads, on a box with n CPU, it uses n 2 CPUs, which leads to disasters if n is largish. Those boxes are often multi tenants [CLS] [SEP] I would be in favor that inner number of threads shouldn't by default exceed 10 [CLS] [SEP] The big picture problem is a hard one: in a real user codebase, these days, we end up with multiple parallel computing systems that are nested: Python's multiprocessing, Python's threads, openMP's parallel computing and several can coexists if eg scikit learn was compiled with GCC and MKL compiled with ICC [CLS] [SEP] What we would really need is dynamic scheduling of resources [CLS] [SEP] Intel's TBB offers that by the way though I'm not suggesting that we use it: D [CLS] [SEP] It's a hard problem, and we won't tackle it here and now [CLS] [SEP] However, it would be good to think about possible evolutions in this directions in the contract that we give to the user in terms of parallel computing [CLS] [SEP] With the next release of CODESCODES which should happened soon, the over subscription problem should be reduced [CLS] [SEP] It will provides two improvements: By default, CODESCODES for the BLAS in child processes will be set to CODESCODES, which is a sensible value to avoid over subscription [CLS] [SEP] This should tackle the main problem which is that when CODESCODES is used in an estimator with CODESCODES on a big cluster, this leads to catastrophic oversubscription, while remaining performant for smaller number of CODESCODES [CLS] [SEP] We had set the limit to 1 before because it was safer due to the re usability of the workers [CLS] [SEP] It will be possible to parametrize this number using the CODESCODES context manager to set different value if it is needed for advance users [CLS] [SEP] This will provide a better control for the user than the usage of global environment variables like CODESCODES that were also reducing the performances in the main process [CLS] [SEP] CODELCODEL. That will also apply to BLAS [CLS] [SEP] Currently we plan to set MKL NUM THREADS, BLIS NUM THREADS, OPENBLAS NUM THREADS and OMP NUM THREADS [CLS] [SEP] ogrisel sorry, I'm not sure I understand your statement [CLS] [SEP] What will apply to what [CLS] [SEP] You're saying that loky will set these environment variables, right [CLS] [SEP] But what if the user already set those variables in a script or on the command line or however they launched a job on their cluster [CLS] [SEP] And so will the default of, say, 10 be enforced in loky or in sklearn [CLS] [SEP] amueller NicolasHug We just merged this over subscription mitigation feature in CODESCODES, could you give it a try and see if it matches what you would expect [CLS] [SEP] You can find some explanation about this new feature in LINKLINK [CLS] [SEP] The current implemented behavior gives: If the user do nothing and uses CODESCODES parallel processes for computation, the call to native libraries with inner threadpools OpenBLAS, MKL, OpenMP, will be restricted to CODESCODES default [CLS] [SEP] If the user set an environment variable such as CODESCODES, this limit is passed in the child process and we do not change the behavior [CLS] [SEP] This is mainly intended for cases where the user disable parallel processing for BLAS calls in the whole program, so with CODESCODES [CLS] [SEP] For more advance usage, the user can programatically override this with the context CODESCODES, which will take precedence over the other behavior and can be used to set locally the resource allocation [CLS] [SEP] tomMoral Looks great [CLS] [SEP] Just a question, say I want to restrict the number of BLAS threads without using any other parallelism [CLS] [SEP] Could something like CODESCODES be made to work, or should I use directly the vendored threadpoolctl in that case [CLS] [SEP] It would be nice if there was a single API for it [CLS] [SEP] Also I guess if you explicitly create thread based parallelism say with joblib, those are not going to be constrained [CLS] [SEP] Or will they be [CLS] [SEP] If you want to restrict the number of thread in a process, you need to either set the env var CODESCODES before starting your process or use CODESCODES [CLS] [SEP] For now, we only rely on the former as we still have some major issues with CODESCODES [CLS] [SEP] In particular, it is quite unreliable [CLS] [SEP] We found multiple ways to create deadlock by using it because of bad interaction between multiple native libraries with threadpools [CLS] [SEP] And indeed for thread based parallelism, we don't have a solution to set the number of inner threads in each threads, except a global limit with env variables [CLS] [SEP] To be more specific it seems that introspecting or changing the size of the threadpools dynamically can be problematic in some case for programs linked with multiple openmp runtimes at the same time [CLS] [SEP] For instance there is a case under investigation here: LINKLINK. So for now I would rather use threadpoolctl as little as possible until we understand better the cause of this aforementioned deadlock [CLS] [SEP] It could very well be the case that we found thread safety bugs in those openmp runtimes and if it's the case we will report time so that they can be fixed upstream [CLS] [SEP] In the mean time the safe way to control the number of threads: Setting the environment variable prior to the launch of the main process or prior to the launch of the worker processes as we now do in joblib master for the child processes [CLS] [SEP] The other option would be to have our scikit learn Cython parallel loops explicitly pass an explicit int value to the num threads parameter [CLS] [SEP] scikit learn could maintain a global variable shall it be threadlocal [CLS] [SEP] that would be used by default and could be set globally [CLS] [SEP] Alternatively we could expose CODESCODES style parameters on a per estimator basis or both [CLS] [SEP] FYI: joblib 0 [CLS] [SEP] 14 [CLS] [SEP] 0 with the fixed version of overscription for the case Python process vs native threadpools is out: LINKLINK OK so my current understanding is the following: CODESCODES will set the CODESCODES env variable of the spawned child processes to a value that avoids over subscription by default. Users can still control the number of openMP threads by either setting CODESCODES which will take precedence, or by using CODESCODES as a context manager and passing CODESCODES. Is that correct [CLS] [SEP] If it is, I'm all in for option 5 not expose anything, and of course properly document this somewhere in the user guide [CLS] [SEP] There's one limitation [CLS] [SEP] It does not include the threading backend [CLS] [SEP] This is why we are working on threadpoolctl [CLS] [SEP] Based on last meeting, option 5 sees to be the preferred option for most places where we want to introduce CODESCODES [CLS] [SEP] So I think I can close this discussion and we can open new ones for specific estimators like hgbt or kmeans [CLS] [SEP] Feel free to re open if you disagree [CLS] [SEP] I'll open a PR soon for documentation. 