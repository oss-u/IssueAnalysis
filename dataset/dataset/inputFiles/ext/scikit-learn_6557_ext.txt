I frequently get asked about post pruning [CLS] [SEP] Often using single trees is important for interpretability, and post pruning can help both interpretability and generalization performance [CLS] [SEP] I can give this a shot thumbs up. The title of this issue is related LINKLINK [CLS] [SEP] This issue LINKLINK also seems to be proposing prune method to the tree but the code has considerably changed in the meantime I suppose [CLS] [SEP] On the topic of interpretability, there has also been published work on creating single decision trees from random forests [CLS] [SEP] As counter intuitive as this sounds, due to the non optimality of standard decision tree algorithms, this can apparently give single decisions tree that are both interpretable and also better classifiers regressors than you would get otherwise [CLS] [SEP] amueller Could you please explain this issue a little bit [CLS] [SEP] I want to work on this [CLS] [SEP] actually maybe the first thing to do would be to add a stopping criterion based on mutual information [CLS] [SEP] lesshaste can you give a citation [CLS] [SEP] I had been thinking about that recently, damn, obviously it has been done. amueller It has been a long time since I looked at this but: Section 2 [CLS] [SEP] 2 of. Rule Extraction from Random Forest: the RF+HC Methods by Morteza Mashayekhi and Robin Gras has all the relevant citations I know [CLS] [SEP] Please let me know if you don't have access to this paper [CLS] [SEP] On an only slightly related note, I remember looking at LINKLINK with some interest [CLS] [SEP] An easy addition would be to stop the construction when CODESCODES, for example when the weighted impurity CODESCODES for the best split CODESCODES becomes less than some user defined threshold CODESCODES [CLS] [SEP] I think this can also be beneficial for some ensemble algorithms like gradient boosting although it's typically not increasing the predictive accuracy for bagging style ensembles for example random forests [CLS] [SEP] Pruning might also be useful to save some memory and decrease prediction times a bit [CLS] [SEP] amueller others: jmschrei and i met to discuss the issue of post pruning a few weeks ago, and we were unsure of how it would fit in the current scikit learn API [CLS] [SEP] Generally, post pruning needs a validation set, but this doesn't seem to fit nicely with how the library is currently organized namely, issues like creation origin of the validation set and whether this would be an argument to fit or a separate method come to mind [CLS] [SEP] How were you thinking of this being implemented from an API point of view [CLS] [SEP] For now, I'm working on looking through the CODESCODES code and adding the early stopping criteria based on weighted impurity for GSoC while thinking about MAE [CLS] [SEP] Seems like there was some discussion about API issues in LINKLINK, but the tree module has changed quite a bit and perhaps we should have the discussion again [CLS] [SEP] For one, we have parameters like CODESCODES and CODESCODES to control growth of the tree a bit [CLS] [SEP] I would be inclined to set up pruning as a separate method [CLS] [SEP] In fact, a separate function that takes a tree and returns the pruned version would be ok to begin with [CLS] [SEP] This is the approach which is used in LINKLINK to generate optimized code for trees [CLS] [SEP] nelson liu With regard to the regularization options, they sometimes lead to subtrees with all nodes belonging to the same class [CLS] [SEP] While it is not pruning in the sense of regularization, it would be nice to have a function to get rid of these extra nodes that only add computational cost during prediction [CLS] [SEP] When would you ever get a subtree with all nodes predicting the same class [CLS] [SEP] You wouldn't make a split if there wasn't a gain in your criterion [CLS] [SEP] In essence, what I'd like is a 'warm start' like method for building a tree, like we have a warm start for building a random forest [CLS] [SEP] You should have a method to add a single node to a tree, but still get a valid estimator before and after adding the node [CLS] [SEP] This would allow users to evaluate the trees performance on the evaluation set as they build it, just like you can add trees to a random forest, evaluating its performance on the validation set each time [CLS] [SEP] This shouldn't be terribly difficult functionally to add, the biggest issue is just the API for this [CLS] [SEP] ogrisel glouppe do you have any thoughts [CLS] [SEP] jmschrei. I meant that a split of 90:10 into 80:0 & 10:10 won't change the performance if the tree stops there [CLS] [SEP] pixelou another important thing to consider is whether we want this to be useable with CODESCODES [CLS] [SEP] a separate method would break that i think [CLS] [SEP] nelson liu Sorry I wasn't clear: I meant that most of the hard work is to write the pruning procedures themselve [CLS] [SEP] Integration shouldn't be too hard but I'm not the one doing it obviously: One the top of my head, I see several options for integration: options given to the tree constructor are then taken into account by CODESCODES. options are given to CODESCODES directly. a separate CODESCODES or CODESCODES method has to be called explicitely. a separate CODESCODES or CODESCODES function takes the tree and returns another pruned tree. It's up to you to decide on this, but I think one can write a separate private class [CLS] [SEP] method for pruning and make it available to the API as one of the above solutions [CLS] [SEP] Note that I have just given the points above without further thinking ; [CLS] [SEP] 2 and 4 are clearly not the sklearn way of doing things, and you just mentioned how 3 can be a problem [CLS] [SEP] The first option of those seems the most consistent with sklearn [CLS] [SEP] ameuller glouppe GaelVaroquaux any opinion on the api for post pruning [CLS] [SEP] I see these three options, the first of which seems the best to me [CLS] [SEP] Just wanted to get some clarification before I start to think about working on this: options given to the tree constructor are then taken into account by [CLS] [SEP] fit. a separate CODESCODES or CODESCODES method has to be called after fitting. a separate CODESCODES or CODESCODES function takes the tree and returns another pruned tree. Yes, the first option is certainly the one that fits best with our API [CLS] [SEP] Isn't it pre pruning if you do it at fit [CLS] [SEP] Aren't we supposed to check the complexity of the tree and use post pruning to reduce it [CLS] [SEP] Start with the third option, then decide whether the others are appropriate [CLS] [SEP] thumbs up for that. by which I mean, the example code might help decide. It's not pre pruning if you do it at fit, if you build the full tree and then go backwards and remove nodes [CLS] [SEP] I agree with glouppe that the first one is the best option, but I also agree with jnothman that since the code will rely on a prune tree method anyway it may be better to create a standalone thing during the development stages [CLS] [SEP] I assumed that the user would want to select whether to post pruning or not based on the built tree [CLS] [SEP] For instance this is how LINKLINK by a separate CODESCODES method which goes backward and trims the tree [CLS] [SEP] Presumably you'd want validation set performance to guide if how to prune the tree [CLS] [SEP] My concern was if users want a peek at the tree between the CODESCODES and post prune operations, they will have to fit again without post pruning enabled [CLS] [SEP] But as you say maybe that is not necessary if the need for post pruning is based on the validation set performance and not on user's decision [CLS] [SEP] nelson liu Can I work on this if you are not [CLS] [SEP] sure go ahead, it could be a good way to get familiar with the tree code [CLS] [SEP] It's definitely not an easy task though, so feel free to ask if you need assistance [CLS] [SEP] Thank you nelson liu [CLS] [SEP] So is this been implemented or still remaining [CLS] [SEP] datavinci it is still remaining [CLS] [SEP] If nobody is working on this feature, I would like to do this on myself or with dalmia [CLS] [SEP] OK [CLS] [SEP] It seems that this is stalled, so go ahead [CLS] [SEP] I don't think dalmia has availability these days [CLS] [SEP] I'm also seeing large subtrees predicting the same class [CLS] [SEP] Even naive pruning would improve interpretability in cases like these and it would have no effect on the predictions [CLS] [SEP] cjmay it would change the predicted probabilities [CLS] [SEP] But yes, adding any sort of pruning would be nice thumbs up It would be nice to see benchmarks of post pruning vs pre pruning [CLS] [SEP] I think min impurity decrease is a relatively reasonable stopping criterion [CLS] [SEP] Or you could grid search CODESCODES n leafs CODESCODES [CLS] [SEP] do people usually grid search the pruning parameter [CLS] [SEP] I would imagine so [CLS] [SEP] It would be interesting to see how often that gives different outcomes [CLS] [SEP] amueller oh, I see [CLS] [SEP] Look forward to it [CLS] [SEP] For some of the ensemble methods this could be worked in naturally [CLS] [SEP] If there is bagging then the OOB sample could be used for validation and pruning [CLS] [SEP] This would be like how oob improvement is calculated in LINKLINK Is anyone working on this right now [CLS] [SEP] Not to my knowledgeis there anybody done it [CLS] [SEP] is anyone have done it that could share your code [CLS] [SEP] cost complexity pruning based on sklearn [CLS] [SEP] decisiontreeclassifier [CLS] [SEP] Found this: LINKLINK. Not sure if it is exactly cost complexity pruning [CLS] [SEP] zanderbraam it's not CCPHi, all, I have implemented: CCP Cost Complexity Pruning Algorithm on. sklearn CART Classification model in Python, ECP Error Complexity Pruning Algorithm on. sklearn CART Regression model in Python, here's the link: LINKLINK. you may like it [CLS] [SEP] appleyuchi thanks [CLS] [SEP] I find it a bit hard to follow the structure of the code, in particular given that file names and comments are in Chinese [CLS] [SEP] There also seems to be a lot of duplicate code [CLS] [SEP] I don't work with DT anymore, but has anyone had a look at LINKLINK [CLS] [SEP] It seems relevant to this issue [CLS] [SEP] A tree is just a very small forest [CLS] [SEP] Can't this implementation scale down to trees [CLS] [SEP] appleyuchi I'm not sure if I follow what you're saying but we will not adopt an implementation based on going to JSON [CLS] [SEP] Any implementation in scikit learn would have to work directly with the scikit learn data structures [CLS] [SEP] appleyuchi thanks for your efforts [CLS] [SEP] Regarding how tricky it may be to implement this feature, we do recognize that touching the tree code base is not necessarily a trivial task [CLS] [SEP] There have been efforts to change that and have a more readable implementation [CLS] [SEP] Besides, this issue isn't labeled as Easy for that exact reason [CLS] [SEP] I hope you find other issues that you may be interested in and you keep the good work on them thumbs up I am working on this issue with a cost complexity pruning CPP algorithm [CLS] [SEP] I see several tests that can be used to check tree pruning: Increasing alpha in CPP should result in smaller or equal number of nodes [CLS] [SEP] Make sure the pruned tree is actually a subtree of the original tree [CLS] [SEP] What other tests would be appropriate for tree pruning [CLS] [SEP] appleyuchi Thanks for sharing [CLS] [SEP] My concern is that, even as a Chinese, it's still pretty hard for me to follow the code [CLS] [SEP] I guess it is better to have the code more modularized so that others can apply your implementation on any data sets [CLS] [SEP] zhenyu zhou. Because almost all of you did NOT ever read the book《classification and regression trees》carefully [CLS] [SEP] The first author of this book has already passed away so you can not contact him for questions [CLS] [SEP] The defect of this book is discussed in. LINKLINK. or. LINKLINK. which point out that CCP ECP algorithm cross validation will fail for unbalanced and small datasets, you should understand the above academic materials before you implement it [CLS] [SEP] I analyzed and summarized the defect in: LINKLINK. The Github link I provide for CCP ECP is just application style, NOT from sklearn's bottom variable style, the latter will be much more efficient and faster. so they reject [CLS] [SEP] It can be applied on many datasets I have tested, I guess you even have NOT clicked in and read the instruction in the Github link. appleyuchi. You are ABSOLUTELY WRONG [CLS] [SEP] To make it short, I just have one question for you: do you provide a clean API, like sklearn did [CLS] [SEP] You shouldn't expect everyone to read every detail of your code before using it, and blame others for that, if you treat your code as a library as sklearn [CLS] [SEP] That's one of the reasons they reject the code [CLS] [SEP] Consider it when you are using other libraries, take sklearn as an example, if it only has a bunch of self contained code for experiments, which requires a certain input format, without a general framework [CLS] [SEP] You need to carefully examine the library code to determine how to split the main logic out and apply to your dataset [CLS] [SEP] Will you use it [CLS] [SEP] I acknowledge that the experiment is interesting, but it is not a library [CLS] [SEP] I just want to kindly post sth in my mind to improve the code but it is too hard [CLS] [SEP] zhenyu zhou. you misunderstand it, what I said is material, not code [CLS] [SEP] Note that it refers to a book, NOT the code I have written [CLS] [SEP] I mean《classification and regression trees》is a famous book, not the code I wrote, so that's not blame [CLS] [SEP] and you refers to new contributors, NOT the existing member of sklearn [CLS] [SEP] again, you should read the book carefully before you implement it [CLS] [SEP] zhenyu zhou. The API style has been dicussed several months ago when you have NOT been here, again, what I have implemented is application style, NOT from sklearn bottom data structure. Good Luck [CLS] [SEP] LINKLINK. ps: Notification from this issue has been cancelled, because I'm busy [CLS] [SEP] me will NOT take effect any longer [CLS] [SEP] If you have any question, contact me via Email please [CLS] [SEP] do you have any code for this [CLS] [SEP] Thanks. 