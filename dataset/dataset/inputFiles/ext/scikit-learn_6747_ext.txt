I've recently see more people using balanced accuracy for imbalanced binary and multi class problems [CLS] [SEP] I think it is the same as macro average recall [CLS] [SEP] If so, I think we might want to create an alias, because it is not super obvious, and maybe add a scorer [CLS] [SEP] Also see LINKLINK. Of course it is [CLS] [SEP] Why didn't I think of that [CLS] [SEP] I think creating an alias and a scorer is a good idea, with the constraint that it applies to binary problems [CLS] [SEP] It could also be calculated per label for multilabel problems and then potentially macro averaged [CLS] [SEP] I think this is moderate seeing as it involved that data format checking and narrative docs [CLS] [SEP] Following from LINKLINK. Balanced accuracy is where you calculate accuracy on a per class basis, then average all of those accuracies [CLS] [SEP] Here is a paper that introduces it: LINKLINK. But by accuracy on a per class basis, you must mean recall ; and we're still only considering the binary classification case [CLS] [SEP] Here's the definition that we use: LINKLINK. In the multiclass case, we simply consider the current class we're calculating accuracy for to be CODESCODES and the other classes to be CODESCODES, for example, a one vs all configuration [CLS] [SEP] That allows us to calculate accuracy normally for each class [CLS] [SEP] Indeed most of the papers that discuss balanced accuracy do so only in the context of binary classification, but it seems reasonable to expand it to the multiclass case in this manner [CLS] [SEP] At a first glance, I don't think that multiclass definition is really appropriate [CLS] [SEP] But I'll think about it at little [CLS] [SEP] I suspect macro averaged recall reflects better the intentions of balanced accuracy [CLS] [SEP] I believe the same procedure is used with LINKLINK [CLS] [SEP] From my understanding, macro averaged recall [CLS] [SEP] balanced accuracy in the multiclass case, only in the binary classification case [CLS] [SEP] Thus I don't think we should label macro averaged recall as balanced accuracy [CLS] [SEP] Balanced accuracy is a separate metric that places more importance on TNR relative to macro averaged recall in multiclass classification problems [CLS] [SEP] macro averaged AUC is explicitly for multilabel [CLS] [SEP] Problem is that I'm not so. sure what TNR means in a multiclass context, or why OvR transform makes. sense for that [CLS] [SEP] I've had a go at simplifying the overall score for a.3 class classification problem, by hand, but haven't got far enough that I. can see how its formula is interesting and value [CLS] [SEP] On 11 June 2016 at 06:04, Randy Olson wrote: Section 2 [CLS] [SEP] 18 in the attached paper describes the mathematical formulation of balanced accuracy [CLS] [SEP] The key part is that you calculate balanced accuracy using a one vs all configuration [CLS] [SEP] So you start with the first class CODESCODES, where you treat the data as a binary classification task such that all records labeled CODESCODES are CODESCODES and all other classes are CODESCODES [CLS] [SEP] Calculate accuracy as you normally would [CLS] [SEP] Then for the next class CODESCODES, repeat the same process except all instances labeled CODESCODES are CODESCODES and the rest are CODESCODES [CLS] [SEP] And so on [CLS] [SEP] Once you have the per class accuracy for every class, average them and that is balanced accuracy [CLS] [SEP] LINKLINK. I'm not persuaded that this is the right thing to do, but I am beginning to be persuaded that this is a logical extension that diverse people are assuming is legitimate [CLS] [SEP] We've worked through the math and logic behind it several times and it checks off for us, but that doesn't mean we're right [CLS] [SEP] I'm very curious to hear why it may not be the right thing to do [CLS] [SEP] The redundancy of information inherent in including both one class's true positives and another's true negatives makes me a little uncomfortable [CLS] [SEP] However, the multiclass case has some niceties: such a macro average over a binary problem actually results in the same formula as the non multiclass treatment; and empirically it seems that random class assignment from a fixed distribution in the multiclass case will still yield a score of 0 [CLS] [SEP] which is pretty neat [CLS] [SEP] I'm coming to appreciate that this may be an appropriate extension. Yes exactly [CLS] [SEP] As with all metrics, balanced accuracy is just an indirect method of capturing what is good performance for our models [CLS] [SEP] As a metric in the multiclass case, balanced accuracy puts a stronger emphasis on TN than TP, at least when compared to macro averaged recall [CLS] [SEP] And as you point out, balanced accuracy has the nice feature that 0 [CLS] [SEP] 5 will consistently be as good as random, with plenty of room for models to perform better 0 [CLS] [SEP] 5 or worse 0 [CLS] [SEP] 5 than random [CLS] [SEP] It'd be great if we could get balanced accuracy added as a new sklearn metric for measuring a model's multiclass performance [CLS] [SEP] If this is the only paper using this definition, I'm not sure we should include it [CLS] [SEP] Where did you get it from [CLS] [SEP] That paper [CLS] [SEP] So LINKLINK says. The paper that rhiever claims introduces the metric does so only for binary, right [CLS] [SEP] The second paper indeed uses the average over the sensitivity + specificity 2 over classes [CLS] [SEP] But I think that is not the standard definition [CLS] [SEP] As long as we can't come up with what the standard definition is if any, I don't think we should add it under this name [CLS] [SEP] We can add CODESCODES or something [CLS] [SEP] This is somewhat similar to macro average f1, right [CLS] [SEP] You also first used a different definition of average precision in your code [CLS] [SEP] That was a bug that we fixed [CLS] [SEP] The definition of balanced accuracy for the multiclass case is in the Urbanowicz paper [CLS] [SEP] The original paper I linked to was only for the binary case, yes [CLS] [SEP] For the Master's thesis that you linked, that definition of balanced accuracy is under a section describing Two Class Evaluation Measures, for example, binary or multilabel classification [CLS] [SEP] I don't think that thesis discusses balanced accuracy in the multiclass case [CLS] [SEP] It's valid to say that balanced accuracy is the macro averaged recall in the binary case [CLS] [SEP] In the binary case, it works out the same mathematically as calculating accuracy on a per class basis then averaging those two accuracies [CLS] [SEP] We're simply proposing an extension to the definition of balanced accuracy to also cover the multiclass case [CLS] [SEP] I'm happy for you to veto this amueller, after my change of heart [CLS] [SEP] I was persuaded by the following features: treating a binary problem as multiclass gives identical results for example CODESCODES. the valuable property of ROC AUC, that regardless of class prevalence a random prediction will produce a fixed score in the limit holds true in the multiclass transformation. perfect classification performance is still 1 [CLS] [SEP] 0 so introducing random error decreases the score towards 0 [CLS] [SEP] etc [CLS] [SEP] These properties are much more persuasively meaningful than any properties of macro averaged P R F in the multiclass case [CLS] [SEP] This extension has also been reinvented in a few places, suggesting it is sought after and reasonable [CLS] [SEP] LINKLINK another paper from the AutoML challenge that defines balanced accuracy for the multiclass case [CLS] [SEP] They use a similar definition, with the only difference being the normalization procedure that they apply at the end where they ensure that as good as average accuracy 1 N, where N is the number of classes [CLS] [SEP] Thanks for the reference, though I think it muddies the water a bit pending a look at their implementation [CLS] [SEP] It's far from clear to me that the accuracies they are averaging class wise in the multiclass case incorporate sensitivity and specificity [CLS] [SEP] By default I assume they mean standard Rand accuracy over each binarization, although this seems a strange choice given that then the binary problem needs to be, as they say, a special case [CLS] [SEP] That correction for chance under a uniform prior allows for an everything incorrect response to score zero assuming I'm correct about their use of Rand accuracy [CLS] [SEP] I don't think your score allows 0, except in the case where the only predicted classes are not in the gold standard [CLS] [SEP] Then classification at random in their measure does not yield a nice score that is invariant of the number of classes, nor one invariant to the distribution of those classes in the gold standard [CLS] [SEP] CODELCODEL. produces. CODELCODEL. Same results if CODESCODES is uniformly sampled, where above sampling is weighted [CLS] [SEP] To make things murkier, that metric description is repeated LINKLINK and hyperlinked to LINKLINK but I don't see the relevance of the latter [CLS] [SEP] Ah [CLS] [SEP] Now I see the relevance [CLS] [SEP] They've actually LINKLINK [CLS] [SEP] Which means that, indeed, the chance correction they propose results in a score of 0 for random predictions [CLS] [SEP] It also means that binary classification isn't actually a special case, despite what they say [CLS] [SEP] But they also have other nonsense in that paper such as We also normalize F1 with F1: F1 R 1 R, where R is the expected value of F1 for random predictions for example R 0 [CLS] [SEP] 5 for binary classification and R 1 C for C class classification problems [CLS] [SEP] Expected value of F1 for random predictions is the prevalence of the positive class in the binary case, not 0 [CLS] [SEP] So while they attempt to throw a principled kitchen sink of evaluation metrics at the task, I'm not sure they are coming from a place of critical expertise, at least in their description of the measures [CLS] [SEP] Still, the fact that they describe something different to your metric with the same name makes it a bit uncomfortable [CLS] [SEP] we can always call it macro average accuracy that's what we're talking about, right [CLS] [SEP] and say that balanced accuracy can mean macro average accuracy or macro average recall depending on who you ask [CLS] [SEP] LINKLINK introduces class balanced accuracy which divides by the max of the row and column norm for each class, so it's always smaller than macro average precision and macro average recall [CLS] [SEP] Ok, send a message to the authors of the AutoML paper to ask them to clarify though one of them immediately bounced thumbs down [CLS] [SEP] rhiever is there any other paper that does multi class balanced accuracy [CLS] [SEP] I only know of LINKLINK that introduces balanced accuracy for multiclass problems, under Section 2 [CLS] [SEP] 18 [CLS] [SEP] Of course, we've used it heavily in the work in our lab since that paper, but we just refer to that paper for the detailed metric description [CLS] [SEP] thanks [CLS] [SEP] That one has 15 citations, 3 of which are you thumbs up oh it's by one of your co authors [CLS] [SEP] Hm looks like most of the 15 are self cites thumbs down I'm not claiming that the paper introducing the multiclass balanced accuracy is a 1k+ citation paper, but it is a useful metric that's widely used in our area [CLS] [SEP] LINKLINK is more commonly cited for balanced accuracy, though that paper only introduces the metric for binary classification problems [CLS] [SEP] It would be good if we had some evidence that it's widely used in the multi class setting [CLS] [SEP] I don't have traditional academic evidence for example, papers with high citation counts for that [CLS] [SEP] Do I get this correct [CLS] [SEP] You do not implement it, as you do not know the name for it [CLS] [SEP] And you need evidence, that it is used [CLS] [SEP] I just wanted to use it just implemented it myself [CLS] [SEP] It is a rather simple concept, isn't it [CLS] [SEP] It is class balanced accuracy [CLS] [SEP] Even wikipedia knows about it in the binary case: LINKLINK. The multiclass case is a straight forward extension [CLS] [SEP] Chrisinger the multi class case is a straight forward extension, for which two different straight forward interpretations exist that are very different [CLS] [SEP] The same name is used for two different algorithms [CLS] [SEP] One of them is implemented as macro average recall in scikit learn [CLS] [SEP] Chrisinger also see 10040 with pointers to the relevant literature and an explanation [CLS] [SEP] amueller Thank you for the hints [CLS] [SEP] This is why I was confused and did not find it when looking for it [CLS] [SEP] I would not use the term recall, as in the two class case it is limited to the positive class [CLS] [SEP] This is why I would never look there [CLS] [SEP] My thought process went like this: Accuracy is a great measure, giving me an estimate, on what is the chance that when I give my classifier a random sample, I get the correct response [CLS] [SEP] For very unbalanced classes, classifiers tend to predict the most frequent class [CLS] [SEP] We can cope with this with the balanced accuracy as described for example LINKLINK [CLS] [SEP] We lose the general estimator, but we get one that has value of 0 [CLS] [SEP] 5 for predictions by chance [CLS] [SEP] 0 is still always wrong, 1 is still perfectly classified [CLS] [SEP] And this measure can be extended easily to the multiclass case [CLS] [SEP] Arguing with a confusion matrix I want an average true prediction rate [CLS] [SEP] Maybe this term helps you with your naming problem [CLS] [SEP] With n classes, 1 n should be achieved by random guessing or by predicting always the same class [CLS] [SEP] I am currently stuck with this argumentation what is the second algorithm for the multiclass extension [CLS] [SEP] amueller jnothman rhiever I realize it's a bit too late to weigh in on this debate over the definition of balanced accuracy, but my understanding of the definition is consistent with Randy's mean of per class accuracy [CLS] [SEP] For what it's worth, it's also the definition that Chalearn uses in their AutoML competition: LINKLINK. It seems like there are enough references to back up either definition [CLS] [SEP] which is unfortunate and confusing for the ML community [CLS] [SEP] As for H2O multiclass metrics, we have mean per class error, but don't use the term balanced accuracy anywhere and don't currently support macro average recall as a default metric [CLS] [SEP] I don't think it's too late [CLS] [SEP] Our implementation hasn't been released yet [CLS] [SEP] But it could be next week [CLS] [SEP] We can roll back multiclass balanced accuracy support [CLS] [SEP] I would be keen to release it but emphasise that other people mean something else by the term [CLS] [SEP] For good or bad, this is true of many metrics [CLS] [SEP] But I'm being a bit dogmatic on this front which is not entirely Scikit learn style, as we prefer to release things that are well established [CLS] [SEP] Since I clearly have a stance on this see my arguments from LINKLINK, I'd be interested to know what you think your definition of multiclass balanced accuracy tells you beyond other metrics; and how it bears any correspondence to the agreed upon definition of binary balanced accuracy [CLS] [SEP] Beyond my arguments above, I also note that macro averaged recall is identical to accuracy with inverse prevalence weightings per sample [CLS] [SEP] That is: CODELCODEL. You can very clearly see here why balanced accuracy, by my definition, is indeed balanced + accuracy [CLS] [SEP] I highly recommend macro average recall as a default metric for multiclass problems thumbs up To summarise my arguments: macro average recall reduces to binary balanced accuracy trivially. macro average recall is identical to accuracy with balanced sample weights. macro average recall has the nice property of ROC AUC that it obtains a constant value for random predictions. On the other hand, I think macro averaged accuracy will give a lot of prominence to small classes, because you will get a very high accuracy due to the large negative class for their contribution [CLS] [SEP] I don't see how that's helpful, or akin to binary balanced accuracy [CLS] [SEP] jnothman I don't have any strong opinions on which metric macro average recall vs macro average accuracy is better to use [CLS] [SEP] My comments refer only to the fact that the term balanced accuracy is not well defined in the machine learning community, so it's potentially problematic to name a metric method after this term [CLS] [SEP] In particular, using the term accuracy when referring to recall seems to contribute to this confusion [CLS] [SEP] I don't see any drawbacks of using an explicit method name like CODESCODES instead of CODESCODES to avoid confusion by users and the larger ML community, so I guess that would be my position on it [CLS] [SEP] As to what is a good default metric in multiclass classification also a hard question and outside of the scope of this ticket perhaps [CLS] [SEP] On Kaggle, multiclass competitions almost exclusively use logloss though that also causes issues with highly imbalanced data, so then there's the debate about using weighted logloss and how to weight [CLS] [SEP] Thanks for the reply [CLS] [SEP] Personally, I find the knowledge that macro recall is. accuracy with balanced sample weights persuasive regarding the name [CLS] [SEP] The. reason to offer it by this name is that people are looking for a multiclass. metric with the desirable properties of the binary balanced accuracy [CLS] [SEP] I. think this metric satisfies those requirements, as well as the name being. apt [CLS] [SEP] I am personally in favour of being prescriptive in this case, encouraging. users to stop calling 'balanced accuracy' what is indeed a different. metric [CLS] [SEP] But that may be a lone opinion [CLS] [SEP] I'd like others' input qinhanmin2014 [CLS] [SEP] rth [CLS] [SEP] amueller [CLS] [SEP] adrinjalali [CLS] [SEP] Our implementation seems reasonable from my side, so more documentation to note the different definitions seems to be enough from my side [CLS] [SEP] The controversial thing is the multi class definition of a metric, so maybe we don't need to worry too much here [CLS] [SEP] Inclusion. The way I see it, scikit learn wouldn't include this proposal if it were a regression or a classification method [CLS] [SEP] The method would be encouraged to be in CODESCODES and we'd advertise it in our docs appropriately [CLS] [SEP] However, since the industry has moved faster than academia in this case hence the lack of nicely cited literature despite the widespread usage and uses multiclass classification models regularly, there's a need for good metrics, and the need for a a balanced one is obviously felt [CLS] [SEP] That, plus the fact that one of the two interpretations of this score is included in enough ML libraries and competitions out there, seem to be convincing enough to include such a metric in CODESCODES [CLS] [SEP] However, due to the lack of academic support for either of the two definitions, I'd argue that we should either include both, or neither definitions [CLS] [SEP] I agree with jnothman's arguments for one being a more sensible option, but those arguments to me are arguments to have one as the default, and I wouldn't use the arguments as inclusion exclusion criterion [CLS] [SEP] Default Score. As for a default score I wish I could say metric, I'd agree with ledell that at least in the industry CODESCODES is a way more popular choice this of course could be subjective to one's experience [CLS] [SEP] Name. So far as the name goes, I'm happy with calling it a score for the following reasons: CODESCODES is a name which sounds intuitive to people looking for a balanced score, and we seem to agree that it's doing a sensible thing, which is a good enough of a reason for most parts of the industry [CLS] [SEP] CODESCODES in this context is a term which is usually if not almost always used for a heuristic in academia at least the parts of academia I've seen, for example you're usually not required to have a mathematical proof for why you're using a certain score again this may be subjective [CLS] [SEP] F1 score the name seems to be an exception, but most academics I know prefer the term F1 measure anyway [CLS] [SEP] Summary. I guess what I personally would prefer is the following please excuse probably the bad function names: CODELCODEL Thanks for your input Adrin [CLS] [SEP] I would really love to know what someone has discovered from macro average. accuracy in an imbalanced multiclass problem, or what justification there. is for it [CLS] [SEP] I really don't understand what it's for [CLS] [SEP] jnothman If you care about the accuracy of each class equally regardless of it's presence in the training set, then it's an appropriate metric to use [CLS] [SEP] It's been used in a variety of places papers, Chalearn AutoML competition, software such as H2O ; isn't that justification that it's useful to the community [CLS] [SEP] I don't know what you mean by the accuracy of each class [CLS] [SEP] The rand. accuracy of a rare class is dominated by agreement on the instances that. are negative for that class [CLS] [SEP] I don't think that corresponds to the. colloquial meaning of the accuracy of each class, but it is entailed by. rand accuracy on an imbalanced binary problem, which binarisation of. multiclass targets is bound to create [CLS] [SEP] So macro average over rand accuracies would seem to make the smaller. classes less important since their contribution to the overall score is. more likely to be close to 1 the less frequently the predictor chooses them [CLS] [SEP] There is no shortage of bad metrics that have been reported, for the sake. of working to a pre stated benchmark [CLS] [SEP] I wish I had time to investigate. where users of this metric obtained insights from it [CLS] [SEP] Definition: For each class, the percentage of instances that were correctly classified [CLS] [SEP] If you can't see why that might be useful to some people in some cases, maybe it would be worth reaching out to Chalearn to ask why they've chosen this metric for their competition [CLS] [SEP] Or email authors of the papers who use it [CLS] [SEP] I am not sure why you're focusing so much on the subjective question of whether it's a good metric or not [CLS] [SEP] Scikit learn provides R 2 which many people think is a terrible metric to use outside of GLMs [CLS] [SEP] but it's still a worthwhile feature of scikit learn because it's a metric some people like to use [CLS] [SEP] Likewise, it seems worthwhile to include the mean per class accuracy as a metric since people use it [CLS] [SEP] This conversation has really departed from the main issue of controversy which is that scikit learn is using the name balanced accuracy which has two contradictory meanings within the machine learning community instead of a more explicit name for the method [CLS] [SEP] Haven't followed this and I'm kinda busy, but this seems like a potential blocker, right [CLS] [SEP] ledell I think jnothman is concerned with what's a good metric because people use what's in sklearn [CLS] [SEP] People use R 2 for regression because it's the default in sklearn [CLS] [SEP] People use 10 trees in a random forest b c it's default in sklearn we are changing the latter, it's hard to change the former [CLS] [SEP] Basically sklearn is prescriptive just because of its wide use, for better or worse [CLS] [SEP] Honestly my conclusion from that would be that we force the user to pick, though [CLS] [SEP] Maybe having an option as adrinjalali suggests, and for the scorers strings only have CODESCODES macro average recall CODESCODES and CODESCODES macro average accuracy CODESCODES [CLS] [SEP] For the record, I think that log loss is a terrible metric for multi class classification [CLS] [SEP] If the true class is 0, these two have the same log loss: CODELCODEL. where the argmax gives a correct classification in the second case, but not the first [CLS] [SEP] Wasn't it the case that chalearn implemented something different in the code than what they said in the paper [CLS] [SEP] at least one of the ml competitions used weighted macro average recall [CLS] [SEP] amueller. Agreed [CLS] [SEP] to clarify, I don't have a preference on what the default metric should be for multi class problems my only concern is the use of a polysemous method name like CODESCODES to refer to only one of the two things this is the current status of the code in the release candidate [CLS] [SEP] If you have an option that allows switching between the two definitions, that seems fine to me [CLS] [SEP] Are there any other scoring methods that have a switch like this [CLS] [SEP] It does not look like it: LINKLINK. I have a preference there: in addition to the points raised by jnothman in LINKLINK, the macro average recall is falling back the the CODESCODES with a balanced dataset while this is not the case for the macro average accuracy [CLS] [SEP] So you are interested about the accuracy and do not want to correct for the class imbalancing [CLS] [SEP] Actually, I was wondering why there is no CODESCODES parameter in the CODESCODES [CLS] [SEP] I would find this option confusing [CLS] [SEP] We all agree that the literature lack of clarity regarding the definition of the metric and this option replicates the same fuzziness in the implementation [CLS] [SEP] So, I am fine with the current behavior and naming of the CODESCODES [CLS] [SEP] I don't see a problem on choosing one of the propose definition in the literature, document it properly, and warn about the controversy [CLS] [SEP] IMO, selecting the macro average recall seems the most appropriate choice equivalence with accuracy in balanced setting [CLS] [SEP] I would be inclined to also add an CODESCODES option to the CODESCODES [CLS] [SEP] This behavior corresponds to the other definition and could be added to the documentation as well [CLS] [SEP] FWIW, an alternative metric used in the imbalanced classification literature is the geometric mean of the per class recall [CLS] [SEP] Why would you find this confusing [CLS] [SEP] Indeed this would mean the implementation reflects the state of the signature and the understanding of the community [CLS] [SEP] There's no fuzzyness if there's two definitions for the same name [CLS] [SEP] And there's lots of literature on multi class metrics and we can go into that at some point, I think ledell makes a good point in being clear about what we implement and allowing alternatives [CLS] [SEP] Similarly we decided not to pick an averaging strategy in any of the multi class metrics and require users to explicitly pass an averaging strategy to clarify what they want [CLS] [SEP] Confusion might not be the right term but returning completely different statistics would surprise me and I am not sure that we can advise to choose either implementation [CLS] [SEP] In short, I am scared that users switch methods because the score obtained is higher [CLS] [SEP] I am also concerned for the string style for the metric [CLS] [SEP] Having CODESCODES and different strings will be difficult to document well [CLS] [SEP] Regarding the metric itself, alternative definitions which do not guarantee to obtain the same result than CODESCODES in a balanced setting seem weird to me [CLS] [SEP] I completely agree with this [CLS] [SEP] I am sure that we can make the documentation better and we should be opened to alternative methods, even if I have my concerns this time with the alternative CODESCODES [CLS] [SEP] That's what we do for the different averaging methods, right [CLS] [SEP] That's true. amueller is right here [CLS] [SEP] You've referenced the binary case, glemaitre [CLS] [SEP] Chalearn AutoML indeed implements macro average recall, adjusted so that random performance is 0: LINKLINK [CLS] [SEP] This is equivalent to our CODESCODES with CODESCODES [CLS] [SEP] I think we can safely eliminate Chalearn as a counter example to our implementation preference, ledell [CLS] [SEP] But we could explicitly note in our docs that adjusted True equates to Chalearn's [CLS] [SEP] I think we may have indeed contacted the authors at some point amueller obviously has a better memory of all this history than I do [CLS] [SEP] If we can presume that Chalearn's description was in error, and that some of the subsequent references to averaged accuracy are copying Chalearn's in error, can we let this go [CLS] [SEP] Can we please lead the community, and define the standard meaning of balanced accuracy because we have identified many arguments for this definition and several against alternatives, and put the discrepancy in the literature to rest [CLS] [SEP] We already do say in our documentation that adjusted True equates to the Chalearn implementation [CLS] [SEP] We do not note that their description is in error [CLS] [SEP] Should we [CLS] [SEP] Yes, I think we can have implement multiple definitions here [CLS] [SEP] thumbs up [CLS] [SEP] It's not so good but maybe we need to do so if we keep the name CODESCODES balanced accuracy score CODESCODES. What's the definition of CODESCODES macro average accuracy CODESCODES here [CLS] [SEP] I don't think we've provided users with references about CODESCODES macro average accuracy CODESCODES, and I can't find any references in the PR [CLS] [SEP] Also, I start to wonder whether it's good to regard class balanced accuracy as a multiclass definition of CODESCODES balanced accuracy score CODESCODES [CLS] [SEP] It seems that they are two different metrics See P46 of Mosley2013, where the authors compare CODESCODES balanced accuracy CODESCODES and CODESCODES class balance CODESCODES [CLS] [SEP] What's more, the authors clearly state that CODESCODES Balanced Accuracy is the Recall for each class, averaged over the number of classes [CLS] [SEP] CODESCODES in P25, am I wrong [CLS] [SEP] I'm too tired in several ways thumbs up to make a decision on this but I think it's the last remaining blocker [CLS] [SEP] I guess a new option will not block the new release [CLS] [SEP] Things we need to consider now is whether we need to change a name for current scorer [CLS] [SEP] Also, if we decide to implement macro average accuracy as another option, we might need to provide some references in the user guide [CLS] [SEP] I'm, FWIW, thumbs down on a new option [CLS] [SEP] I don't want to perpetuate the misreading of the Guyon et al Chalearn AutoML paper where they have inaccurately described their implementation [CLS] [SEP] I'm okay with having macro average accuracy available, only I don't know what use it is [CLS] [SEP] So the only reference we have for the so called CODESCODES macro average accuracy CODESCODES is Guyou et al [CLS] [SEP] 2015 [CLS] [SEP] If so, I might prefer to to close the issue and leave CODESCODES balanced accuracy score CODESCODES as it is [CLS] [SEP] I think by saying CODESCODES the average of class wise accuracy CODESCODES, they actually mean CODESCODES macro average recall CODESCODES [CLS] [SEP] I'll vote +0 maybe thumbs down to include it unless we can find some references which clearly define CODESCODES macro average accuracy CODESCODES [CLS] [SEP] At least I can't find any references in the PR [CLS] [SEP] The references to variant multiclass balanced accuracy are discussed in. model evaluation [CLS] [SEP] rst. jnothman Which entry [CLS] [SEP] Seems that class balanced accuracy and balanced accuracy from Urbanowicz et al [CLS] [SEP] 2015 are not the so called CODESCODES macro average accuracy CODESCODES discussed here [CLS] [SEP] Sorry [CLS] [SEP] This conservation has been thoroughly confused [CLS] [SEP] Partially because of. the time passed since we solved it and wrote it up [CLS] [SEP] No one refers to macro. averaged accuracy as balanced accuracy [CLS] [SEP] That is only a misunderstanding due. to Guyon et al [CLS] [SEP] We could provide an Urbanowicz style implementation but I think it's a. really poor reuse of the name balanced accuracy for something that has. nothing to do with it [CLS] [SEP] Binary balanced accuracy does not incorporate. precision, it incorporates specificity [CLS] [SEP] They account for the same kind of. error but are fundamentally different quantifications of that error [CLS] [SEP] Notably, the demonstrate denominator of specificity depends only on y true, while the denominator of precision depends on y pred [CLS] [SEP] It behaves very. differently depending on the biases of the classifier to particular classes [CLS] [SEP] Agree [CLS] [SEP] I don't think the definition from Urbanowicz et al [CLS] [SEP] 2015 is widely accepted, unless provided with more references [CLS] [SEP] jnothman Close the issue [CLS] [SEP] I'm happy to have it closed [CLS] [SEP] I think rhiever and ledell do, right [CLS] [SEP] but ok to leave this closed [CLS] [SEP] I'm happy to reopen if someone provides some references except for Guyou et al [CLS] [SEP] 2015. No, ledell cited macro averaged accuracy as what Guyon et al call balanced accuracy, and as something offered in H2O, but as mean zero one loss, not under the name balanced accuracy [CLS] [SEP] As far as I can glean from above rhiever has used the Urbanowicz definition, which I was incorrect above to say incorporates precision that was Mosley et al [CLS] [SEP] ; I need this mess like I need a hole in the head [CLS] [SEP] but rather is the average of binary balanced accuracies for each class [CLS] [SEP] Need I argue against this again [CLS] [SEP] 