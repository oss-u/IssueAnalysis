 6372 adds CODESCODES to CODESCODES [CLS] [SEP] It accepts a list of names of CODESCODES or substitutes with defaults and constructs feature name strings that are human readable and informative [CLS] [SEP] Similar support should be available for other transformers, including feature selectors, feature agglomeration, CODESCODES, and perhaps even PCA giving the top contributors to each component [CLS] [SEP] CODESCODES should be modified to handle the case where an argument is supplied [CLS] [SEP] A proposal for support in CODESCODES is given in 6424 [CLS] [SEP] Modelled on 6372, each enhancement can be contributed as a separate PR [CLS] [SEP] Note that default names for features are CODESCODES. CODESCODES amueller 6372. feature selection and randomized L1 yenchenlin1994. feature agglomeration yenchenlin1994. CODESCODES nelson liu 6431. scalers, normalizers and imputers: should be trivial like CODESCODES 6431. CODESCODES 6431. CODESCODES 6441. CODESCODES yenchenlin1994. PCA [CLS] [SEP] CODESCODES [CLS] [SEP] CODESCODES [CLS] [SEP] other transformers [CLS] [SEP] jnothman May I try this [CLS] [SEP] On which family of estimator [CLS] [SEP] Hi jnothman, I am interested in taking this issue [CLS] [SEP] Could you please suggest how I can get started on this issue [CLS] [SEP] I'll handle implementing this for CODESCODES for now, and we'll see if there's more classes to implement this in after I'm done thumbs up. jnothman I'll modify the CODESCODES [CLS] [SEP] Is feature agglomeration here refering to CODESCODES [CLS] [SEP] yenchenlin1994 I assume so [CLS] [SEP] nelson liu Thx [CLS] [SEP] If so, I would also love to implement it for CODESCODES [CLS] [SEP] I have added an extended list of transformers where this may apply and noted the default feature naming convention though maybe its generation belongs in CODESCODES. Hello jnothman, What should CODESCODES do when input features passed into CODESCODES is None [CLS] [SEP] CODESCODES doesn't suffer from this since it set both CODESCODES and CODESCODES during CODESCODES [CLS] [SEP] Maybe CODESCODES should set CODESCODES too during CODESCODES [CLS] [SEP] Fair question, which I don't currently have an answer for [CLS] [SEP] One option is for it to just return CODESCODES even if that means returning CODESCODES [CLS] [SEP] Oh and even if CODESCODES passed into CODESCODES of CODESCODES is not None, I guess what it can do is to return CODESCODES, which is the same with CODESCODES in this case [CLS] [SEP] yes, trivial, as noted in the issue description. On 24 February 2016 at 00:01, Yen wrote: Oh okay [CLS] [SEP] I will also do scalars, normalizers and imputers and Binarizer [CLS] [SEP] Will send a PR right away [CLS] [SEP] Thanks for your clarification [CLS] [SEP] Hello jnothman, about. Do you mean all classes listed here: LINKLINK. It seems that all these classes may be put into CODESCODES and therefore need CODESCODES too [CLS] [SEP] Please correct me if I'm wrong [CLS] [SEP] Thanks [CLS] [SEP] Yes, I mean those [CLS] [SEP] On 24 February 2016 at 17:44, Yen wrote: Hi everyone, if it is fine I too would like to work on this issue [CLS] [SEP] Would be helpful if the estimators which are currently worked on could be mentioned, so that I can try something which does not overlap [CLS] [SEP] Thanks [CLS] [SEP] I think I can also work on. maniteja123 from CODESCODES to the end of the issue description is not yet done. yenchenlin1994, thanks for letting me know [CLS] [SEP] jnothman It would be of great help if you could confirm if the output for PCA needs to have shape CODESCODES where each element is the input feature having the maximum contribution [CLS] [SEP] Should the case of multiple features having high contribution along one component be handled [CLS] [SEP] Thank you [CLS] [SEP] I'm really not sure about PCA [CLS] [SEP] Try make something useful [CLS] [SEP] If you think it. will be helpful to users to have names for projection style features, submit a PR [CLS] [SEP] There is definitely a component of art to this [CLS] [SEP] On 25 February 2016 at 01:12, Maniteja Nandana. wrote: Thanks for the reply [CLS] [SEP] My doubt is mainly about choosing dominant features and also that all the components are not equally significant [CLS] [SEP] Since multiple features can have almost same contribution along a component, there might be need for some threshold to figure out the number of input features to be considered [CLS] [SEP] Anyway I will create a initial PR with just the most dominant feature along the component and continue the discussion there [CLS] [SEP] Hope it is fine [CLS] [SEP] yenchenlin1994 one more question [CLS] [SEP] I am not sure how to handle this for CODESCODES and CODESCODES [CLS] [SEP] Have you worked already on all of these [CLS] [SEP] If you have already started working, will be waiting for your PRs thumbs up Thanks [CLS] [SEP] I've started, thanks [CLS] [SEP] for PCA I'm not sure if we want CODESCODES or if we want CODESCODES I can see both being useful, but the second one can become very verbose very quickly [CLS] [SEP] pca currently implements both and switches using a function parameter [CLS] [SEP] However, that will be hard to do in a pipeline, right [CLS] [SEP] Or do we have one argument for the whole pipeline and propagate it through [CLS] [SEP] Should we have this for all transformers in the decomposition module [CLS] [SEP] It's kinda weird to have it for PCA but nothing else [CLS] [SEP] I think this will be most helpful for feature extraction and feature selection, but possibly useful beyond. I'd actually would like to put an embargo on any of the. get feature names that are poping up everywhere, and have a design. discussion enhancement proposal, rather than fixing things here and. there [CLS] [SEP] I am having a hard time seeing the big picture across the entirety of. scikit learn [CLS] [SEP] For instance, last week, I was struggling with feature. names and the Imputer which was removing some features due to too many. missing data [CLS] [SEP] While these are not directly related problems, it would be. good to think about how we design a consistent API [CLS] [SEP] I think that the right way to tackle that is to list a set of example. problems, and possible solutions, and then break this into atomic tasks, which all get a PR [CLS] [SEP] WDYT [CLS] [SEP] I'm sorry, I didn't reply, because I felt that I needed to free time to. think about this to be able to have a constructive comment, but it didn't. happen, and I am drowning, so it won't happen before this week end [CLS] [SEP] We did discuss the design in another place before [CLS] [SEP] I think it's pretty straight forward for feature extraction and feature selection [CLS] [SEP] I'm unsure about the rest, but don't think that's a blocker [CLS] [SEP] There was discussion here: 5172 there seems to be decent consensus [CLS] [SEP] I think the tasks are: add the functionality to all transformers. add the functionality to pipelines. the questions are: what do transformers that create linear combinations do [CLS] [SEP] what do transformers that create arbitrary functions RBMs, kernel pca, spectral embedding do [CLS] [SEP] how does that fit with the RFE model [CLS] [SEP] For some reason, RFE, in contrast to other modle based feature selection methods, does not have a transform, and assumes that the model that is used for feature selection is the same that is used for predicting [CLS] [SEP] Maybe that's a design problem with RFE [CLS] [SEP] The open questions seems to be relatively unrelated to adding the functionality to the places where it's obvious what to do [CLS] [SEP] I think there is basically one use case: having a pipeline with a model at the end, find out what the coefficients feature importances mean [CLS] [SEP] example: CODESCODES. CODESCODES. CODESCODES. btw, I gave the use cases already in 5172. actionable input: do the obvious CODESCODES ones, and think about the linear non linear features ones [CLS] [SEP] GaelVaroquaux: Admittedly I was being a bit sneaky by putting together this issue [CLS] [SEP] I suspected that embargo was inevitable, but wanted to see what contributors and review came up with rather than design without implementations to critique [CLS] [SEP] I think we should avoid merging these PRs until we're ready, or perhaps merge them into a branch where we can continue to refine, but use the PRs to try to get a consensus on each family of transformers of what we think would be useful [CLS] [SEP] With amueller and jakevdp starting the ball rolling, I was hoping to approximate agility with this approach [CLS] [SEP] P. Along those lines, and replying to amueller's Should we have this for all transformers in the decomposition module [CLS] [SEP] It's kinda weird to have it for PCA but nothing else [CLS] [SEP] I suggested PCA as a testbed before someone got carried away with code duplication [CLS] [SEP] I think there is no doubt that in simple cases this is something that users have sought and will make model inspection somewhat more accessible [CLS] [SEP] I might go as far as to say we need something like this [CLS] [SEP] The details will be disputed, but I also think it's reasonable to allow CODESCODES to have parameters that tweak the output for example for decomposition, and which can be underscore prefixed in CODESCODES CODESCODES is unlike CODESCODES, CODESCODES, CODESCODES in that it does not involve sample correlated input, and has only parameters we don't care to set with model search [CLS] [SEP] the underscore prefixed version of the parameters sounds verbose but sensible [CLS] [SEP] Better than anything I could come up with [CLS] [SEP] So not merge 6372 [CLS] [SEP] I think this is a something we urgently need [CLS] [SEP] It looks to me all linear decomposition methods can have a common CODESCODES if they all use CODESCODES correctly [CLS] [SEP] The discussion has been open in 5172 since 9 month, with the only disagreement on whether the function should be public or private, as far as I can see [CLS] [SEP] I realized public would indeed be better, so I went ahead thumbs up. Not sure you actually mean 5172 there [CLS] [SEP] By urgently you mean you need it merged before the book goes to press [CLS] [SEP] p. Maybe thumbs up I tried to write a book chapter that explains feature extraction without it, but I was too embarrassed by the current interface of CODESCODES [CLS] [SEP] I actually use pandas for one hot encoding currently in the book because OneHotEncoder is soo bad I put vighneshbirodkar on a mission to fix that [CLS] [SEP] But also something I'm asked for basically every time I give a talk [CLS] [SEP] The two most common questions are why are categorical variables so hard and why is using column names so hard [CLS] [SEP] Well maybe additionally why is inspecting models so hard [CLS] [SEP] I meant these comments: LINKLINK LINKLINK. Sorry for nitpicking but what is wrong with RFE [CLS] [SEP] It has a transform that is inherited from CODESCODES right [CLS] [SEP] MechCoder you're right, I overlooked that [CLS] [SEP] GaelVaroquaux did you have time to think about the issue [CLS] [SEP] I keep finding that I need something like this [CLS] [SEP] I'm working with heterogeneous data like electronic medical records, with patient attributes, clinical notes, medical terminology entries [CLS] [SEP] I need to be able to inspect a model consisting of nested pipelines and feature unions [CLS] [SEP] I am sure that for other users, lacking feature name information leads to breaches of best practice: either giving up on model inspection, or not using pipelines and hence perhaps leaking data from test into train [CLS] [SEP] I very strongly believe we should go ahead and implement this, in lieu of a straightforward alternative [CLS] [SEP] The main problem I see with it is the arbitrary string based representation, and nesting of transformers annotating those string representations, and any backwards compatibility concerns with that [CLS] [SEP] GaelVaroquaux, will you consent to merging the enhanced CODESCODES with an Experimental: the API and output of this method may change in future versions tag [CLS] [SEP] And if we really want to hedge our bets, we can make this contingent on the user importing CODESCODES [CLS] [SEP] I'd also be most interested in kmike's opinion on what better feature name support for pipelines should look like, whether aiming for perfection or aiming for agility [CLS] [SEP] kmike's comment at LINKLINK might suggest that in practice an interface like CODESCODES saves computation and memory costs: CODELCODEL. Then a CODESCODES can with enough information stored only query the necessary constituent transformers [CLS] [SEP] But I'm sure this involves more scaffolding work than CODESCODES as proposed here does [CLS] [SEP] Hey, Here are the bits of structured information we missed recently [CLS] [SEP] Maybe they are a bit too specific, but anyways: For CountVectorizer, HashingVectorizer and TfIdfVectorizer we needed start, end spans which map feature names back to the input text [CLS] [SEP] It required quite a lot of copy paste to implement: LINKLINK, and then you need to pass this information using a side channel [CLS] [SEP] This allows to implement highlighting like that: LINKLINK. For FeatureHasher and HashingVectorizer there can be several sub feature names for a single dimension when recovering possible features from a corpus there can be collisions [CLS] [SEP] Each sub feature name also has a sign [CLS] [SEP] For each dimension we're showing such sub feature names sorted by their frequency in the corpus; if the top first sub feature name is negative then the whole feature is treated as negative, and signs of all other sub feature names are inverted [CLS] [SEP] Collisions are also truncated for display with an option to expand if there is too may of them [CLS] [SEP] For FeatureUnion it'd be nice to map feature names back to transformers, for example to know which transformer is responsible for a feature; currently this is done with CODESCODES [CLS] [SEP] Ideally, it'd be nice to have the whole chain, along with meta information, available in a structured form [CLS] [SEP] For pipelines it'd be nice to preserve meta information about feature names, for example spans from 1 if CountVectorizer is followed by TfIdfTransformer [CLS] [SEP] All of the above is unconvenient and hacky to implement with feature names as strings [CLS] [SEP] Also, as jnothman said, performance can be a problem sometimes for example if you're building feature names for HashingVectorizer then the dimension is huge, and most feature names but not all are empty or auto generated [CLS] [SEP] Another use case is FeatureUnion of several transformers where some of them don't have get feature names defined; that's nice to still be able to inspect feature names which are defined, and have some kind of auto generated names for undefined ones [CLS] [SEP] This It is not a super big deal, but it may cause several seconds delays in interactive usage; it could add up quickly if scikit learn start to use auto generated feature names everywhere [CLS] [SEP] That said, I see the appeal of plain string feature names; they are much easier to understand and implement [CLS] [SEP] Somehow this turned into an eli5 wishlist to scikit learn [CLS] [SEP] Hmm [CLS] [SEP] Can we leave that out of the picture for now [CLS] [SEP] Or do you just mean that this is part of a structured representation you would appreciate in eli5 [CLS] [SEP] I'm not sure what exactly you mean by this [CLS] [SEP] I proposed an age ago having an attribute on a CODESCODES to describe which output features come from which constituent transformer [CLS] [SEP] It's easy to get that information when CODESCODES is called, but not when CODESCODES is called without CODESCODES ; hence its design is trickier than it looks [CLS] [SEP] So you mean a structured feature description [CLS] [SEP] I can see the need for this, but designing it would be a big effort, and still best if it remains marked experimental [CLS] [SEP] I would rather have something that provides the user with information for basic cases, but could be expanded later [CLS] [SEP] Heh, right [CLS] [SEP] I don't have a good API feedback, so I just enumerated related problems we had [CLS] [SEP] Sure, this is just an example of structured feature name representation [CLS] [SEP] Actually it is no longer 'feature names' that's some structured object which describes where the feature came from, like a link to the transformer in case of FeatureUnion [CLS] [SEP] It could be a naming issue; structured representation could be irrelevant if we're talking only about feature names [CLS] [SEP] All of this can be expanded later by adding new methods which return information for example in lists of the same length as CODESCODES [CLS] [SEP] So yeah, structured representation is not a blocker for get feature names improvements [CLS] [SEP] For my work, I've created a module that monkey patches scikit learn transformers with a CODESCODES method that can generate feature names for a pipeline featureunion construction: LINKLINK Never mind that; I should have used CODESCODES there instead of monkey patching and have updated it accordingly [CLS] [SEP] More importantly, I've submitted a patch to eli5 which should handle explaining feature importances in pipelines LINKLINK [CLS] [SEP] I hope eli5, particularly with its prolific use of singledispatch, is able to adopt this with greater agility than scikit learn [CLS] [SEP] A flaw with this design: I would like to build a transformer which selects or excludes features by name [CLS] [SEP] It can be designed as a meta transformer which gets the transformed feature names from the base transformer [CLS] [SEP] However, doing this properly requires that the input feature names are known at fit time: CODELCODEL. For this application, it is necessary to pass the feature names alongside CODESCODES [CLS] [SEP] Can you get rid of that with a CODESCODES ColumnTransformer CODESCODES [CLS] [SEP] I guess the question is a bit whether it's always possible to have the CODESCODES ColumnTransformer CODESCODES be right at the beginning of the pipeline, where we still know the names positions of the columns [CLS] [SEP] kmike Are the structured annotations mostly needed because of the ranges [CLS] [SEP] Also, GaelVaroquaux any more opinions on this [CLS] [SEP] I doing the easy cases like feature selection and imputation which might drop columns, in addition to having some support in FeatureUnion and Pipeline and ColumnTransformer will be very useful [CLS] [SEP] amueller once feature names get more complex for example pca on top of tf idf, showing them as a text gets more and more opinionated, and maybe problem specific [CLS] [SEP] How concise should be a feature name, for example should we show only top PCA components how many [CLS] [SEP] or all of them [CLS] [SEP] Note the amount of bikeshedding jnothman got from me at LINKLINK [CLS] [SEP] It seems the root of the problem is that formatting a feature name is not the same as figuring out where feature comes from [CLS] [SEP] This is where structured representation helps [CLS] [SEP] Full information all PCA components, or start, end ranges in case of text vectorizers can be excessive for a default feature name, but it allows richer display: highlighting features in text, showing the rest of the components on mouse hover click [CLS] [SEP] kmike thanks for the explanation thumbs up Maybe doing strings first would still work [CLS] [SEP] For PCA I would just basically do CODESCODES pca1 CODESCODES, CODESCODES pca2 CODESCODES etc for now aka punt jnothman GaelVaroquaux should we include this in the Townhall meeting [CLS] [SEP] feature names DataFrames come together to some extent, so yes [CLS] [SEP] Yes. Any news [CLS] [SEP] kiros32 13307 i think CODESCODES can be added to this list. 