Some data transformations including over under sampling 1454, outlier removal, instance reduction, and other forms of dataset compression, like that used in BIRCH 3802 entail altering a dataset at training time, but leaving it unaltered at prediction time [CLS] [SEP] In some cases, such as outlier removal, it makes sense to reapply a fitted model to new data, while in others model reuse after fitting seems less applicable [CLS] [SEP] As noted LINKLINK, transformers that change the number of samples are not currently supported, certainly in the context of CODESCODES s where a transformation is applied both at CODESCODES and CODESCODES time although a hack might abuse CODESCODES to make this not so [CLS] [SEP] CODESCODES s of CODESCODES s also would not cope with changes in the sample size at fit time for supervised problems because CODESCODES s do not return a modified CODESCODES, only CODESCODES [CLS] [SEP] To handle this class of problems, I propose introducing a new category of estimator, called a CODESCODES [CLS] [SEP] It must define at least a CODESCODES method, which CODESCODES will call at CODESCODES time, passing the data unchanged at other times [CLS] [SEP] For this reason, a CODESCODES cannot also be a CODESCODES, or else we need to define their precedence [CLS] [SEP] For many models, CODESCODES needs only return CODESCODES [CLS] [SEP] For sample compression approaches for example that in BIRCH, this is not sufficient as the representative centroids are modified from the input samples [CLS] [SEP] Hence I think CODESCODES should return altered data directly, in the form of a dict with keys CODESCODES, CODESCODES, CODESCODES as required [CLS] [SEP] It still might be appropriate for many CODESCODES s to only modify CODESCODES ; if necessary, another CODESCODES can be chained that realises the weights as replicated or deleted entries in CODESCODES and CODESCODES [CLS] [SEP] I hear this positively after discussing this very same pb with MechCoder. can you write a few lines of code the way you would like to pipe something. like Birch with an estimator that supports sample weights [CLS] [SEP] I'm not sure about piping birch with sample weights, but BIRCH could be implemented as. CODELCODEL. Not that it's so neat, but it gives an example of the power of the approach [CLS] [SEP] CODESCODES simply takes the predictions of a method and returns it as the CODESCODES for the input CODESCODES [CLS] [SEP] I think we should list a few use cases to come up with an API that does the. job [CLS] [SEP] The code seems a bit too generic for a single use case, which again I. acknowledge the relevance given our work on birch [CLS] [SEP] I think that this issue is a core API issue, and a blocker for 1 [CLS] [SEP] Thanks for bringing the debate [CLS] [SEP] Why conflating fit and resample [CLS] [SEP] I can see usecases for a separate fit. and resample [CLS] [SEP] Also, IMHO, the fact that transform does not modify y is a design failure. mine [CLS] [SEP] I would be happier to define a new method, similar to transform, that modifies y I am looking for a good name, and to progressively out. phase 'transform' [CLS] [SEP] That way we avoid introducing a new class of object, and a new concept [CLS] [SEP] The more concepts and classes of objects there are in a library, the. harder it is to understand [CLS] [SEP] Finally, I don't really like the name 'resample' [CLS] [SEP] I find that it is too. specific, and that their are other usecases to the method than resample. semi supervised learning to propagate labels to unlabelled data, for. instance [CLS] [SEP] Here are suggestions of names: apply. change. modify. mutate. convert. The name transform is just too good, IMHO [CLS] [SEP] In the long run, we could come. back to it, after a couple of years of deprecation of the old behavior [CLS] [SEP] The new behavior would be that it always return the same number of arrays. than it is given and raises an error if only X is given for a supervised. method that needs y [CLS] [SEP] Modifying CODESCODES is not the fundamental issue here [CLS] [SEP] Yes, that's something else that needs to be handled [CLS] [SEP] The issue here is that the set of samples passed out of resample is not necessarily the set passed in [CLS] [SEP] This sort of operation of which resampling is emblematic, but I am happy to find it a better name is frequently required for training, and is rarely the right thing to do at test time when you want the predictions to correspond to the inputs [CLS] [SEP] Not just the mostly happens at fit time and yes, as above, there are cases where a fit model will be reapplied, especially outlier detection sets this apart from transformers that must equally apply at fit and runtime, but the idea that the sample size can change [CLS] [SEP] So never mind modifying CODESCODES [CLS] [SEP] A transformer that allows the sample size to change cannot be used in a CODESCODES [CLS] [SEP] A transformer that allows the sample size to change cannot be used in a CODESCODES unless it modifies CODESCODES also because CODESCODES will break, but even so it seems a strange definition of scoring a dataset if it is modified as such [CLS] [SEP] So as much as redesigning the transformer API may be desirable, there is value IMO in a distinct type of estimator that: a has effect in a CODESCODES during training and none otherwise; b is allowed to change the sample size, where CODESCODES s or their successors should continue not to [CLS] [SEP] The idea of the name resample is that the most important job of this class of estimators is to change the sample size in some way, by oversampling, otherwise re weighting, compressing, or incorporating unlabelled instances from elsewhere [CLS] [SEP] That's the argument that I was missing [CLS] [SEP] Thanks [CLS] [SEP] Are there other cases [CLS] [SEP] Based on your arguments justifying the need of the new class, I've been. thinking about the name [CLS] [SEP] And indeed, it should revolve around the notion. of sample, and maybe even the term sample, as this is what we use in. scikit learn [CLS] [SEP] The most explicit term would be transform samples, but I. think that this is too long we might need things like. fit transform samples [CLS] [SEP] One thing that I am worried about, however, is that if we introduce resample and keep the old transform method, it will be ambiguous what. a pipeline means [CLS] [SEP] Of course, we can introduce an argument to the. pipeline, or create a new pipeline variant [CLS] [SEP] However, I am worried that. the added complexity for users and developers does not justify creating. the extra object compared to the Transformers [CLS] [SEP] In other tmers, I think. that we would be better off saying that some transformers change the. number of samples and we can create an extra sub class for that [CLS] [SEP] And would this subclass of transformers also only operate at fit time [CLS] [SEP] I think this is different enough to motivate a different family of estimators, but I might be wrong [CLS] [SEP] This type of estimator pipelining can also be easily modelled as meta estimators [CLS] [SEP] The only real problem there is the uncomfortable nesting of param names although I did once play with some magic that allows a nested structure to be wrapped so that LINKLINK, and that flat is better than nested [CLS] [SEP] Is there a reason why a fit transform wouldn't solve that problem [CLS] [SEP] CODESCODES solves that component if CODESCODES and CODESCODES are allowed to have different results [CLS] [SEP] I think transformers are confusing enough to many users even while more or less promising the functional equivalence of CODESCODES and CODESCODES [CLS] [SEP] Quite clearly I agree with you that breaking this equivalence would be a. very bad idea [CLS] [SEP] But I am not sure why it would be necessary although I am starting to. get your point, I am not yet convinced that it is not possible to. implement a transform method that has the logic necessary to have the. I'm preparing some examples so that we have something to point at in. discussion, but it takes longer than writing quick responses [CLS] [SEP] On 17 November 2014 20:45, Gael Varoquaux wrote: Thank you [CLS] [SEP] This is very useful [CLS] [SEP] Thanks for restarting the discussion on this [CLS] [SEP] So with implementing something that, say, resamples the classes to equal sizes during training, there are three distinct problems:1 It changes 2 It resamples during training, but we want to have predictions for all samples during test time [CLS] [SEP] 3 This estimator could not be CODESCODES 'ed with anything else [CLS] [SEP] The first one might be solved by changing the behavior of transformers, for the other two it is not as obvious as to what to do [CLS] [SEP] I think we might still get away with the transformer interface, though [CLS] [SEP] I would not worry too much about 3 [CLS] [SEP] I think raising a sensible error when someone tries that would be fine [CLS] [SEP] This should be pretty easy to detect [CLS] [SEP] 3 is maybe the most tricky one as it will definitely require some new mechanism and we should be careful if it is worth adding this complexity [CLS] [SEP] How about adding CODESCODES or CODESCODES or something keyword, that controlls whether dropping samples is allowed or not [CLS] [SEP] In a pipeline the option could then depend on whether someone called fit or not [CLS] [SEP] That makes me think: what are the cases when we want different behavior during fitting and predicting [CLS] [SEP] Do we always want to resample during learning, but not during prediction [CLS] [SEP] What if we want to do some visualization and want to filter out outliers for both [CLS] [SEP] jnothman. As far as I understand this discussion, sorry if I missed something, I just quickly skimmed through, especially just the parts that say Birch: P, you mean to subclass Birch and other instance reduction methods from a new class of estimators, called Resamples, and whose CODESCODES method we call during the CODESCODES of Pipeline, right [CLS] [SEP] Some naive questions for starters [CLS] [SEP] In a way like in Birch, one can view the centroids obtained from MBKMeans and some other clusterers, as instance reduction, especially when CODESCODES is large enough, how do we draw a line between whether a CODESCODES or a CODESCODES should be called [CLS] [SEP] What are the transform methods of transformers typically used for in a pipeline [CLS] [SEP] It seems to me that using CODESCODES might be much more useful than transforming the input data in the CODESCODES space, especially when piped with CODESCODES et [CLS] [SEP] al which is what is being done internally [CLS] [SEP] MechCoder. Firstly, I'm not sure that reimplementing BIRCH is what I intend here [CLS] [SEP] It's more that this type of algorithm can be framed as a pipeline of reduction, clustering, etc [CLS] [SEP] There should be a right way to cobble together estimators into these sorts of things in scikit learn, to whatever extent it is facilitated by the API [CLS] [SEP] As for reimplementing BIRCH itself, the resampler could be pulled out as a separate component, and the full clusterer can be offered as well [CLS] [SEP] Yes, using MBKmeans for the instance reduction is equally applicable; the fact that it happens to define CODESCODES with some different semantics means that however it is wrapped as a resampler needs to appear as a separate class somewhat like how CODESCODES and CODESCODES are distinct classes [CLS] [SEP] Classifiers or clusterers or regressors that happen to implement CODESCODES are a little problematic in general because, as you suggest, the semantics of the associated transformation are not necessarily inherent to the predictor, are not necessarily described in the same reference texts as the predictor, etc [CLS] [SEP] For instance, despite in 2160 suggesting that for consistency all estimators with CODESCODES or CODESCODES should also have CODESCODES to act as a feature selector, I later thought the approach of the now stale 3011 would be more appropriate, where we replace this mixin with a way to wrap a classifier regressor so that it acts as a feature selector; alternatively, a method of a classifier regressor like CODESCODES could perform the same magic [CLS] [SEP] The idea is to more clearly separate model and function [CLS] [SEP] amueller. Did you mean 2 [CLS] [SEP] I think this is a key question [CLS] [SEP] Certainly there must be a way to reapply the fitted resampling where appropriate; visualisation is a good example of such [CLS] [SEP] Yet perhaps this is no big deal to expect users to do without the pipeline magic [CLS] [SEP] jnothman yes, I meant 2 [CLS] [SEP] Sorry, I'm not sure I understand your reply [CLS] [SEP] What do you mean by without the pipeline magic [CLS] [SEP] That users should not be able to use pipline in this case [CLS] [SEP] Or that the heuristic of not applying resampling for CODESCODES, CODESCODES or CODESCODES should be the default but there should be an option to not use this heuristic [CLS] [SEP] Btw, this heuristic gives me no option to compute the score on the training set that was used, which is a bit odd [CLS] [SEP] I'm not entirely happy with it, but I've mocked up some examples not plots, just usage code at LINKLINK. I mean that currently there are cases where Pipeline can't reasonably be used [CLS] [SEP] It's particularly useful for grid searches, etc [CLS] [SEP] where cloning and parameter setting is involved, while requiring the visualisation of inliers to not use a Pipeline object probably doesn't hurt the user that much [CLS] [SEP] I agree it's a bit upsetting that this model would not provide a way to compute the training score [CLS] [SEP] To summarize a discussion with GaelVaroquaux, we both thought that breaking the equivalence of CODESCODES and CODESCODES might be a viable way forward [CLS] [SEP] CODESCODES would subsample, but CODESCODES would not [CLS] [SEP] I think it's time to resolve this [CLS] [SEP] We are already breaking fit transform and transform equivalence elsewhere [CLS] [SEP] But are you sure we want to allow fit transform to return X, y, props sometimes and only X at others [CLS] [SEP] Do we then require transform to return only X or is it also allowed to change y I think we should not allow it to change y; it is a bad idea for evaluation [CLS] [SEP] We also have a small problem in pipeline's handling of fit params: any fit params downstream of a resampler cannot be used and should raise an error [CLS] [SEP] Any props returned by the resampler need to be interpreted with the pipeline's routing strategy [CLS] [SEP] Indeed maybe it is a design fault in pipeline, but the handling of sample props and y there assumes that fit transform's output is aligned with the input, sample for sample [CLS] [SEP] I find these arguments together compelling to suggest that this deserves a separate method, for example fit resample, not just an option for a transformer to return a tuple that results in very different handling [CLS] [SEP] I do not, however, think we should have a corresponding sample method and find imblearn's Pipeline [CLS] [SEP] sample method quite problematic [CLS] [SEP] At test time, transform should be called, or else we could consider all resamplers to perform the identity transform at test time [CLS] [SEP] On objects supporting fit resample, fit transform should be forbidden [CLS] [SEP] Let's make this happen [CLS] [SEP] I think for now we should forbid resamplers from implementing transform, as the common use cases are identity transforms, and allowing transform is then possible in the future without breaking backwards compatibility [CLS] [SEP] Proposal of work on this and 9630: Implementation. Create an CODESCODES as a concrete example of an estimator with CODESCODES [CLS] [SEP] CODESCODES is defined to return CODESCODES corresponding to only the inliers [CLS] [SEP] This way outlier detectors will act as outlier removers in a Pipeline once the rest of the work is complete see 9630 [CLS] [SEP] They are here only as a tangible example of a resampler [CLS] [SEP] CODESCODES is merely a dict of params that would be passed to fit [CLS] [SEP] CODESCODES or CODESCODES most often [CLS] [SEP] Inherit from CODESCODES where appropriate [CLS] [SEP] Test it [CLS] [SEP] Extend common tests to cover resamplers [CLS] [SEP] It should: check the output of CODESCODES is of correct structure. check the output of CODESCODES has consistent lengths. check the output of CODESCODES is consistent for repeated calls. assert that having CODESCODES means no CODESCODES or CODESCODES. handle CODESCODES in CODESCODES, making sure that props are handled correctly no props should already be set for downstream pipeline steps; returned props should be interpreted as CODESCODES 's CODESCODES are. perform identity transform for resamplers in Pipeline [CLS] [SEP] transform, predict, [CLS] [SEP] add CODESCODES method to Pipelines whose last step has CODESCODES. perhaps implement other resamplers for example oversample, perhaps based on 1454. Documentation. add modify example of outlier removal. discuss outlier removal in outlier detection docs.10 [CLS] [SEP] discuss resamplers in Pipeline docs.11 [CLS] [SEP] add OutlierDetectorMixin to classes [CLS] [SEP] rst.12 [CLS] [SEP] mention OutlierDetectorMixin in glossary under outlier detector.13 [CLS] [SEP] entry for resampler in glossary.14 [CLS] [SEP] describe resamplers in developer docs. I'm happy to open this to a contributor or a GSoC if others think this is the right way to go [CLS] [SEP] Ping glemaitre. Do you think that CODESCODES will be a good naming for resamplers [CLS] [SEP] An outlier detector is a kind of resampler that removes outliers [CLS] [SEP] Not all resamplers are outlier detectors [CLS] [SEP] The idea is to start by implementing something tangible, rather than an abstract API or Pipeline that cannot be tested [CLS] [SEP] I've clarified above [CLS] [SEP] OK [CLS] [SEP] Could you link to the props PR or issue [CLS] [SEP] Otherwise the plan seems good [CLS] [SEP] Maybe we should add that the different methods need to pass the common test of the estimators checks [CLS] [SEP] By props I just mean a dict of params that would be passed to fit [CLS] [SEP] CODESCODES or CODESCODES most often [CLS] [SEP] This looks like a very interesting project [CLS] [SEP] I would like to explore more and ask for help if there's a doubt [CLS] [SEP] I've marked this help wanted and would really like to see someone pursuing LINKLINK, if only so that we have a concrete implementation to reach consensus on [CLS] [SEP] I would name the Mixin as CODESCODES or CODESCODES [CLS] [SEP] Also, I would name the method CODESCODES to be inline with imbalanced learn [CLS] [SEP] I think doing this in line with imbalance learn would be good and we should just adopt their stuff [CLS] [SEP] cc NicolasHug also thumbs up I could work on thatgo for it, I'd say [CLS] [SEP] I would be happy to see this moving forward [CLS] [SEP] Ping me if I can help thumbs up I recall some weird behaviours in imblearn that I don't want to replicate. here [CLS] [SEP] Generative estimators support a sample method so fit sample sounds. like something else [CLS] [SEP] It also sounds to me like the naive user would expect. it to mean fit on a sample, for example partial fit [CLS] [SEP] So I'm against fit sample. I recall CODESCODES from your hand if I am not wrong [CLS] [SEP] I don't mind what else, i just don't like fit sample. thumbs up. It's a bit of a kludge, but this was my implementation for an sklearn pipeline that can resample: LINKLINK I don't think we can reuse the transform verb [CLS] [SEP] And I don't think we can. transform y at test time [CLS] [SEP] Why did you think it was appropriate to do so in. seglearn [CLS] [SEP] I agree with you Joel that most use cases will not need or even prohibit resampling during test [CLS] [SEP] seglearn deals with time series sequences and resampling is still required at test for segmenting the data [CLS] [SEP] I am not sure if any other applications would also require resampling at test [CLS] [SEP] None come to mind at the moment [CLS] [SEP] In any case, you could use the transform verb for the resampler as I have and call it only during fit transform [CLS] [SEP] However, the implementation you proposed also makes sense [CLS] [SEP] I just posted my implementation as a working example [CLS] [SEP] Happy to help out if needed [CLS] [SEP] David. dmbee, you're welcome to help out with a contribution here [CLS] [SEP] We need someone capable and dedicated to push this through with mentoring from the core dev team [CLS] [SEP] jnothman ok I'll get started and aim to complete it over the winter holidays [CLS] [SEP] jnothman I have a one question about your proposed implementation: sample weight is passed to the pipeline during fit fit transform fit predict as part of fit params and has to be prefixed with the label for the final estimator eg 'logreg sample weight'. I think the cleanest solution is to have pipeline check if sample weight is in fit params and pass this as sample weight to the resampler [CLS] [SEP] The resampler will then return X, y, sample weight, which can be used to overwrite sample weight for the final estimator [CLS] [SEP] I am not sure why you want a dictionary props to be returned by fit resample [CLS] [SEP] Since each step in the pipeline only receives the fit params assigned to it using the prefixing api [CLS] [SEP] Cheers, David. Roughly here is the template I was thinking could work well for resamplers: class ResamplerMixin object: def fit resample self, X, y, fit params: sample weight None. if 'sample weight' in fit params: sample weight fit params [CLS] [SEP] pop 'sample weight'. return self [CLS] [SEP] fit X, y, fit params [CLS] [SEP] resample X, y, sample weight. def fit self, X, y: return self. def resample self, X, y, sample weight: return X, y, sample weight. You're asking the right questions, so this is very encouraging thumbs up. Well, we do have a problem that we've not really agreed on the design of a sample property routing mechanism, and Pipeline's one at the moment is simply not designed to work where the properties cannot be specified to each step when fit is called [CLS] [SEP] But if we want CODESCODES to be able to modify or generate CODESCODES then it needs to be able to modify or generate other things unspecified that align to each sample; alignment to the original samples doesn't make sense [CLS] [SEP] Hence a dict [CLS] [SEP] However, the handling of this in a Pipeline is open for debate. I think this must be the case [CLS] [SEP] For now I would consider raising a NotImplementedError if in a pipeline and the dict is not empty, and we can work out what appropriate behaviour should be once we've worked out sample prop routing may I have the time to consider it [CLS] [SEP] Thanks Joel [CLS] [SEP] I'm pretty sure I understand what you are after [CLS] [SEP] Essentially: Some fit params eg sample weight and perhaps others are tied to the samples and must be accordingly modified by the resamplers for any downstream estimators [CLS] [SEP] Let's call these sample props [CLS] [SEP] A backwards compatible implementation that would involve the least changes to the API existing transformers, estimators, etc could be as follows [CLS] [SEP] Add an optional parameter to the pipeline fit routines called sample props [CLS] [SEP] This is a list of strings which correspond to keys of the fit params that are sample props and need to be modified by the resamplers [CLS] [SEP] The relevant parameters can then be passed and updated by the resamplers [CLS] [SEP] The pipeline fit routine is modified to get the relevant fit params at each step as follows: for step idx, name, transformer in enumerate self [CLS] [SEP] steps: fit params step key [CLS] [SEP] split ' ', 1: fit params [CLS] [SEP] pop key for key in fit params if name + in key ;. if isinstance transformer, ResamplerMixin: props key: fit params for key in sample props. X, y, props transformer [CLS] [SEP] fit resample X, y, fit params step, props. fit params [CLS] [SEP] update props. else: is a regular transfomer. No rush on making this architecture decision, but I'd like to have a plan before moving forward with writing the code [CLS] [SEP] edited to use pop to avoid recalculating sample props for upstream transformers. Don't worry about the Pipeline fit routine in particular [CLS] [SEP] Worry about the. fit resample API [CLS] [SEP] In general, fit methods can take additional parameters. beyond X, y that are aligned with X, for example sample props [CLS] [SEP] I believe. fit resample needs to be able to return such things as well as take them as. input [CLS] [SEP] How they are then handled in a Pipeline is an open question, and IMO. we don't need to commit on that yet to create the resampling API [CLS] [SEP] Fair enough [CLS] [SEP] I suppose modifying the pipeline was part I found most interesting [CLS] [SEP] In any case, you're ok with this as the rough outline for the API [CLS] [SEP] class ResamplerMixin object: def fit resample self, X, y, fit params, props None: return self [CLS] [SEP] fit X, y, fit params [CLS] [SEP] resample X, y, props. class TakeOneSample BaseEstimator, ResamplerMixin: def init self: pass. def fit self, X, y None: return self. def resample self, X, y, props: return X, y, k: v for k in props. I don't know any use case that requires a resample method, really, and it. certainly complicates things in Pipeline: why would we change the set of. samples at test time, and how would that affect scoring a pipeline's. predictions [CLS] [SEP] That is, I propose supporting fit resample, but not resample [CLS] [SEP] Yes, changing the number of samples at test time is a semantic that is. unclear to me [CLS] [SEP] I would rather frown away from it [CLS] [SEP] thumbs up for only having CODESCODES defined in the Mixin [CLS] [SEP] I don't recall any use case having only CODESCODES [CLS] [SEP] Regarding the CODESCODES implementation, I think that the changes done in LINKLINK should make the trick or at least a good start [CLS] [SEP] It will remain the issue regarding the CODESCODES handling [CLS] [SEP] Regarding the handling of the CODESCODES in the resampler itself, it looks like dmbee would go in the same direction than what we thought to handle CODESCODES: LINKLINK. dmbee do not hesitate to retake some of the code tests of CODESCODES [CLS] [SEP] You can also ping me to review the PR [CLS] [SEP] I'm going to become active again in CODESCODES from next week [CLS] [SEP] Above, it was not my intent to support resampling at test time [CLS] [SEP] I understand that is out of scope here niche [CLS] [SEP] Allowing separate fit resample methods instead of just fit resample does not affect how the resampler is used in pipeline or the complexity of the pipeline in my view see my pipeline code above [CLS] [SEP] It came to mind that it may be useful to separate the fit and resample methods for some potential use cases eg generative resampling [CLS] [SEP] However, if this capability is not desirable we can use a very similar API to imblearn adding handling of sample props which is straightforward if the resampler is just indexing the data [CLS] [SEP] See rough example below [CLS] [SEP] Let me know your thoughts [CLS] [SEP] import numpy as np. from sklearn [CLS] [SEP] base import BaseEstimator. from abc import abstractmethod. class ResamplerMixin object: self [CLS] [SEP] check data X, y, props. return self [CLS] [SEP] fit resample X, y, props, fit params. abstractmethod must be implemented in derived classes. def fit resample self, X, y, props None, fit params: return X, y, props. def check data self, X, y, props: to be expanded upon. if props is not None: if not np [CLS] [SEP] all np [CLS] [SEP] array for k in props len y: raise ValueError. def resample from indices self, X, y, props, indices: if props is not None: return X, y, k: props for k in props. else: return X, y, None. class TakeOneSample BaseEstimator, ResamplerMixin: def init self, index 0: self [CLS] [SEP] index index. def fit resample self, X, y, props None: return self [CLS] [SEP] resample from indices X, y, props, self [CLS] [SEP] index. glemaitre thank you [CLS] [SEP] I certainly have no desire to replicate what has been done in CODESCODES imblearn CODESCODES I like and use that package by the way [CLS] [SEP] It seems the main thing lacking atm from CODESCODES imblearn CODESCODES is the pipeline resampler changes required to support sample props [CLS] [SEP] Otherwise, it seems very compatible with CODESCODES sklearn CODESCODES [CLS] [SEP] David. I don't really like that we're committing to a format for the sample props here in a sense, but I guess it's not that different from the handling of CODESCODES fit params CODESCODES in the cross validation code right now [CLS] [SEP] So I think it should be good do go [CLS] [SEP] Actually, you should take whatever works for scikit learn from imblearn [CLS] [SEP] Our idea is to contribute whatever is good upstream and remove from our code base [CLS] [SEP] We are at a stage that the API start to be more stable and we recently made some changes to reflect some discussions with jnothman [CLS] [SEP] Bottom line, take whatever is beneficial for scikit learn thumbs up. Well, in theory, we would need to work on the SLEPs [CLS] [SEP] However, it may be a good exercise to move this issue forward, in order. to be able to write a concise SLEP [CLS] [SEP] OK good to know thanks [CLS] [SEP] I should have used re implement rather than replicate [CLS] [SEP] It seems most things from CODESCODES imblearn CODESCODES can be readily ported [CLS] [SEP] Not sure what a SLEP is [CLS] [SEP] A SLEP is an under used Scikit learn enhancement proposal [CLS] [SEP] LINKLINK. Regarding the API proposal in LINKLINK, yes, that approach to resampling looks good [CLS] [SEP] Not all estimators supporting CODESCODES can do so from indices, but I think you are aware of that; sample reduction techniques will not, for instance [CLS] [SEP] OK good stuff [CLS] [SEP] Yes I am aware that not all resamplers will sample from indices [CLS] [SEP] Those that do not will have to either implement their own method of dealing with props if there is a sensible option hard to know for sure without knowing what will be in props or otherwise raise a notimplemented error if props is not none [CLS] [SEP] Is anyone in Paris working on this [CLS] [SEP] I'd be happy to help and the api would be useful for fitting semi supervised classifiers, as discussed in LINKLINK [CLS] [SEP] Yep I started to port the imblearn implementation [CLS] [SEP] Do you want to take other and I can help for the review instead [CLS] [SEP] Sure, where can I find you [CLS] [SEP] I started working on this I'm not in Paris, but got too busy over the last couple of months [CLS] [SEP] I am happy to share what I have done already, and continue working on it [CLS] [SEP] I'm starting work on this now for the sprint [CLS] [SEP] If you have existing work that you think could be useful, I'd be more than happy to build on what you've done [CLS] [SEP] here it is look at resample folder in sklearn. LINKLINK. 