**System information**
- OSX
- TF 2.3.0-dev20200602


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

Conversion code:
```
    converter = tf.lite.TFLiteConverter.from_saved_model(curr_dir + "saved_model")
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with tf.io.gfile.GFile(curr_dir + '/model.tflite', 'wb') as f:
        f.write(tflite_model)
```


Inference code:
```
    # Compare Inference
    import tensorflow as tf

    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path="./model.tflite")
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
```
The model I'm trying to convert to tflite and run inference on is SSDLite_MobileNetV2, obtained rom the Model Zoo:

http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz

**Failure details**

Conversion is successful, however I cannot run inference: Here is the error that I run into:
```
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. 
Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.
```

I've been playing around with converter settings with no luck
i.e. combinations of: 
```    
    # converter.optimizations = [tf.lite.Optimize.DEFAULT]
    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
    #                                        tf.lite.OpsSet.SELECT_TF_OPS]
```

With none of the settings above set, or the supported_ops set, I can convert the model but cannot run inference, with a similar error as above.
With optimizations set to default, it gives me an error in conversionUsing flex delegate in python is not yet supported https://www.tensorflow.org/lite/guide/ops_select#python_pip_package
However, the feature will be landed really soon, might be sometimes next week, so please wait a little bit.Thanks for the information. I'll keep a watch Are you satisfied with the resolution of your issue?
<a href="https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157">Yes</a>
<a href="https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157">No</a>
Is there an approximate ETA on Python support for flex delegate?The PR got all required approval. I think it won't take too long.The feature is delivered at the HEAD of master. aselva-eb you can try it now.@thaink Would you please post a sample code of how to use this in ```Interpreter```? Thank you @thaink for continuing to give me updates on this - I appreciate it very much!

I tried to update my Tensorflow (using tf-nightly to get latest HEAD of master) and run inference on a model with both ops - however I still run into the same error. Perhaps there's a flag in Interpreter that needs to be set to enable flex ops?

Here's the code I'm using for inference:

```
# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="./model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

```

Error happens on `allocate_tensors()` function:
Here is the error:
```Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.```Hi aselva-eb,
There was a regression with the PR so it got rolled back and just re-submitted yesterday.
Can you give it a try again?
There should be no additional step to use Flex delegate.@thaink still no luck. Was it rolled-back again by any chance? I'm on: `tf-nightly-2.3.0.dev20200619`aselva-eb,
The PR was not rolled back anymore. 
Could you send me your converted model so I can do a check.This is absolutely fabulous! First tests on tf-nightly 06-22  seem to work perfectly. I have not evaluated the results of the converted models, but FlexDelegate loads automatically for models that need it.Great to hear that.@thaink - I've attached the converted model:

[model.tflite.zip](https://github.com/tensorflow/tensorflow/files/4819404/model.tflite.zip)
> Thank you @thaink for continuing to give me updates on this - I appreciate it very much!
> 
> I tried to update my Tensorflow (using tf-nightly to get latest HEAD of master) and run inference on a model with both ops - however I still run into the same error. Perhaps there's a flag in Interpreter that needs to be set to enable flex ops?
> 
> Here's the code I'm using for inference:
> 
> ```
> # Load the TFLite model and allocate tensors.
> interpreter = tf.lite.Interpreter(model_path="./model.tflite")
> interpreter.allocate_tensors()
> 
> # Get input and output tensors.
> input_details = interpreter.get_input_details()
> output_details = interpreter.get_output_details()
> ```
> 
> Error happens on `allocate_tensors()` function:
> Here is the error:
> `Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.`

have you fixed it yet? I have the same issue :((((In my test today. It is working.

[test_flex.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4829629/test_flex.ipynb.zip)
> In my test today. It is working.
> 
> [test_flex.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4829629/test_flex.ipynb.zip)

Hi @thaink 
I tried your notebook with a fresh environment and wasn't able to get it to work. Still get the same issue:

```
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.
```

Version of tf-nightly: `2.3.0-dev20200625` running with Python 3.7

This very odd...

Can you re-try with a fresh environment and let me know? Hi @thaink just wanted to follow up with ^. Are you able to reproduce your results with a fresh environment?Hi aselva-eb,
I tried a fresh environment with venv today and it is still working fine.
Can you try VenV? Are you getting the error at interpreter.allocate_tensors()?

> Hi aselva-eb,
> I tried a fresh environment with venv today and it is still working fine.
> Can you try VenV? Are you getting the error at interpreter.allocate_tensors()?

Oh really??? No I'm using Conda.. 
Yes, my error is on allocate_tensors()I installed tf-nightly with Venv and run the following script:
import tensorflow as tf
tf.__version__

interpreter = tf.lite.Interpreter(model_path="./model.tflite")
interpreter.allocate_tensors()
print("all ok")
> I installed tf-nightly with Venv and run the following script:
> import tensorflow as tf
> tf.**version**
> 
> interpreter = tf.lite.Interpreter(model_path="./model.tflite")
> interpreter.allocate_tensors()
> print("all ok")

I tried the above with Venv and got the following:
```
2.5.0-dev20200629
Traceback (most recent call last):
  File "inference.py", line 7, in <module>
    interpreter.allocate_tensors()
  File "/Users/abc/test/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py", line 243, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.
```

What version of Python are you running? OS?
I'm on Python 3.7.7 with Mac OS MojaveOh, Interesting.
I just tried on a linux docker container, and here's what I get:

```
2020-06-30 16:58:39.637247: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-06-30 16:58:39.637334: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2.5.0-dev20200629
INFO: Created TensorFlow Lite delegate for select TF ops.
2020-06-30 16:58:41.132576: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-06-30 16:58:41.140496: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2208000000 Hz
2020-06-30 16:58:41.141281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x464c640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-30 16:58:41.141333: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-06-30 16:58:41.147742: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-06-30 16:58:41.147801: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-06-30 16:58:41.147832: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (beb39de6fc37): /proc/driver/nvidia/version does not exist
INFO: TfLiteFlexDelegate delegate: 28 nodes delegated out of 233 nodes with 6 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 6 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 6 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 8 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 7 nodes with 2 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 99 nodes delegated out of 3383 nodes with 3 partitions.

all ok
```

Must be something funny between the two platforms installation of tf_nightly. (OSX Mojave vs Linux)

Are those errors at the top concerning? Looks like it's just trying to run on GPU when the drivers are not accessible.


I tried running the invoke command, but here's the error I get now:

```
2020-06-30 17:01:47.430400: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
Traceback (most recent call last):
  File "inference.py", line 19, in <module>
    interpreter.invoke()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py", line 524, in invoke
    self._interpreter.Invoke()
RuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
	 (while executing 'TensorArrayScatterV3' via Eager)Node number 233 (TfLiteFlexDelegate) failed to invoke.
```

I wonder if the above invoke error is as result of an TF1 model that is incompatible with TF2.> I wonder if the above invoke error is as result of an TF1 model that is incompatible with TF2.

I don't think so. Let me ask another what is different between python package of Linux and MacOS.@terryheo 
Hi Terry, Is there any differences between pip package for Linux and MacOS?
The flex delegate works on linux but seems to fail on MacOS.It's currently only works with Linux.
MacOS and Windows support isn't ready yet. (I have a plan to do it)That explains it. Thanks for your help, all. :)
@thaink , Although I can get a few steps farther with using a Linux Docker Image instead of MacOS, I still have issues when running invoke on the model I downloaded from the Tensorflow Object Detection Model Zoo.. 
Could it be a model compatibility issue? Or does this look like something specific to Flex delegate support?  
(See log here: https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-651923086)> @thaink , Although I can get a few steps farther with using a Linux Docker Image instead of MacOS, I still have issues when running invoke on the model I downloaded from the Tensorflow Object Detection Model Zoo..
> Could it be a model compatibility issue? Or does this look like something specific to Flex delegate support?
> (See log here: [#40157 (comment)](https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-651923086))

I get the same error and I want to know that is this because TFLite has not supported it now?@aselva-eb is the python script the same? Or can you explain the steps to reproduce this?
Flex delegate should only use CPU. Seems like it is trying to use Cuda here.Yes @thaink, the python script is the same.

I'll paste it again below:

```
import numpy as np
import tensorflow as tf
print(tf.__version__)

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="./model.tflite")
interpreter.allocate_tensors()
print("all ok")

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```@thaink 

I'm on tf-nightly with the tensorflow docker image. Upon `import tensorflow as tf` this is the stdout:

```
2020-07-08 23:30:21.138117: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-07-08 23:30:21.138183: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
```

The rest of the stdout is related to the loading of the model/invocation using the script above:

```
INFO: Created TensorFlow Lite delegate for select TF ops.
2020-07-08 23:28:36.616600: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-08 23:28:36.623904: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2208000000 Hz
2020-07-08 23:28:36.624427: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a83282f820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-08 23:28:36.624461: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-08 23:28:36.630149: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-07-08 23:28:36.630210: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-07-08 23:28:36.630312: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (beb39de6fc37): /proc/driver/nvidia/version does not exist
INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 149 nodes with 1 partitions.

all ok
Traceback (most recent call last):
  File "inference.py", line 19, in <module>
    interpreter.invoke()
  File "/root/miniconda3/envs/python3p7/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py", line 524, in invoke
    self._interpreter.Invoke()
```

Because this error happens upon even importing tensorflow, I wonder if there is some default set in the image/container for the GPU. 

Alright. Let me check it on my machine.In my test, it fail on both MacOS and Linux. We haven't test flex delegate on docker container before, Let me try to get more log.Hi aselva-eb,
After doing some more tests, it turn out that the problem is with TensorArrayScatterV3 only.
That op fails with flex delegate. is there a way to avoid that op in your model?Hi @thaink ,
Potentially - but in the mean time will there be dev work to support that op? Estimated ETA for a fix?I'll need to investigate more about the failure before estimating time for a fix, thanks.TF ops support (Flex delegate) is now enabled for MacOS.
You can try nightly (tf_nightly-2.4.0.dev20200721-cp36-cp36m-macosx_10_9_x86_64.whl or later)

Let me know if you still have an issue here.Hello,
I know this issue is open for MacOS but as already discussed this also happens on Linux and I am having a very similar problem if not the same with Linux. 

I just tried tf_nightly-2.4.0.dev20200721-cp37-cp37m-manylinux2010_x86_64.whl getting similar results.

Running on a GCP Notebook 

- Python 3.7.6

- tf_nightly-2.4.0.dev20200721

- Linux


The stdout is slightly different but the same error, I am executing the same script 
```
INFO: Created TensorFlow Lite delegate for select TF ops.
2020-07-22 09:10:53.192648: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions i
n performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-22 09:10:53.202567: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz
2020-07-22 09:10:53.202919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5640cc7d71e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-22 09:10:53.202961: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-22 09:10:53.210611: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such fil
e or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64
2020-07-22 09:10:53.210659: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-07-22 09:10:53.210692: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2): /proc/driver/nvidia/version does not exi
st
INFO: TfLiteFlexDelegate delegate: 55 nodes delegated out of 432 nodes with 15 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 15 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 15 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 11 nodes with 2 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 9 nodes delegated out of 114 nodes with 3 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 17 nodes with 2 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 10 nodes with 2 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 11 nodes delegated out of 167 nodes with 3 partitions.

2020-07-22 09:10:53.296992: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per
_step_0/_tensor_arraysTensorArrayV3_0)
Traceback (most recent call last):
  File "untitled.py", line 22, in <module>
    interpreter.invoke()
  File "/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py", line 524, in invoke
    self._interpreter.Invoke()
RuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
         (while executing 'TensorArrayScatterV3' via Eager)Node number 432 (TfLiteFlexDelegate) failed to invoke.
```

Thanks> TF ops support (Flex delegate) is now enabled for MacOS.
> You can try nightly (tf_nightly-2.4.0.dev20200721-cp36-cp36m-macosx_10_9_x86_64.whl or later)
> 
> Let me know if you still have an issue here.

Cool! Thanks - Will try it out and let you know!@SergioPN could you share your model? (or some simple step to reproduce?)Sure,

I was trying to use this frozen_inference_graph in particular
https://github.com/kwea123/fish_detection/tree/master/fish_inception_v2_graph2/frozen_inference_graph.pb

You also can check the saved_model

Thanks @terryheo 

@terryheo  and @SergioPN 
The TensorArrayScatterV3 input is resource, which is not well supported now.
It is the problem with the op alone, not the problem of flex mechanism on MacOS.@terryheo  
are you planning to add support for Flex delegate on windows also? I am using tf.signal.stft in my model and i am able to convert and run it on linux but not on windows. @Shubham3101  Flex delegate is supported on Windows. Did you use tf-nightly ?@terryheo No i am using TF(r2.3). which tf-nightly should i use?
The feature will be enabled with TF r2.4. For now, tf-nightly only supports it.Thanks, it is working with tf-nightlythank all of you，I read a lot of handbook , blog ,material but not deal with it in python.So before it supports, just use java,tf-nightly.@terryheo  Does this works with tflite_runtime, I am facing same issues with tflite_runtime

```
    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 1 (FlexTensorArrayV3) failed to prepare.


```@terryheo  Same issue even with  2.4.0-dev20200803 on linux 
```
all ok
2020-08-03 18:52:27.116473: W tensorflow/core/framework/op_kernel.cc:1772] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
Traceback (most recent call last):
  File "test_local.py", line 25, in <module>
    interpreter.invoke()
  File "/home/tiru/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py", line 525, in invoke
    self._interpreter.Invoke()
RuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
	 (while executing 'TensorArrayScatterV3' via Eager)Node number 462 (TfLiteFlexDelegate) failed to invoke.


```same issue on 2.4.0-dev20200811

 java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
    	 (while executing 'TensorArrayScatterV3' via Eager)
    Node number 285 (TfLiteFlexDelegate) failed to invoke.I use tensorflow==2.3.0
and 
```
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS
  ,tf.lite.OpsSet.SELECT_TF_OPS
  ]
```
and this problem is solved. 

But, I have another error when I run "interpreter.invoke()". There is nothing happen, it looks like the process is aborted.
I run my code on google colab. I find out that "SELECT_TF_OPS" require [delegating](https://www.tensorflow.org/lite/performance/delegates)
Tensorflow Lite has tutorial for delegating with android code. 

My question is can I delegate to gpu on colab ? I found nothing about delegating to colab gpu.
Sorry for my bad English.We used the same library version on android, tensorflow=2.3.0 and founded the same issue as described in my previous comment, in our case this seems to be replicated on the 2.3.0 version of tensorflow android library.Did this break again as I am unable to run the inference on the lastest tf-nightly-2.4.0dev20200626
I am using the ssd_mobilenet_v2_coco_2018_03_29 to do some object detection with the OD api.
The inference code is same as aselva-eb gave here: https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-655807893
this is the error:
RuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
	 (while executing 'TensorArrayScatterV3' via Eager)Node number 244 (TfLiteFlexDelegate) failed to invoke.
@Raphaeal19 did you convert tflite with SELECT_TF_OPS?
https://www.tensorflow.org/lite/guide/ops_select#converting_the_modelYes, this is the code for it @terryheo 

import tensorflow as tf
print(tf.__version__)
converter = tf.lite.TFLiteConverter.from_saved_model('/content/workspace/exported_graph/saved_model',signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with tf.io.gfile.GFile('model_36k_egohands.tflite', 'wb') as f:
  f.write(tflite_model)Oh I see. According to https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-663306760, TensorArrayV3 is not supported with SELECT_TF_OPS@tiru1930 tflite_runtime doesn't support TensorFlow ops since it doesn't contain TF kernel implementation.oh I see. Could you kindly direct me how to resolve this issue with regard to Object detection API @terryheo ? I am just starting out in this.
Should i change my model completely?Also, i just checked https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/flex/allowlisted_flex_ops.cc.
allowlisted_flex_ops has the op TensorArrayV3 as well as TensorArrayScatterV3 in it. @terryheo @Raphaeal19 Since the ops is not working. I'll remove the op from the allowedlist.
@thaink can you give me an eta on the removal of the ops?@Raphaeal19 Removing is a simple cl. So it'll be removed soon.i tried adding the --input_shape arg during the exporting of inference graph to the command and it worked for me.
`!python /content/models/research/object_detection/export_inference_graph.py \
    --input_type image_tensor \
    **--input_shape 1,300,300,3 \**
    --pipeline_config_path /content/workspace/models/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \
    --trained_checkpoint_prefix /content/workspace/training/model.ckpt-36676 \
    --output_directory /content/workspace/exported_graph/`
> @thaink can you give me an eta on the removal of the ops?

Flex-ops only models can support this ops so it will not be removed.@thaink @terryheo 
Hi 
i convert my frozen graph to tflite. when inference it, the inference code is 


interpreter = tf.lite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)


i get the below error:

 RuntimeError: Container __per_step_0 does not exist. (Could not find resource:__per_step_0/_tensor_arraysbidirectional_rnn/bw/bw/dynamic_rnn/input_0_1)
 (while executing 'TensorArrayScatterV3' via Eager)Node number 91 (TfLiteFlexDelegate) failed to invoke.

can you help me ?


@saeedkhanehgir TensorArrayScatterV3 does not works with flex delegate in most case.
For now, you'll need to remove it from the mode.@thaink 
thanks. For your answer
Could you please explain further where I should delete it?@saeedkhanehgir I think you have to replace it in the training code. Then train the model and convert it again.@thaink 
Do I have no other solution?@thaink 
can i build a custom TensorArrayScatterV3 op?@thaink @terryheo 
I think this error was because of lstm layer . if lstm layer was created by keras, this error was solved.@saeedkhanehgir Great info. Thanks for letting us know.Hi @thaink,

I train and save .pb model with TensorFlow 1.15.0 and  try to convert .pb file to .tflite file with tf-nightly

Main codes:
coverter = tf.compat.v1.lite.TFliteConverter.from_frozen_graph()
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT]
converter.experimental_new_converter = True

Error:
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. Node  number 3 (FlexSign) failed to prepare.

Seems like it's the problem with tf.sign and I DO have tf.sign in my model and there is BiLSTM layer in my model. So does it mean I have to implement tf.sign op on my own or are there other solutions? 

Thank you.
@le8888e Sign is supported by flex delegate so the conversion must be fine.
I think the problem here is a typo.
could you use tf.lite.OpsSet.SELECT_TF_OPS instead of tf.lite.OpsSet.SELECT 
Hi @thaink,

I am using tf.lite.OpsSet.SELECT_TF_OPS, sorry for my mistake. And I'm trying tf-nightly 2.4.0 for  flex delegate.

Another question,  I'm using tf 15.0 to define and train the model. In the step of defining model, is 'tf.enable_control_flow_v2()' a must?
Without this, there are errors like 
‘converting unsupported op Enter and TensorArrayV3’

Thank you@le8888e I don't think define the model in a version and convert an a different version is a good practice.
Could you try to define the model with nightly? You can just try to convert it, no need to train.@thaink OK, I will give it a try. Is tf-nightly 1.* available and where can I get them? Since code between tf 1.* and 2.* is a little different, I want to try tf-nightly 1.*
Thank you
 I don't think we have nightly for 1*
You can try disable_v2_behavior on nightly and see if it is what you expect.Hi @saeedkhanehgir,

I came up the same problem with you. In my case there are
tf.contirb.cnn.BasicLSTMCell()
and
tf.nn.bidirectional_dynamic_rnn() 
in my model and there are other layers. Do I only need to implement these two layers by keras and keep other layers unchange？ The tf version I‘m using is tf 1.15.0

Thank you.Hi  @thaink,

After implementing BiLSTM layer by Keras, I successfully convert and inference .tflite model.
I want to serve .tflite model by tf-serving on PC with python api. Is this feasible and is there any guidance for this?

THANK YOU@le8888e You implemented the BiLSTM for Tensorflow?
Is it in C++ or python?@thaink Just replace TF layer declaration by Keras
Original:
outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedded_chars, dtype=tf.float32)
New:
outputs = tf.keras.layers.Birdirectional(lstm_cell, merge_mode="concat")(embedded_chars)

Then errors are gone.@le8888e Are you able to run the tflite model?
@thaink Yes, I can run .tflite model by
interpreter = tf.lite.Interpreter(model_path)
interpreter.allocate_tensors()
...
...
interpreter.invoke()

The outputs of .pb and .tflite are exactly the same.

I'm wondering if I can serve .tflite model with TF Serving.

Thank you There is an on-going effort of supporting tflite on TF Serving.
https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/tflite_session.cc
It's not ready yet but it'll be available soon.Are you satisfied with the resolution of your issue?
<a href="https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157">Yes</a>
<a href="https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157">No</a>
Hi @terryheo @thaink ,

I was building tf-nightly viac pip. Would XNNPACK built by default? And how can I check it?

Thank you It's not enabled by default.
You can enable it with "--use_xnnpack=true" option.

https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html

You can see "Created TensorFlow Lite XNNPACK delegate for CPU." from log.Hi. I got the same error. I dont understand. Which version of tf-nightly do you use to make it work.

TensorArray need resources type support, which is currently in progress.In the TF-nightly version or TF 2.5 version (not tflite-runtime 2.5 version), this problem has resolved already.Hello @thaink & @abattery 

I am running in this error with NonMaxSuppression operation while running inference from TFlite model. 
(tf.image.non_max_suppression is one of the final layers of my model)
I tested using both TF v2.3.2 and v2.4.1

```
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. 
ERROR: Node number 368 (FlexNonMaxSuppressionV3) failed to prepare.
```

Is this operation supported under any recent Tensorflow releases?
The discussion [here](https://github.com/tensorflow/tensorflow/issues/32004#issuecomment-559760072) says I need to build TFLite using tensorflow/lite/delegates/flex:delegate as dependency.

I would be glad if you could elaborate how to do this. 

Thanks.@suraj-maniyar You can find our guide here: https://www.tensorflow.org/lite/guide/ops_select#run_inference
How to use it depends on what environment you are running with.@thaink 
I am using C++ API on CentOS 8 machine and I get the above error on model loading.@suraj-maniyar Are you using bazel? How did you install the dependency for TFLite?@thaink Yes, I am using Bazel (v3.7.1)
I followed all the instructions for building TFLite C API from source from [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/c#building-the-c-api).I see. You can have two options:
- add tensorflow/lite/delegates/flex:delegate to the list of your dependency
- build //tensorflow/lite/delegates/flex: tensorflowlite_flex with --config=monolithic and add the .so file to your projectHello @thaink 
I tried your 2 approaches.

For first approach : 
I updated this file :
```
<tensorflow checkout>/tensorflow/lite/BUILD
```

I changed the tflite_cc_shared_object to add the flex delegete dependency like so ([reference](https://github.com/tensorflow/tensorflow/issues/33980#issue-517305280)) :
```
tflite_cc_shared_object(
    name = "tensorflowlite",
    # Until we have more granular symbol export for the C++ API on Windows,
    # export all symbols.
    features = ["windows_export_all_symbols"],
    linkopts = select({
        "//tensorflow:macos": [
            "-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)",
        ],
        "//tensorflow:windows": [],
        "//conditions:default": [
            "-Wl,-z,defs",
            "-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)",
        ],
    }),
    per_os_targets = True,
    deps = [
        ":framework",
        ":tflite_exported_symbols.lds",
        ":tflite_version_script.lds",
        "//tensorflow/lite/kernels:builtin_ops_all_linked",
        "//tensorflow/lite/delegates/flex:delegate",
    ],
)
```

And built tensorflowlite using this command :
```
bazel build -c opt //tensorflow/lite/c:tensorflowlite_c --jobs 16
```

This built successfully, but during runtime, I still get error in model loading that contains NonMaxSuppression layer.

For the second approach I get this error : 
```
ERROR: Skipping '//tensorflow/lite/delegates/flex:tensorflow_flex': no such target '//tensorflow/lite/delegates/flex:tensorflow_flex': target 'tensorflow_flex' not declared in package 'tensorflow/lite/delegates/flex' defined by /root/TensorFlow-dev/2.3.2/tensorflow/tensorflow/lite/delegates/flex/BUILD
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/lite/delegates/flex:tensorflow_flex': target 'tensorflow_flex' not declared in package 'tensorflow/lite/delegates/flex' defined by /root/TensorFlow-dev/2.3.2/tensorflow/tensorflow/lite/delegates/flex/BUILD
INFO: Elapsed time: 0.083s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```

when I try to build tflite with flex delegate. 
```
bazel build //tensorflow/lite/delegates/flex: tensorflow_flex --config=monolithic --jobs 16
```

Please let me know if there is anything that I am probably doing the wrong way here. I am new to using TensorFlowLite.
Thanks.@thaink 
Any updates on this issue?For the first approach, you are building the wrong target: It should be //tensorflow/lite:tensorflowlite
For the second one, You should sync to our master branch and the name is tensorflowlite_flex not tensorflow_flex@thaink 
Thank you very much for the correction. I tried building tensorflowlite using the first approach.
It gave me the following error:
```
ERROR: /root/TensorFlow-dev/2.3.2/tensorflow/tensorflow/lite/BUILD:644:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/e9dcbb52a816a0ad322c991a68a41fbc/execroot/org_tensorflow && \
  exec env - \
    PATH=/root/CMake/cmake-3.20.0-rc4/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib64/python3.6/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_ENABLE_XLA=1 \
  /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params)
Execution platform: @local_execution_config_platform//:platform
stderr (/root/.cache/bazel/_bazel_root/e9dcbb52a816a0ad322c991a68a41fbc/execroot/org_tensorflow/bazel-out/_tmp/actions/stderr-2) exceeds maximum size of --experimental_ui_max_stdouterr_bytes=1048576 bytes; skipping
Target //tensorflow/lite:tensorflowlite failed to build
INFO: Elapsed time: 143.768s, Critical Path: 143.48s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
```

Is this also supposed to be built under master branch? I am building it under v2.3.2


Can you add --experimental_ui_max_stdouterr_bytes=1000048576 or a bigger number of necessary. And don't forget to add --config=monolithic Thanks a lot @thaink. This built successfully!
I have one additional question: I tried building TF Lite v2.4.1 with flex delegate dependency using cmake and again got the unsupported operator error.
Is flex delegate not supported in v2.4.1 under CMake?
If so, could you give an estimate when the support would be added?@terryheo can you update about the cmake build?We don't have a plan to support Flex delegate with CMake. But I'm wondering why you can't use Bazel for the purpose.@thaink I use tf.image.combined_non_max_suppression in my model. I can transfer it to tflite. But when I use it in Android or use python tf.lite.interpreter, i got the issus.
![image](https://user-images.githubusercontent.com/54882489/118105343-d752c600-b40e-11eb-9810-5a12f18e1ce3.png)
How can I resolve it 

I use the code to convert my model. 
```
converter = tf.lite.TFLiteConverter.from_saved_model('model')
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()

```
@mao381332619 Please upload a new issue. We would like to keep each issue focused.ok. thanks @abattery It works for me now. with Tensorflow == 2.3.0. Merci a lot.

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Garanti
sans virus. www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

Le jeu. 13 mai 2021 à 10:01, mao381332619 ***@***.***> a
écrit :

> ok. thanks @abattery <https://github.com/abattery>
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-840454586>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFUFGTBWB2D5G2SHYCK4EYDTNOPO5ANCNFSM4NSYL2OQ>
> .
>


-- 
*Franck MIGONE*
*Ingénieur Statisticien*
*+225 77 55 34 45 / +225 40 48 92 27*
*Chef Services Ressources et Formation, ENSEA Junior Services*
> We don't have a plan to support Flex delegate with CMake. But I'm wondering why you can't use Bazel for the purpose.

@thaink @terryheo 
Actually, I was able to use the Bazel build for my project. Thanks a lot for your help.
@thaink @terryheo 
Not sure if this falls under a different topic, but I am interested in running inference using flex delegate on single thread.
For that I configured my interpreter like this : 
```
tflite::ops::builtin::BuiltinOpResolver builtins;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter, 1);
interpreter->SetNumThreads(1);
thread_interpreter.interpreter_.reset(interpreter.release());
```

Upon doing that I get an error in destructor for Flex delegate : 
```
(gdb) bt
#0  0x00007ffff5e314c0 in __pthread_timedjoin_ex () from /lib64/libpthread.so.0
#1  0x00007ffff3686661 in ?? ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#2  0x00007ffff3696137 in ?? ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#3  0x00007fffeb67dd69 in ?? ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#4  0x00007fffeb67e1b1 in ?? ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#5  0x00007fffeb640353 in tflite::flex::DelegateData::~DelegateData() ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#6  0x00007fffeb639134 in tflite::FlexDelegate::~FlexDelegate() ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#7  0x00007fffee762e3f in tflite::TfLiteDelegateFactory::DeleteSimpleDelegate(TfLiteDelegate*) ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#8  0x00007fffeb61799e in tflite::impl::Interpreter::~Interpreter() ()
   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so
#9  0x00007ffff72bbdc7 in boost::checked_delete<tflite::impl::Interpreter> (x=0xe83960)
    at /home/smaniyar/Projects/test_project/vendor/boost/boost/checked_delete.hpp:34
#10 0x00007ffff72bc998 in boost::detail::sp_counted_impl_p<tflite::impl::Interpreter>::dispose (this=0x129b190)
    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_impl.hpp:78
#11 0x00007ffff70af8b6 in boost::detail::sp_counted_base::release (this=0x129b190)
    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_base_gcc_x86.hpp:146
#12 0x00007ffff70af949 in boost::detail::shared_count::~shared_count (this=0x100bd88, __in_chrg=<optimized out>)
    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/shared_count.hpp:371
#13 0x00007ffff72b962c in boost::shared_ptr<tflite::impl::Interpreter>::~shared_ptr (this=0x100bd80, __in_chrg=<optimized out>)
    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/shared_ptr.hpp:328
#14 0x00007ffff72bca60 in TensorflowInferenceCore::ThreadInterpreter::~ThreadInterpreter (this=0x100bd80,
    __in_chrg=<optimized out>)
    at /home/smaniyar/Projects/test_project/src-main/deep_learning_inference/tensorflow_lite/TensorflowInferenceCore.h:17
```

Is there a way I can confine inference with flex delegate to run on single thread?
I also built tensorflow with xnnpack support but it did not use single thread.
@suraj-maniyar It should be a different topic. Can you file a new issue for it?@thaink I opened a new issue [here](https://github.com/tensorflow/tensorflow/issues/49266). Could you please take a look?
Thanks.