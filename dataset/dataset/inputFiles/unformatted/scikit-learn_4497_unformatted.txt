This is an issue that I am opening for discussion.

**Problem**:

Sample weights (in various estimators), group labels (for cross-validation objects), group id (in learning to rank) are optional information that need to be passed to estimators and the CV framework, and that need to kept to the proper shape throughout the data processing pipeline.

Right now, the code to deal with this is inhomogeneous in the codebase, the APIs are not fully consistent (ie passing sample_weights to objects that do not support them will just crash).

This discussion attempt to address the problems above, and open the door to more flexibility to future evolution

**Core idea**

We could have an argument that is a dataframe-like object, ie a collection (dictionary) of 1D array-like object. This argument would be sliced and diced by any code that modifies the number of samples (CV objects, train_test_split), and passed along the data.

**Proposal A**

All objects could take as a signature fit(X, y, sample_props=None), with y optional for unsupervised learners.

sample_props (name to be debated) would be a dataframe like object (ie either a dict of arrays, or a dataframe). It would have a few predefined fields, such as "weight" for sample weight, "group" for sample groups used in cross validation. It would open the door to attaching domain-specific information to samples, and thus make scikit-learn easier to adapt to specific applications.

**Proposal B**

y could be optionally a dataframe-like object, which would have as a compulsory field "target", serving the purpose of the current y, and other fields such as "weight", "group"... In which case, arguments "sample_weights" and alike would disappear into it.

People at the Paris sprint (including me) seem to lean towards proposal A.

**Implementation aspects**

The different validation tools will have to be adapted to accept this type of argument. We should not depend on pandas. Thus we will accept dict of arrays (and build a helper function to slice them in the sample direction). Also, this helper should probably accept data frame (but given that data frames can be indexed like dictionaries, this will not be a problem.

Finally, the CV objects should be adapted to split the corresponding structure. Probably in a follow up to #4294
To track the evolution of ideas here previous mentions of related idea:
https://github.com/scikit-learn/scikit-learn/issues/2904#issuecomment-36354214
It would fix #2879, and be a clean alternative to #1574 and #3524
  Sorry to be obtuse, but where does the reticence to depend or better integrate `pandas` come from?  It's hard to find applied examples of `sklearn` in the community that **don't** include `pandas` these days, and the marginal dependencies over `numpy` and `scipy` are only `dateutil` and `pytz`.  It seems as if we'd have to reinvent much of the masking and group-by wheel anyway to support data-dependent CV use cases.
> It's hard to find applied examples of sklearn in the community that
> don't include pandas these days,

I never use it: my data are images, and images don't fit well in pandas.

> It seems as if we'd have to reinvent much of the masking and group-by
> wheel anyway to support data-dependent CV use cases.

Only masking, which is trivial, not group by.
Proposal A along with dict of arrays seems like a good solution to me... :)
@GaelVaroquaux IIRC, you said you were considering a dataset object for out-of-core learning. If that's indeed the case, this should probably part of our reflexion. 
+1 for A.

I'm still reflecting whether we need to change the API of all estimators, though. I'd like to avoid that, but I'm not sure it is possible.

I have nothing better than `sample_props`
It means that everybody that uses `sample_weight` needs to change their code. Which is better than everybody that ever used `y` needs to change their code (which they'd have to for B).
What's the advantage of A over kwargs? 
can you elaborate?
A is a dict of names with array values. These variables could be passed directly as **kwargs, similarly resulting in a dict, without changing the current sample weight handling.
So you would add `**kwargs` to all fit methods and ignore those that are not used?
Perhaps not, but I want to know in what ways this is really a worse
solution than sample_props.

On 8 April 2015 at 00:59, Andreas Mueller notifications@github.com wrote:

>  So you would add **kwargs to all fit methods and ignore those that are
> not used?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/4497#issuecomment-90597704
> .
> Perhaps not, but I want to know in what ways this is really a worse
> solution than sample_props.

Two aspects. 
- First, it would implie that models tend to swallow arguments without
  raising errors. For instance if I don't know that Python is case
  sensitive, I write
  
  "fit(X, Y=y)",
  
  I won't be getting an error message that I didn't pass a valid argument
- Second, exposing sample_props as one argument will be making it more
  obvious that it is a homogenous type. It will also make people's life
  easier if they are already using pandas. (I must say that I am a bit
  scared of coupling too much with pandas, upgrades to pandas tend to
  break our code, as in #4540).

I also find that "**kwargs" is harder to understand for someone who is
not a Python expert.
I think mostly in being a little stricter with the interface. Also, there could be arguments to fit that are not of length n_samples (thought we try to avoid them).
@GaelVaroquaux I think the issue you mentioned is caused by upgrading sklearn, not upgrading pandas ;)
> @GaelVaroquaux I think the issue you mentioned is caused by upgrading sklearn,
> not upgrading pandas ;)

Well pandas.Series.dtype.kind was certainly present in Pandas 0.14.1. I
didn't check for 0.15.
I just thought it worth raising as devil's advocate, so thanks for the initial responses.

> First, it would implie that models tend to swallow arguments without raising errors.

Sure, though naming errors are as much a real issue with `sample_props`. Indeed a confused user may have `sample_props={'sample_weight': [...]}` or `sample_props={'weights': ...}` instead of `sample_props={'weight': ...}`.

Another issue in which all proposed solutions fail (but the incumbent approach of "pass `sample_weight` explicitly works fine): if an estimator does not have `sample_weight` support but then it is implemented, its behaviour will change implicitly though the data does not. Is there any way we can avoid this backwards compatibility issue? I don't think the friendly answer is `UserWarning("Weight support was recently added. Your results may differ from before.")`
> Sure, though naming errors are as much a real issue with sample_props.
> Indeed a confused user may have sample_props={'sample_weight': [...]}
> or sample_props= {'weights': ...} instead of sample_props={'weight':
> ...}.

Yes, I agree. I think that the proposal is slightly better than
"**kwargs" in this respect but not much better.

> Another issue in which all proposed solutions fail (but the incumbent
> approach of "pass sample_weight explicitly works fine): if an estimator
> does not have sample_weight support but then it is implemented, its
> behaviour will change implicitly though the data does not. Is there any
> way we can avoid this backwards compatibility issue?

That's a very good point. We could suggest a global flag "raise", "warn",
"ignore" to deal with unknown sample_props, controlled in the same style
as np.seterr, which is an incredibly useful debugging feature in numpy.
Somewhat related question: will transformers also output a modified `sample_props`? They must, right?
Or perhaps we should at least have a way of introspecting which sample
props an estimator or method knows about so that the user can make
assertions in upgrades...? Too frameworkish?

On 8 April 2015 at 04:06, Andreas Mueller notifications@github.com wrote:

>  Somewhat related question: will transformers also output a modified
> sample_props? They must, right?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/4497#issuecomment-90681327
> .
Fixes #2630, also see [the wiki](https://github.com/scikit-learn/scikit-learn/wiki/Pipeline-et-al.-design-issues#passing-parameters-such-as-sample_weight-to-methods-cf-2630)
To summarize the current state of the discussion, I think something like this would be a nice solution:

Estimator:

```
def fit(X, y, sample_props={}):
    weights, groups = check_props(sample_props, required=["weights"], optional=["groups"])
    ...
```

User:

```
sklearn.seterr("raise")  # 'ignore', 'warn', 'raise', 'call', 'print', 'log'
est = Bla().fit(X, y, sample_props={"weight": ...})
```

-> ValueError("Sample properties 'weights' are missing, unknown sample properties 'weight'")

The only thing that is missing is a good way to document the required and optional sample properties of an estimator. I have no idea how we can do this. An advantage of having `sample_weights` as an explicit argument in `fit` is that you can directly see that an estimator uses it and it is obvious whether the description in the docstring is missing or not.
I think just mentioning it in the fit docstring and / or the estimator docstring should be fine, shouldn't it?
I don't think sklearn.seterr("raise") is good btw. It should be `sklearn.seterr(sample_props="raise")`.
I could see `sklearn.seterr(deprecation="raise")` or `sklearn.seterr(type_conversion='ignore')` or convergence issues etc.
That sounds reasonable. I think this is a more general feature that has an impact on many parts of the library. We should make a separate pull request for it before we deal with the sample properties, shouldn't we? Are there any disadvantages of having such a global state?
+1 for a separate PR.
Anything in the way of implementing A? I don't think the error mechanism is a requirement, it is just an added precaution.
> I don't think sklearn.seterr("raise") is good btw. It should be sklearn.seterr
> (sample_props="raise").

+1
> Anything in the way of implementing A?

Maybe the data-independent CV should be merged first.
So, `sample_props` will go to `fit`. @amueller pointed out that it should be taken (and also returned by) `transform`. Would it also go to `score`? To `predict`? I suppose so. ([Elsewhere](https://github.com/scikit-learn/scikit-learn/pull/4294#issuecomment-83269134), @jnothman speculated an estimator that would play nicely with memmapped arrays in multiprocessing by fitting and scoring with cross-validation binarized sample_weights. If using this in `cross_val_score` with a custom scorer, it'd need to be passed to `predict`.)

This goes back to the "routing" discussion in #3524. Now, is there any reason why objects doing cross-validation (searches, cross_val_score) would not slice and route all of these parameters everywhere?
If there is, we could use the "param_routing" constructor attribute suggested by @jnothman. Either way, the slicing and routing code should ideally be in one place.

EDIT: I just realized my comment would maybe be more relevant in #4632. The "routing" idea could solve the question "how to specify whether scoring should be weighted or not?", and in learning to rank, `group_id` definitely needs to be sliced and passed to scoring.
Not sure whether to post this here or in #4632, but here goes a case study that I think could test the API. 

On Reddit there are many posts, and under each post there are multiple comment threads.
Let's say I want to learn to rank comments (or learn to predict the best comment) in a Reddit comment thread.
(Threads are tree structured, but let's linearize them here for simplicity.)

```
post_1
   thread_1 < ... (n_samples_1)
   thread_2 < ... (n_samples_2)

post_2
  thread_3 < ... (n_samples_3)
...
```

All threads under a post are roughly on the same topic, but they are independent conversations. When training, predicting, and scoring I want to only compare samples from the same thread (`group_id=thread_k`). For cross-validation though I'd like to split over posts (`group_id=post_k`)

Things I'd need:
- Group-aware scorer that could express things like "is the true highest-scoring sample among the k best samples my classifier returns **among this thread**?
- `LeavePLabelOut` (or preferably just `KFold` actually) that splits across **posts**.
- This should mostly look like standard classification, except for the definition of a custom scorer and a custom group-aware learner. This would mean support in search objects and things like `train_test_split` (the latter should take dict-of-arrays sample_props too). So I could do

```
(X_train, y_train, sample_props_train,
    X_test, y_test, sample_props_test) = train_test_split(X, y, sample_props)
grid = GridSearchCV(
    ...,
    scorer=MyTopKScorer,
    cv=sklearn.model_selection.LeavePLabelOut,
    # missing API part that specifies what the sample_props mean
)
grid.fit(X_train, y_train, sample_props_test)  # splits according to OP
y_pred = grid.best_estimator_.predict(X_test, y_test, sample_props_test)
```

This is clearly a desirable use case. How much of this should be in scikit-learn's scope? 
- I'd argue that cross-validating by a certain label should be in scope.  (And I'd like it to be supported by other CV generators, not just the `Leave*LabelOut` estimators.)
- Currently none of the estimators support any `sample_props` in predict or score, but they should probably support `sample_weight`, so we could allow other props to tag along. This would allow learning to rank, and all sorts of controlled paired classifiers (when you don't always want to form all possible pairs). Clearly useful in my research, I wonder how you feel.
What I wonder is if there is a common case where the `group_id` for learning to rank is different to that for cross validation; we have a problem here of global namespacing. Would I then need name mangling like `cv__group_id` and `estimator__group_id`?? Or will we just assume that the user will glue things together manually if it doesn't fit into the global meaning of `sample_props` keys?
regarding

>  # missing API part that specifies what the sample_props mean

I get the impression that Gaël's idea of `sample_props` means all these are passed around globally.
In which case you want the CV object to pick up the `group_label` and the estimator and scorer to pick up the `group_id`. This would indeed allow me to implement what I proposed above, even if the CV objects in scikit-learn would be hardcoded to use `group_label`, assuming that the scorer and estimator have to be implemented by me. So I can make them use any `sample_prop` I want.

Good point, I guess it kind of works. Until scikit-learn will provide some estimator that uses some sort of `group_label` or `group_id`.

Am I missing anything? Would passing them around globally break anything?

Also, should `train_test_split` then use `group_label` if a `sample_props` is passed and it's available?
Alternatively, to avoid the need for any routing or horrible name mangling in the future, any function or object that will look at `sample_props` could have arguments that specify which prop to use. e.g. `LeavePLabelOut(label_name="group_label")`. It would add an avoidable arg, but in the context of data coming from some other source such as a pandas DataFrame, this could actually make code more readable.
I didn't get the argument why the labels are needed for `predict` with a custom scorer. The scorer should get the sample weights, right?

I'm not sure I'm entirely following your train of thought @vene. The idea was that all sample properties are spliced by cross_val_core and GridSearchCV. What do you mean by routing? I thought we wanted to pass all properties everywhere.

Why wouldn't this work with sklearn estimators using `group_label`?
To provide a good API to the user, the `predict` function should be grouping-aware. (In learning to rank, you would want to return a ranking for each group.) Even more, for some models, it would be impossible to predict without having the group labels (explicit pairs might need to be formed).  The good news is this would only impact `_fit_and_predict` which is only called by `cross_val_predict`. But it would still mean a change to all `predict` signatures in the code base. At least there is no `cross_val_predict_proba` or `cross_val_decision_function`.

(EDIT: or just explicitly warning `sample_props` is not supported in `cross_val_predict`, if passed. I'd rather do this, than change all `predict`s to add an argument that's not used.)

About the routing, sorry. That part is noise, I think. I had missed that we'd pass all properties everywhere. When Joel pointed that out I realized that my case study would work. But I still think there's value in thinking about a end-to-end use case like that. It shows that `train_test_split` needs changing too, for instance. Therefore so would `shuffle`/`resample`. And it raises the question whether `train_test_split` should look for a `sample_label` and avoid mixing it.

I am not arguing against the proposal, I'm just trying to push it and see how it could be used.
Change to predict signature will also affect the scorer's call to
predict/decision_function.

On 29 April 2015 at 08:05, Vlad Niculae notifications@github.com wrote:

>  To provide a good API to the user, the predict function should be
> grouping-aware. (In learning to rank, you would want to return a ranking
> for each group.) Even more, for some models, it would be impossible to
> predict without having the group labels (explicit pairs might need to be
> formed). The good news is this would only impact _fit_and_predict which
> is only called by cross_val_predict. But it would still mean a change to
> all predict signatures in the code base. At least there is no
> cross_val_predict_proba or cross_val_decision_function.
> 
> About the routing, sorry. That part is noise, I think. I had missed that
> we'd pass all properties everywhere. When Joel pointed that out I realized
> that my case study would work. But I still think there's value in thinking
> about a end-to-end use case like that. It shows that train_test_split
> needs changing too, for instance. Therefore so would shuffle/resample.
> And it raises the question whether train_test_split should look for a
> sample_label and avoid mixing it.
> 
> I am not arguing against the proposal, I'm just trying to push it and see
> how it could be used.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/4497#issuecomment-97240238
> .
Yes, but default scorers could avoid passing the props, and third party estimators that require the props in predict will require custom scorers (which they probably will anyway).
FYI I was working on an implementation of the proposal, but it is a bit stalled as I'm sick :-/
I'm wondering what we should do with `fit_params`. Should we deprecate the slicing there? It kinda duplicates this functionality, only with the wrong interface.
Btw, is `sample_props` the name we want to go with? I realize my branch uses `attributes` because that is what @agramfort suggested when I started coding it. If we agree on `sample_props` I can change it now.
don't always do what I suggest :)

attributes, sample_props, sample_info all work for me.
I would prefer having the word `sample` in the name.  `sample_prop(ertie)s`, `sample_info`, `sample_attr(ibute)s` seem a bit better than `attributes` etc.
votes on sample_props vs sample_properties vs sample_attrs [seems hard to pronounce] vs sample_attributes?
Hum, one more thing: if this is added to the interface, all thrid-party estimators will break.
So will we check if fit has it and otherwise raise a deprecation warning? And deprecate all instances of sample_weights?
What would be a good test for the API?

``` python
cross_val_score(GridSearchCV(BernoulliNB(), params), X, y, sample_props={'sample_weights': sample_weights}))
```

That is currently not possible, right?
See #4696 for wip
> In learning to rank, you would want to return a ranking for each group

If we ever provide such functionality, I would rather add a new method, like `predict_rank` for instance. For the time being, I would require the user to call `decision_function` (classifier) or `predict` (regressor) on each group to obtain scores and then sort the samples by scores.
@GaelVaroquaux's proposal is a bit vague w.r.t. the goals. To make it more concrete, I propose these two use cases:

``` python
cross_val_score(GridSearchCV(SGDClassifier(), params), i_dont_care_what_its_called={'sample_weights':sample_weights})
```

and

``` python
cross_val_score(GridSearchCV(SGDClassifier(), params, cv=LeaveOneLabelOut()), i_dont_care_what_its_called={'labels': groups}, cv=LeaveOneLabelOut())
```

The second clearly needs the data independent CV, but I think we can still have it in mind.

I'm not sure if we agreed on what should happen with parameters that are not used / supported.
Gael said he doesn't want the estimator to crash. What do we want? Optional warnings? With the default being what?
In the two examples above, the groups will only be looked at by the CV objects, while the weights will only be looked at by the estimators. How can I make sure that I don't get a warning here, but I would get a warning when using an estimator that doesn't support sample_weights?
Yes, so the nested CV cases is why we might want everything to just be grouped into `sample_props` or similar. A more explicit and hence safe -- but verbose -- alternative is:

``` python
cross_val_score(GridSearchCV(SGDClassifier(), params, cv=LeaveOneLabelOut(), routing={'sample_groups': ['split'], 'sample_weights': ['fit', 'score']}), sample_labels=groups, sample_weights=weights, cv=LeaveOneLabelOut(), routing={'sample_groups': ['split', 'fit'], 'sample_weights': ['fit', 'score']})
```
Not to derail the discussion, but is there any long-term plan for attaching properties to columns / features as well, as it could impact API choices on how this is implemented? eg. categorical variables in #3346 for instance.
@trevorstephens Very good point. I think we should design a solution that can handle column meta-data as well.

One option we haven't discussed yet is creating a Dataset class in scikit-learn. The idea would be to pass X as an instance of this object in fit, score, etc. The advantage is that we can encapsulate extra meta-data. Of course, NumPy arrays and SciPy sparse matrices would still be supported. In the future, if we want to tackle out-of-core learning, such a dataset object could be responsible for loading small batches from an on-disk dataset. Some algorithms such as SGD could be rewritten to take advantage of this. The dataset could also potentially be used to do cross-validation without allocating the sliced dataset, which is something I would like to have. I know that we have been avoiding dataset objects so far but we should at least consider it (a small prototype would be nice).
I am a bit afraid of adding a dataset class. Things seem to be pointing in this direction, though.
I would really like to ground this discussion in concrete usecases.
So a dataset class would be nice for online learning. But that could probably also be done by just accepting an iterator, right?
Iterators would be nice for sequential learning (with an iterator you loose the notion of fixed dataset of size n_samples x n_features). A dataset object would be nice for out-of-core learning, which is different. But before going into that, we should decide whether a dataset object would solve the present issue better than the sample_props approach (a dictionary-like object containing extra data used for fit / score / transform) . I am not saying that it would, but we should at least discuss the pros and cons. One advantage is that the extra data is encapsulated into the same object as the main data and thus, we don't need to add an extra argument to all methods in the project. Would the dataset approach work for nested CV?

For the dictionary-like approach, a better name than `sample_prop` would be `data_prop` (or `extra_data` or `meta_data`) if we want to handle @trevorstephens 's use case.
Well, the condition on the dictionary-like approach was somewhat that each value is n_sample long.
So that wouldn't really handle @trevorstephens usecase. If we drop this assumption, how are we going to decide what to slice?

The dataset would solve the nested CV.
Everything that starts with `sample_`.
do we have an actual use-case for attaching meta-data to the columns? so categorical vs numerical for trees? And we want to do that by passing in a dictionary and not using pandas?
categorical vs. numerical would be useful for OneHotEncoder too, I guess... I think a DataFrame is a dict of 1d arrays of all the same size (can't mix n_samples arrays and n_features arrays). So maybe we need `sample_prop` and `feature_prop` then ;)
Maybe we should look more into how R does these things? They already solved all problems, right ;)

We should really come up with a list of requirements, I'd love to get some feedback from @GaelVaroquaux who started this thread.
And does anyone have a better idea on how to be "safe" than @jnothman above?
I feel @jnothman's version is a bit too verbose to be practical, but are we ever going to give helpful error messages without being that explicit? Or do we just give up on that?
Maybe add a `routing_rules()` method to `BaseEstimator`? The method would return the routing rules for `sample_weights` and `sample_groups` by default but could be overridden for returning more. For example, if an estimator needs `sample_blabla`, it would need to specify the routing rules for it by overriding `routing_rules()`. Or maybe the method could be added to the dataset object, if we have one.
For me the problem was more "what if I'm passed more than I need". So if the estimator doesn't handle `sample_weights` what will happen?
If you are not passed something that is necessary, you can always just crash.
> I feel @jnothman https://github.com/jnothman's version is a bit too
> verbose to be practical, but are we ever going to give helpful error
> messages without being that explicit?

I agree it's too verbose for what we want! It is less verbose when you
consider that we will set a backwards-compatible "sensible default" (and
ignore any props that have routing specifications but aren't provided),
perhaps with a warning that weighted scoring will become default (such that
providing weights but having unweighted scorer will raise exception by
default, albeit after a slow fit process :s). I only think routing needs to
be dealt with in metaestimators, such as pipeline (where perhaps only some
transformers want weighting, etc) and CV (where you may need to pass
_everything_ to the base estimator in a nested CV context).

> So if the estimator doesn't handle sample_weights what will happen?

In sample_props approach, the estimator will ignore it silently I presume.
In a routing approach, if sample_weights is provided and the estimator
doesn't support sample_weights then an exception is possible and
appropriate.

> If you are not passed something that is necessary, you can always just
> crash.

Sure.

The problem is the case of optional support; and worse, the case where no
support changes to optional support, which I think is a real concern.
Again, consider the change to weighted scoring.

On 14 May 2015 at 05:37, Andreas Mueller notifications@github.com wrote:

>  For me the problem was more "what if I'm passed more than I need". So if
> the estimator doesn't handle sample_weights what will happen?
> If you are not passed something that is necessary, you can always just
> crash.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/4497#issuecomment-101788960
> .
> We should really come up with a list of requirements, I'd love to get some
> feedback from @GaelVaroquaux https://github.com/GaelVaroquaux who
> started this thread.
> 
> we're both swamped with Gael but we should talk maybe on hangout to
> facilitate convergence on this big API issue...
I'd be happy to chat maybe @jnothman can join us?
What is not entirely clear to me is: what should `sample_weight` do in a pipeline?
You could warn / error if none of the steps support it, or you could warn/error if any of the steps doesn't support it. Or you could provide an explicit list of which steps it should be passed to. 
The last one seems a bit complicated, though.
when can we skype?
> when can we skype?

I personnally don't think that a skype is a more efficient than a written
discussion backed by thoughts and constructed arguments. I find that in a
skype, the flow of thoughts is hard to follow. I also like that in a
discussion like the one we are having on this issue, it is easy to go
back and look at the points made by various people. The fact that I am
not commenting does not mean that I am not reading this thread, trying
to understand everybody's point of view, and trying to have a big
picture. I just find it's hard :).
ok. I'm fine with either. I agree that in comments it is easier to structure arguments, and share code. I would really appreciate if you could come up with some use-cases that you had in mind when writing your original issue.
Did you guys have time to discuss this?
Some. The main use-case is subsampling and "labeled" cross-validation I believe. I still don't know how to deal with the fact that some attributes are given to the estimator and some to the cross-validation.

So the example I gave above is still unclear to me:

``` python
cross_val_score(GridSearchCV(SGDClassifier(), params, cv=LeaveOneLabelOut()), i_dont_care_what_its_called={'labels': groups, 'sample_weights': sample_weights}, cv=LeaveOneLabelOut())
```

should this warn about anything? If this doesn't warn, can we ever warn without giving very explicit routing? What is sample_weights is only supported by some steps in a pipeline? Should we warn?

Also, I am not entirely sold on other properties apart from the labels to split on and sample_weights, but that might be because I am not familiar with the ranking setting you mentioned.
I guess there are two routes as outlined by @jnothman above:
route everything explicitly and give meaningful errors, or pass around `sample_props` to everything and silently ignore things that you don't use. Which will lead to silent behavior changes if we add sample_weights support to anything, for example.
@amueller @vene @GaelVaroquaux @jnothman this will be the next thing I'll work on... Which route do I take?
@rvraghav93 I think I would rather work on nesting grid-search and EstimatorCV. @vene what do you think? I have a PR that the sample props partially in #4696. The main issue was that we don't have that many use-cases and that we have ill-specified requirements. For the nesting grid-search and EstimatorCV, it is pretty clear what needs to happen, I think.
Sorry, I'm catching up still.
Hi all,
I saw there is an ongoing discussion on how to include the sample_weight option into RFECV, but I'm not sure to understand what the current status is.
I'm working at my PhD thesis and I need to apply event-by-event weights to a given sample when using RFECV, in order to take the mis-modeling of the generated events into account.
I attach an example of what I would like to do (which is not not working at the moment, since the sample_weight option is not recognised).
Could you help me?
Not sure what the status of this is, but after reading this discussion I'm using the following on something I'm doing:

``` python
def screen_kwargs(fn, kwargs):
    sig = inspect.signature(fn)
    if any(param.kind == param.VAR_KEYWORD
           for param in sig.parameters.values()):
        return kwargs
    else:
        kws = {k for k, param in sig.parameters.items()
               if param.kind in {param.KEYWORD_ONLY, param.POSITIONAL_OR_KEYWORD}}
        return {k: v for k, v in kwargs.items()
                if k in kws}
def screened(fn):
    @functools.wraps(fn)
    def wrapped(*args, **kwargs):
        screened_kwargs = screen_kwargs(fn, kwargs)
        return fn(*args, **screened_kwargs)
    return wrapped

def pipeline_fit(pipeline, X, y=None, **fit_params):
    # Pipeline._pre_transform
    Xt = X
    for name, transform in pipeline.steps[:-1]:
        if hasattr(transform, "fit_transform"):
            Xt = screened(transform.fit_transform)(Xt, y, **fit_params)
        else:
            Xt = screened(transform.fit)(Xt, y, **fit_params).transform(Xt)
    # Pipeline.fit
    screened(pipeline.steps[-1][-1])(Xt, y, **fit_params)
    return pipeline

def pipeline_predict(pipeline, X, **predict_params):
    Xt = X
    for name, transform in pipeline.steps[:-1]:
        Xt = screened(transform.transform)(Xt, **predict_params)
    return screened(pipeline.steps[-1][-1].predict)(Xt, **predict_params)
```

The screened wrapper checks automatically drops any kwargs not supported in the function signature (var kwargs imply all kwargs can be supported). It completely ignores the previous API with the special param names, though implementing both would be pretty trivial.

_Caveat utilitor: I think there might be an oddity about how method wrappers are treated back in python 2._
Thinking about this again given the merge of #8278, I would like to note a couple of things about my routing proposal:
* As long as the sample props are always passed, it is generally possible to do routing (and renaming of sample props sought by @vene) by wrappers, mixins and the like: simply remove the `weight` column or rename it to opt out of its use.
* However it remains necessary for compatibility *across versions* to have an explicit way to specify routing in meta-estimators and CV estimators, and default routing needs to maintain former behaviour (i.e. not pass sample weights to `SearchCV` scorers, nor transformers in a `Pipeline`) as clear from an issue like #4632. The only safe (wrt backwards compatibility) alternative to this is that every time `sample_weight` (or another attribute) support is added to something pre-existing, it needs to also add a parameter `ignore_sample_weight=False` at least to facilitate deprecation in the short term.
* The necessity of some kind of routing management is not contingent on a generic sample properties implementation. It applies even in the simple case of enabling weighted scoring in `GridSearchCV` or `cross_val_score` (#4632).
* In `Pipeline` we already have selective passage of `fit` parameters using double-underscore notation, but this requires that the fit parameter be repeated for each target estimator. I think with these semantics extended to other contexts, this would still have full expressive power of routing, but with a routing parameter in `Pipeline` and other meta/CV estimators, the routing specification becomes more localised and this multiplication of input parameters (not to mention the munging of names) is avoided.

So while it's a separate but related issue, I'd like to hear which is preferable: a routing parameter, or always route but require attributes to be ignored by default over a deprecation period.Hi. So is there a way to take sample weights into account for cross validation and gridsearch ? Thanks in advance. for fitting, yes, for scoring no. See fit_params in cross_val_score, for
instance
If we had a "deep" approach, where the user needs to explicitly specify the path they want a sample prop to take when passing in the sample prop, would it be acceptable to have non-identifier keyword argument names? For example:
```
GridSearchCV(...).fit(X, y, **{'sample_weight': w, '[score]sample_weight': w})
```
Python accepts this. And if we want such a deep (path-based) solution unlike my #9566, we need to express paths with more than just double-underscores which should be reserved for nested estimators. (I note that such deep property mechanism makes modifying sample properties in resamplers as in #3855 problematic: a resampler would need to return props modified with the fully-qualified names.)Any updates on being able to use 'sample_weights' when performing RFECV?Sigh. No. I would like to be able to spend time on this for 0.21... but I'm
not sure where that time is going to come from, and I need to get some
commitment from other core devs that they will review it if I work on it.
No worries, completely understandable.  Thanks for all your hard work on Sklearn library.  I love how simple and universal this package is.  

One last question, do you have any suggestions for a quick and dirty way to implement sample weights to RFECV? I just want to mess around with how samples weights change the features I get. Solution here for those interested: https://stackoverflow.com/questions/49581104/sklearn-gridsearchcv-not-using-sample-weight-in-score-function 

Basically, you can define your own scorer and do index matching to get the right weights inside that function. GridSearchCV / RandomizedSearch won't do the splitting for you (yet...)Yes, that's quite a neat pandas-based hack. Another solution would put the
weights as a column in X then have a transformer that drops them for
training....
I have not read all of the referenced PRs, issues and comments (it's a lot) but I went over this thread briefly.

One comment I have: would it be possible to introduce this new parameter while keeping the existing `sample_weight`, `groups`, etc. parameters? Then as things get updated to support the new parameter, there can be a check that raises an error or warning if the user specifies both `sample_weight` as it's own parameter and also `sample_weight` within `sample_properties`. Then once `sample_properties` is fully implemented in the ecosystem support for `sample_weight` could be dropped, with warning of course.

I am also going to briefly detail my use case and results below to support this feature.

I am working on classifying activity data (accelerometer). My data looks something like this:
| subject | walk | feature_1 | feature_2 | y_true | y_pred |
|---------|------|-----------|-----------|--------|--------|
| 0       | 0    | 0.2784    | 0.146     | 1      | ?      |
| 0       | 0    | 0.1428    | 0.1286    | 1      | ?      |
| 0       | 1    | 0.127     | 0.8127    | 2      | ?      |
| 1       | 0    | 0.8721    | 0.328     | 3      | ?      |
| 1       | 1    | 0.146     | 0.376     | 2      | ?      |
| 1       | 1    | 0.4879    | 0.274     | 2      | ?      |

In this case, `X` would be `[feature_1, feature_2]` and `[subject, walk]` are the 'sample properties'.
I use `subject` as the groups in LOGO cross validation because my end goal is to predict for a subject with no existing data. 
I need to classify each `walk` into one of the categories in `y_true`, but I don't really need to classify _each datapoint_ (in reality, I have thousands of datapoints for each walk). So ideally I would group `y_pred` by `walk` and select datapoints for each `walk` that either have above a certain threshold of prediction probability or pick the top 3 datapoints, etc. The physical reasoning for this is that if the subject did something weird for 1-2 steps I am happy to discard that because I have a lot of other data to work off of.
I was able to achieve this by essentially copy-pasting `sklearn.model_selection.__validation.py` and hacking it up to passthrough a parameter `metadata` (a `dict`) to the `scoring` function, which I made to have the signature `score(estimator, X, y, metadata=None)`.

Doing this increased my cross-validation scores considerably, I guess I had a lot of "bad" data points that I am now discarding.I've not yet looked at all of your contribution yet, @adriangb, but you might want to check out the latest proposal at #16079I haven't contributed to scikit-learn @jnothman, but would love to start! Thanks for referring me to the current discussion, I will comment there.This is a very intricate place to start!! It has challenged those of us who
know scikit-learn API deeply for years.
Well, I don't expect to be able to do too much, but it cant' hurt to try! I was also interested in working on IterativeImputer (https://github.com/scikit-learn/scikit-learn/issues/16638#issuecomment-613588952). That's probably not any easier.IterativeImputer doesn't have as many API quandaries and intricacies
involved. More algorithmic questions.
