**System information**
- TensorFlow version (you are using): 1.13.1 (but willing to use 2.0.0-alpha0 if there is a good reason)
- Are you willing to contribute it (Yes/No): Yes (given some pointers on how to best go about it)

**Describe the feature and the current behavior/state.**
Currently there is no obvious way to apply `tf.contrib.quantize.create_training_graph` to a keras model. The keras API only allows access to the graph after it has already created a session. Attempting to modify the graph at this point does not work:
https://stackoverflow.com/questions/55123417/quantization-aware-retraining-a-keras-model
https://stackoverflow.com/questions/52259343/quantize-a-keras-neural-network-model

I have also tried to create a new session after rewriting the graph, without success:
```
tf.contrib.quantize.create_training_graph(input_graph=tf.keras.backend.get_session().graph, quant_delay=0)
# create a new session after rewriting the graph
new_session = tf.Session()
tf.keras.backend.set_session(new_session)
```

Results in this error when I try to fit the model:
```
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable dense_5/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/dense_5/bias/class tensorflow::Var does not exist.
        [[{{node dense_5/BiasAdd/ReadVariableOp}}]]
```

**Will this change the current api? How?**
Probably, but in a backwards-compatible way. I imagine some kind of graph rewriting hook would probably be necessary in the tf.keras API.

**Who will benefit with this feature?** Users of TF Lite / Edge TPU wishing to easily train quantized models using the keras API (which is being pushed as the new "one true API" for tensorflow).

**Any Other info.**
Related issue on the main keras project https://github.com/keras-team/keras/issues/11105This is my code for quantization aware training in keras.
https://gist.github.com/rocking5566/26637a1e969f0057811753050966a3a7

You can use  `tf.keras.backend.get_session()` to get the session, 
then you can rewrite the graph created by keras.
Note that you should call `sess.run(tf.global_variables_initializer())` after rewrite the graph.

However, there is still another issue in this code.
If you save keras model and load again, fakequant layer disappear...QQ
Because keras does not know this layer.
You need to call `tf.contrib.quantize.create_training_graph()` again to get the training graph.
However, you cannot initialized the variable... because the min max in fakequant may disappear.
I still do not know how to solve this problem...How about support in Tensorflow 2.0?@suharshs for quantization + KerasHi, we are actively working on a Keras replacement for contrib/quantize. We hare hoping to have it ready by the end of Q2. Thanks!@rocking5566 very interesting implementation! Did you also freeze the graph and convert it to a .tflite model?@rocking5566 so how can i use this quantization with fine-tuning and transfer learning? do you have any code for that?Following @rocking5566 code, you can recover a model with something like:
 

    tf.keras.backend.clear_session()
    g = tf.keras.backend.get_session().graph
    with tf.Session(graph=g) as session:

        model_clean = ...
        tf.contrib.quantize.create_eval_graph(input_graph=g)
        optimizer_not_used, loss = ...

        # initialize automatically quantized variables
        session.run(tf.global_variables_initializer())
        # compile the model
        model_clean.compile(optimizer=optimizer_not_used,
                            loss=loss,
                            metrics=['accuracy'])
        # recover the model
        saver = tf.train.Saver()
        saver.restore(session, model_path)`

@suharshs In https://www.tensorflow.org/model_optimization/guide/roadmap there is no specification of how the issue will be addressed. Is there any place where discussions can be followed?

Thanks> @rocking5566 very interesting implementation! Did you also freeze the graph and convert it to a .tflite model?

Did anyone find a way to convert the quantized model to a tflite file? 

I tried to adapt the instructions from the github readme but with no success
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize@suharshs Is there any update for the "Keras replacement for contrib/quantize" or the timeline for implementation? I appreciate your assistance.I build simple keras model and convert it to tflite file for Edge TPU. It seems works well.

https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb@ohtaman Thank you very much for your help.
Unfortunately, I run into problems when using less trivial models.
I added a batchnorm layer in 
```
def build_keras_model():
    return keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(128, activation=tf.nn.relu),
        tf.keras.layers.BatchNormalization(),
        keras.layers.Dense(10, activation=tf.nn.softmax)
    ])
```

and now I get the following error complaining about the min value in the batchnorm being unitialized
```
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min
	 [[node training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min (defined at <ipython-input-4-13f3a7476e38>:17) ]]
	 [[node save/control_dependency (defined at <ipython-input-4-13f3a7476e38>:20) ]]
```

sorry to bother with that but I don't understand the origin of the error, it looks like there is a fake layer missing but I don't get why nor how we could specify to add it> 
> 
> @ohtaman Thank you very much for your help.
> Unfortunately, I run into problems when using less trivial models.
> I added a batchnorm layer in
> 
> ```
> def build_keras_model():
>     return keras.Sequential([
>         keras.layers.Flatten(input_shape=(28, 28)),
>         keras.layers.Dense(128, activation=tf.nn.relu),
>         tf.keras.layers.BatchNormalization(),
>         keras.layers.Dense(10, activation=tf.nn.softmax)
>     ])
> ```
> 
> and now I get the following error complaining about the min value in the batchnorm being unitialized
> 
> ```
> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min
> 	 [[node training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min (defined at <ipython-input-4-13f3a7476e38>:17) ]]
> 	 [[node save/control_dependency (defined at <ipython-input-4-13f3a7476e38>:20) ]]
> ```
> 
> sorry to bother with that but I don't understand the origin of the error, it looks like there is a fake layer missing but I don't get why nor how we could specify to add it

You may be able to do the following:

```
def build_keras_model():
    return keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(128, activation=tf.nn.relu),
        tf.keras.layers.BatchNormalization(),
        keras.layers.Dense(10, activation=tf.nn.softmax)
    ])

tf.keras.backend.clear_session()
g = tf.keras.backend.get_session().graph
with tf.Session(graph=g) as session:

    model_clean = build_keras_model()
    tf.contrib.quantize.create_eval_graph(input_graph=g)
    optimizer_not_used, loss = ...

    # initialize automatically quantized variables
    session.run(tf.global_variables_initializer())
    # compile the model
    model_clean.compile(optimizer=optimizer_not_used,
                        loss=loss,
                        metrics=['accuracy'])
    # recover the model
    saver = tf.train.Saver()
    saver.restore(session, model_path)
```
Thanks!

Using it without batchnorm works but when I add batchnorm I get:
```
ValueError: Input 0 of node batch_normalization_v1/cond/ReadVariableOp/Switch was passed float from batch_normalization_v1/gamma:0 incompatible with expected resource
```

do you have any idea why I don't get the same error? (I'm not well versed into tf.Session() and in general its APIs except for keras)@NatGr  Hi! I updated my sample notebook. This may help you.

1. Add BatchNormalization layer.
2. Remove create_eval_graph() after train_model.fit.
    - It is the cause of the FailedPreconditionError and I found that it is not necessary.
3. Add keras.backend.set_learning_phase(0)
   - Then we can avoid the ValueError

https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb@ohtaman, great! thank you very much (again)!
I still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture
```
def build_keras_model():
    return keras.Sequential([
        tf.keras.layers.Conv2D(16, kernel_size=3, activation="relu", padding="same", use_bias=False, input_shape=(28, 28, 1)),
        keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
        keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
        tf.keras.layers.AveragePooling2D(pool_size=7),
        tf.keras.layers.Flatten(),
        keras.layers.Dense(10, activation='softmax')
    ])
```

(you also need to change the inputs and command for the scipt to run):
```
train_images = np.reshape(train_images, [-1, 28, 28, 1])
test_images = np.reshape(test_images, [-1, 28, 28, 1])
```
and
```
tflite_convert \
    --output_file=model.tflite \
    --graph_def_file=frozen_model.pb \
    --inference_type=QUANTIZED_UINT8 \
    --input_arrays=conv2d_input \
    --output_arrays=dense/Softmax \
    --mean_values=0 \
    --std_dev_values=255
```

Here is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW

I get an error related to the batchnorm layers
```
2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
```
This issue is also faced here but they were not answered:
https://github.com/tensorflow/tensorflow/issues/27952

Do you have any idea on how to solve it?> @ohtaman, great! thank you very much (again)!
> I still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture
> 
> ```
> def build_keras_model():
>     return keras.Sequential([
>         tf.keras.layers.Conv2D(16, kernel_size=3, activation="relu", padding="same", use_bias=False, input_shape=(28, 28, 1)),
>         keras.layers.BatchNormalization(),
>         tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
>         keras.layers.BatchNormalization(),
>         tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
>         tf.keras.layers.AveragePooling2D(pool_size=7),
>         tf.keras.layers.Flatten(),
>         keras.layers.Dense(10, activation='softmax')
>     ])
> ```
> 
> (you also need to change the inputs and command for the scipt to run):
> 
> ```
> train_images = np.reshape(train_images, [-1, 28, 28, 1])
> test_images = np.reshape(test_images, [-1, 28, 28, 1])
> ```
> 
> and
> 
> ```
> tflite_convert \
>     --output_file=model.tflite \
>     --graph_def_file=frozen_model.pb \
>     --inference_type=QUANTIZED_UINT8 \
>     --input_arrays=conv2d_input \
>     --output_arrays=dense/Softmax \
>     --mean_values=0 \
>     --std_dev_values=255
> ```
> 
> Here is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW
> 
> I get an error related to the batchnorm layers
> 
> ```
> 2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
> ```
> 
> This issue is also faced here but they were not answered:
> #27952
> 
> Do you have any idea on how to solve it?

I'm having the exact same issue. Here's what my model looks like:

```
def build_keras_model():
  
    return keras.Sequential([
            keras.layers.Conv2D(32, (3, 3), padding="same", input_shape=(28,28,1)),
            keras.layers.Activation("relu"),
            keras.layers.BatchNormalization(axis=chanDim),
            keras.layers.Conv2D(32, (3, 3), padding="same"),
            keras.layers.Activation("relu"),
            keras.layers.BatchNormalization(axis=chanDim),
            keras.layers.MaxPooling2D(pool_size=(2, 2)),
            keras.layers.Dropout(0.25),
            keras.layers.Conv2D(64, (3, 3), padding="same"),
            keras.layers.Activation("relu"),
            keras.layers.BatchNormalization(axis=chanDim),
            keras.layers.Conv2D(64, (3, 3), padding="same"),
            keras.layers.Activation("relu"),
            keras.layers.BatchNormalization(axis=chanDim),
            keras.layers.MaxPooling2D(pool_size=(2, 2)),
            keras.layers.Dropout(0.25),
            keras.layers.Flatten(),
            keras.layers.Dense(512),
            keras.layers.Activation("relu"),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(classes),
            keras.layers.Activation("softmax")
    ])


```

ERROR:
`2019-06-08 15:09:20.754790: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
`


[Link to modified ipynb](https://colab.research.google.com/gist/1n1n1t3/19a634f76bbfca9051efdfa865aba93e/edgetpu-with-keras.ipynb#scrollTo=gSfaOfSTnXUR)> @ohtaman, great! thank you very much (again)!
> I still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture
> 
> ```
> def build_keras_model():
>     return keras.Sequential([
>         tf.keras.layers.Conv2D(16, kernel_size=3, activation="relu", padding="same", use_bias=False, input_shape=(28, 28, 1)),
>         keras.layers.BatchNormalization(),
>         tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
>         keras.layers.BatchNormalization(),
>         tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
>         tf.keras.layers.AveragePooling2D(pool_size=7),
>         tf.keras.layers.Flatten(),
>         keras.layers.Dense(10, activation='softmax')
>     ])
> ```
> 
> (you also need to change the inputs and command for the scipt to run):
> 
> ```
> train_images = np.reshape(train_images, [-1, 28, 28, 1])
> test_images = np.reshape(test_images, [-1, 28, 28, 1])
> ```
> 
> and
> 
> ```
> tflite_convert \
>     --output_file=model.tflite \
>     --graph_def_file=frozen_model.pb \
>     --inference_type=QUANTIZED_UINT8 \
>     --input_arrays=conv2d_input \
>     --output_arrays=dense/Softmax \
>     --mean_values=0 \
>     --std_dev_values=255
> ```
> 
> Here is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW
> 
> I get an error related to the batchnorm layers
> 
> ```
> 2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
> ```
> 
> This issue is also faced here but they were not answered:
> #27952
> 
> Do you have any idea on how to solve it?

adding parameter fused=False did the trick for me. I hope it's not messing much with performance. Accuracy seems to be unaffected. 

`keras.layers.BatchNormalization(fused=False),
`> > @ohtaman, great! thank you very much (again)!
> > I still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture
> > ```
> > def build_keras_model():
> >     return keras.Sequential([
> >         tf.keras.layers.Conv2D(16, kernel_size=3, activation="relu", padding="same", use_bias=False, input_shape=(28, 28, 1)),
> >         keras.layers.BatchNormalization(),
> >         tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
> >         keras.layers.BatchNormalization(),
> >         tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation="relu", padding="same", use_bias=False),
> >         tf.keras.layers.AveragePooling2D(pool_size=7),
> >         tf.keras.layers.Flatten(),
> >         keras.layers.Dense(10, activation='softmax')
> >     ])
> > ```
> > 
> > 
> > (you also need to change the inputs and command for the scipt to run):
> > ```
> > train_images = np.reshape(train_images, [-1, 28, 28, 1])
> > test_images = np.reshape(test_images, [-1, 28, 28, 1])
> > ```
> > 
> > 
> > and
> > ```
> > tflite_convert \
> >     --output_file=model.tflite \
> >     --graph_def_file=frozen_model.pb \
> >     --inference_type=QUANTIZED_UINT8 \
> >     --input_arrays=conv2d_input \
> >     --output_arrays=dense/Softmax \
> >     --mean_values=0 \
> >     --std_dev_values=255
> > ```
> > 
> > 
> > Here is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW
> > I get an error related to the batchnorm layers
> > ```
> > 2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
> > ```
> > 
> > 
> > This issue is also faced here but they were not answered:
> > #27952
> > Do you have any idea on how to solve it?
> 
> adding parameter fused=False did the trick for me. I hope it's not messing much with performance. Accuracy seems to be unaffected.
> 
> `keras.layers.BatchNormalization(fused=False), `

Amazing, thank you!
It seems it finally works for me as well@1n1n1t3 did you test the runtime of your generated tf-lite files?
training seems to go correctly, the input and output details of my .tflite file seem consistent
```
>>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')
>>> interpreter.allocate_tensors()
>>> input_details = interpreter.get_input_details()
>>> output_details = interpreter.get_output_details()
>>> print(input_details)
[{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]
>>> print(output_details)
[{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]
```

The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).

However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.
Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.
This should not be the case.> @1n1n1t3 did you test the runtime of your generated tf-lite files?
> training seems to go correctly, the input and output details of my .tflite file seem consistent
> 
> ```
> >>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')
> >>> interpreter.allocate_tensors()
> >>> input_details = interpreter.get_input_details()
> >>> output_details = interpreter.get_output_details()
> >>> print(input_details)
> [{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]
> >>> print(output_details)
> [{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]
> ```
> 
> The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).
> 
> However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.
> Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.
> This should not be the case.

Are you using the Coral USB Accelerator ? I tried it on the Coral Dev Board and the model I posted above is very fast. It goes something like 1500 fps. I'm used the MNIST Fashion dataset (input: 28,28,1) though. Not sure if that is fast, normal or slow for that, tbh, but the MobilenetSSD models run at about 300 fps. 

For sure you are not supposed to get slower performance on quantized models though. Maybe the problem is something specific to the Raspberry PI and the environment. 


> > @1n1n1t3 did you test the runtime of your generated tf-lite files?
> > training seems to go correctly, the input and output details of my .tflite file seem consistent
> > ```
> > >>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')
> > >>> interpreter.allocate_tensors()
> > >>> input_details = interpreter.get_input_details()
> > >>> output_details = interpreter.get_output_details()
> > >>> print(input_details)
> > [{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]
> > >>> print(output_details)
> > [{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]
> > ```
> > 
> > 
> > The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).
> > However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.
> > Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.
> > This should not be the case.
> 
> Are you using the Coral USB Accelerator ? I tried it on the Coral Dev Board and the model I posted above is very fast. It goes something like 1500 fps. I'm used the MNIST Fashion dataset (input: 28,28,1) though. Not sure if that is fast, normal or slow for that, tbh, but the MobilenetSSD models run at about 300 fps.
> 
> For sure you are not supposed to get slower performance on quantized models though. Maybe the problem is something specific to the Raspberry PI and the environment.

Nope, I'm running on native Raspberry PI (it's for my master thesis, I just use the Raspberry PI as a benchmarck which is why I don't use USB accelerators). I did not precise it but I'm using a MobileNetv1 adapted to CIFAR-10's input size.

Yeah that extremely weird, at least it's not specific to the Raspberry Pi since it happens on my desktop computer as well. It might be worthwile to test the floating point 32 model in your use case as well (I'm not asking for anything, I'm already extremely grateful for your help with the BatchNorm).@ohtaman, perhaps you have guidance on resolving the problem I'm having.  I've been using [keras.applications MobileNet](https://keras.io/applications/#mobilenet) and run into missing quantization-aware training values when restoring the checkpoint after training.

    NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

    Key conv1/act_quant/max not found in checkpoint
	     [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]
	     [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]

Here's a modified Colab that demonstrates the problem:
https://colab.research.google.com/drive/15itdlIyLmXISK6SDAzAFGUgjatfVr0Yq

> @ohtaman, perhaps you have guidance on resolving the problem I'm having. I've been using [keras.applications MobileNet](https://keras.io/applications/#mobilenet) and run into missing quantization-aware training values when restoring the checkpoint after training.
> 
> ```
> NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key conv1/act_quant/max not found in checkpoint
>      [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]
>      [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]
> ```
> 
> Here's a modified Colab that demonstrates the problem:
> https://colab.research.google.com/drive/15itdlIyLmXISK6SDAzAFGUgjatfVr0Yq

I had the same use case and I ran into a lot of issues. I think the simplest is to modify the architecture a bit and code it yourself. Resizing to 224*224 is much more resource consuming than having a 28*28 base input. Personally I only managed to make it work when using Sequential models, you should set fused=False in you Batchnormalization layers and I also integrated the activations to the convolutions layers, I don't remember what problem I had when I did that and it might be useless when taking the other adaptations into account.

That being said, I still have the huge performance gap I mention earlier that nullifies the interest of quantizing the model.

Here is the code I used (for my build_keras_model function, the depth is measured in number of blocks and not number of layers, it is not written anywhere but it build a MobileNetv1) https://github.com/NatGr/Master_Thesis/blob/master/training_from_scratch/train_quantize_save.py
@NatGr, thanks for the alternative suggestion!  

The reason I'm using the Keras Applications MobileNet is to retain the pre-trained weights.  As far as performance, my input images are full scale images from a 12 MP CMOS camera, in which areas of interest are resized to 224x224 for inference.

I am unaware on how to modify the network so that I can load pre-trained weights into the custom network with modified BNs, which I believe would also address my problem.@oursland @NatGr 

the cause of the error is that  the result of the create_training_graph and the create_eval_graph wereinconsistent. I found that just we have to do is adding keras.backend.learning_phase(1) before the create_training_graph function. 
See

https://colab.research.google.com/gist/ohtaman/578bd54aaa5a44234d477cfc61b21531/edgetpu-with-keras-applications.ipynb

The code above works in this case but not for some other cases. The Quantization Aware Trainingprocess is complicated and I don't understand everything.

In TensorFlow2.0, we can use Post Training Integer Quantization to get EdgeTPU model. I haven't tried it yet, but I think this is a simpler way and will fit most of you.

https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9baBased on feedback that the contrib/quantize quantization-aware training tool is a bit brittle and hard to use on some model architectures, we have released a [post-training integer quantization tool](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba), that requires a small calibration dataset. Please take a look at the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb) and give it a try, it should work much better! And let us know if you run into any issues.

We are rethinking and working on an api to replace contrib/quantize quantization-aware-training (although post-training quantization above should be sufficient for the majority of use cases), but do try Post Training Integer Quantization as a much simpler approach :)

Thanks!
-Suharsh
@ohtaman @suharshs 

Sorry, I got a bit confused right now. Can we use Post-training integer quantization tool for creating models that run on the EdgeTPU on Google Coral devices (Dev board/USB accelerator) ? @1n1n1t3 Yes, it says it makes the weights and activations int8 which is what Coral needs. As far as I can find, at the moment Coral and EdgeTPU essentially mean the same thing as Google hasn't released any other products to the public using this chip.@suharshs  I tried the new post-training integer quantization tool on my keras model and got following error:
```
  File "quant_convert.py", line 65, in _main
    tflite_quant_model = converter.convert()
  File "/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py", line 923, in convert
    inference_output_type)
  File "/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py", line 201, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File "/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/calibrator.py", line 75, in calibrate_and_quantize
    self._calibrator.FeedTensor(calibration_sample)
  File "/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py", line 112, in FeedTensor
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)
ValueError: Cannot set tensor: Got tensor of type NOTYPE but expected type FLOAT32 for input 3, name: input_1
```
But when checking the input info I do see it is a float32 tensor:
```
    model = load_model('test.h5')
    print(model.input)   ###Tensor("input_1:0", shape=(?, 416, 416, 3), dtype=float32)
```
Anything I missed?Can you share your code that constructs the representative_dataset and calls the TFLiteConverter?

It looks like the representative dataset is yielding an array that is not of type float.@suharshs  That's it. I yield a numpy array in the representative dataset generator with default numpy dtype, which is float64:
```
    def data_generator():
        for i in range(sample_num):
            image, _ = get_random_data(annotation_lines[i], model_input_shape, random=True)
            image = np.array([image]) #image.dtype==float64
            yield [image]

    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = data_generator
```
Now it has been fixed with "image = np.array([image], dtype=np.float32)" and the convertion works. Thanks a lot for your help!@suharshs  when verifying the converted model, following error happens:
```
>>> interpreter = tf.lite.Interpreter(model_path='model_quant.tflite')
INFO: Initialized TensorFlow Lite runtime.
>>> interpreter.allocate_tensors()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/root/.virtualenvs/py3tf_nightly/lib/python3.5/site-packages/tensorflow_core/lite/python/interpreter.py", line 184, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File "/root/.virtualenvs/py3tf_nightly/lib/python3.5/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/dequantize.cc:67 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteFloat16 was not true.Node number 64 (DEQUANTIZE) failed to prepare.
```
I checked the tflite model file in visualize tool Netron. The error node 64 is a Dequantize node with following properties:
![2](https://user-images.githubusercontent.com/25219223/59481812-7627be00-8e98-11e9-9b4c-a7c3d8b149ee.jpg)
I have a hunch on the issue (int32 intermediate tensors) will send a fix ASAP.

edit: I have found and verified a fix, should be submitted soon. thx@suharshs
I'm trying to get efficientnet-b3 to Google Coral Dev Board. 
I get this error when using the target ops parameter:
RuntimeError: Quantization not yet supported for op: REDUCE_MAX

Can I do something about it without modifying the model?


UPDATE: Nevermind me. The problem was actually from my added top layers ....... the network converts successfully (yay). Too bad it doesn't want to go through the EdgeTPU compiler and it doesn't even tell why... :(  It's just "Internal compiler error. Aborting! " message.Hello
I have tried to convert my keras model using post training integer quantization following the tutorial from this Medium [post](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba)

I keep getting this error:
ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.

I have tried putting it putting the model into a tf.function to get a concrete function following [this](https://www.tensorflow.org/lite/r2/convert/concrete_function), but then at compilation I still get the error that the model is not quantized.


@suharshs 

UPDATED: Sorry, at first I wrote 'quantization aware training' in mistake. I want to know about Post Training Integer Quantization.

--

Hi, I tried post training  integer quantization but it looks like the generated tflite model doesn't support EdgeTPU. Is that correct? (post training 
integer quantization  does not support EdgeTPU yet, right?)

In fact, I got `RuntimeError: Error in interpreter initialization` at initialization of BasicEngine.
If this behavior is not supposed, I will send a issue separately.Hi the expectation is that is should work, but currently EdgeTPU requires the inputs and outputs to be uint8. 

This can be achieve by setting:
converter.inference_input_type and converter.inference_output_type to tf.uint8.

Additionally, by default Optimizations.DEFAULT with representative data set will leave ops that dont have quantized implementations in floating point. This does not work with EdgeTPU. To guarantee that all ops are quantized (and give you an error saying which ops aren't) you can set:"

converter.target_ops = [tf.lite.OpSet.TFLITE_BUILTINS_INT8]

Hope that helps!@suharshs 
Thanks,  great! I could build custom EdgeTPU model using post training integer quantization.
I can share the example notebook (I implimented an small autoencoder):
https://colab.research.google.com/gist/ohtaman/7101e90f1f71a1cb054364d179845866/tf2-0-edgetpu-keras-cnn-autoencoder.ipynb

In this notebook, I have to use `tf.compat.v1.lite.TFLiteConverter` instead of `tf.lite.TFLiteConverter`.
It seems that `tf.lite.TFLiteConverter` does not use inference_input/output_type property in quantization process.  
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L395Hi @suharshs , now I've been able to do the post-training integer quantization on my pretrained keras model and run it in TF-lite runtime. But I found the inference accuracy seems got some loss after the convertion. Since it is an object detection model for CV tasks, so what I saw is the inferenced object location and confidence score result got obvious deviation with the ground truth.
I tried to feed the representative dataset generator with more calibration samples, but doesn't help. When I rollback the converter to "post_training_quantize" mode, the accuracy performance being fine, but the latency goes bad since it's a float32 model now.
Is there any way for me to get a better accuracy on the integer quantized model? Should I retrain the model under the new tf-nightly build env?Hi @ohtaman, @suharshs.
I have been able to compile the model to a tf-lite file (using @ohtaman method --thanks for that by the way -- and thus using "tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(tmp_keras_file)").
Post-training quantization reduces model file size without affecting inference time too much, however, when trying to use tensorflow benchmark tool version 1.13.1 (a tf binary using the tf-ilte binaries); I get the following error:
```
Didn't find op for builtin opcode 'AVERAGE_POOL_2D' version '2'
```

Should it not be "tf-lite 1.13.1" compatible since I use tf.compat.v1.lite.TFLiteConverter?

I also tried with a binary for tf-lite 2 (EDIT: tf-nightly) and get the same error but I might have a problem with that binary (I'm working on it).

EDIT: managed to compile the binary of tf.nightly, it works now!Hi guys. Thanks for the useful thread!
I'm trying to quantize trained model using `tf.compat.v1.lite.TFLiteConverter` and `tf.lite.TFLiteConverter` but both produce same error: `Invalid quantization params for op MAX_POOL_2D at index 2 in subgraph 0`. 
Do you know what could be the case? @suharshs 
 
Code for `tf.compat.v1`
```
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('ssrnet_model.h5')
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()
with open('ssrnet_model.tflite', 'wb') as o_:
    o_.write(tflite_model)
```

Error details:
```
RuntimeError                              Traceback (most recent call last)
<ipython-input-27-4ab92887ea65> in <module>()
      4 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
      5 
----> 6 tflite_model = converter.convert()
      7 
      8 with open('ssrnet_model_a.tflite', 'wb') as o_:

3 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, input_py_type, output_py_type, allow_float)
    113 
    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):
--> 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister
    117 CalibrationWrapper_swigregister(CalibrationWrapper)

RuntimeError: Invalid quantization params for op MAX_POOL_2D at index 3 in subgraph 0
```Hello @suharshs 

Have you fixed/submitted the correction already? 

Thanks!you might want to check into this

https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9baHello,

actually, I did everything that is explaining there. I get to the point of converting the tflite model. However, when I try to use the edgetpu compiler I receive an error in compilation time. As good as it is, the error does not give any information appart from a path and a number (google does not say anything about it).

Have you encountered a similar problem?> Hello,
> 
> actually, I did everything that is explaining there. I get to the point of converting the tflite model. However, when I try to use the edgetpu compiler I receive an error in compilation time. As good as it is, the error does not give any information appart from a path and a number (google does not say anything about it).
> 
> Have you encountered a similar problem?

Hi, 

I have encountered the same problem trying to convert an EfficientNet model and deploy it t Coral Dev Board. I wrote email to Coral team and I was simply replied that "We don't support converting EfficientNet to EdgeTPU yet. Check the site regularly for updates"... I guess this is the cost of being an early adopter. I think they are having some problems with the compiler compiling more complex and newer models and are working on it, but no word on what and when.Hi,

the model I use is actually a stack of Dense Keras layers and I am getting that error. The problem is that we do not know, from the compiler, what is the trouble coming from. Is it the model? Is it the tflite compilation? Is it another thing?@1n1n1t3 to be honest, I ended up using the Intel Neural Compute Stick 2 just because I needed to get something working soon and the software stack is more mature. I still like the Coral and Google has been very upfront that it is still in beta, so no complaints there.

The NCS 2 is a very different product as it does everything in FP16 and the architecture at a high level sounds closer to a desktop GPU, which probably means it can't do as many matrix multiplications as the EdgeTPU, but also means there is much less hassle getting models to run as you can just round off the weights from FP32 to FP16 and the performance is generally OK.

I think once the software stack matures and we can easily combine the high level Keras API with the raw power of the coral devices this will be an awesome product!> Hi,
> 
> the model I use is actually a stack of Dense Keras layers and I am getting that error. The problem is that we do not know, from the compiler, what is the trouble coming from. Is it the model? Is it the tflite compilation? Is it another thing?

I think you are doing something wrong then, because I managed to get other models to compile and run on the EdgeTPU. Did you convert the input/outputs in uint8 setting converter.inference_input_type and converter.inference_output_type to tf.uint8 ? 

@ed-alertedh Yes, but if we compare NCS 2 with EdgeTPU we get a game changing performance gain which is essential for ML devices on the edge. The loss of conferting to 8bit integers seems negligible in big models. Let's hope they polish things and get out of beta soon. 

Hello @1n1n1t3,

I am using this piece of code to convert my model to tflite, I indeed use inference_types as tf.uint8. 

```python
converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)
converter.representative_dataset = self.representative_dataset_gen
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

def representative_dataset_gen(self):
    for i in range(len(self.samples)):
        data = self.samples[i: i + 1]
        yield [data]
```

Actually, in the code I found they were using different pieces of code for the first two lines, but this did not work for me. Those original lines were:

```python
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_model_path)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
```

I save my keras models with a simple: `model.save(filename + '.h5')` and my model looks like:

```python
model = Sequential()
model.add(Dense(units=int(64), input_shape=(layers[1],layers[2]), activation=activation))
model.add(Dense(units=int(64), activation=activation))
model.add(Flatten())
model.add(Dense(units=layers[3], activation='linear'))
```

Do you use some different code?

CheersI was able to compile  keras mobilenet model for Edge-TPU using following steps:
1. Train model using a quantization-aware training technique
2. Convert model using  post-training quantization 
Important: tf-nightly  and tf.compat.v1.lite.TFLiteConverter were used
Thanks, @ohtaman for the tip https://github.com/tensorflow/tensorflow/issues/27880#issuecomment-503400613

All of this quantization for Edge-TPU looks like black magic right now, to be honest. > @oursland @NatGr
> 
> the cause of the error is that the result of the create_training_graph and the create_eval_graph wereinconsistent. I found that just we have to do is adding keras.backend.learning_phase(1) before the create_training_graph function.
> See
> 
> https://colab.research.google.com/gist/ohtaman/578bd54aaa5a44234d477cfc61b21531/edgetpu-with-keras-applications.ipynb
> 
> The code above works in this case but not for some other cases. The Quantization Aware Trainingprocess is complicated and I don't understand everything.
> 
> In TensorFlow2.0, we can use Post Training Integer Quantization to get EdgeTPU model. I haven't tried it yet, but I think this is a simpler way and will fit most of you.
> 
> https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba

Hi @ohtaman , I encountered the following error using your code:

`Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0708 19:10:21.210556 140681547597568 deprecation.py:323] From /home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
Traceback (most recent call last):
  File "common/experimental/leoli/pspnet_train/pspnet_train.py", line 192, in <module>
    [eval_net.output.op.name]
  File "/home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py", line 297, in convert_variables_to_constants
    source_op_name = get_input_name(node)
  File "/home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py", line 254, in get_input_name
    raise ValueError("Tensor name '{0}' is invalid.".format(node.input[0]))
ValueError: Tensor name 'conv1_1_3x3_s2_bn/cond/ReadVariableOp/Switch:1' is invalid.
` 

It happened when I was trying to freeze the model. > @suharshs
> Thanks, great! I could build custom EdgeTPU model using post training integer quantization.
> I can share the example notebook (I implimented an small autoencoder):
> https://colab.research.google.com/gist/ohtaman/7101e90f1f71a1cb054364d179845866/tf2-0-edgetpu-keras-cnn-autoencoder.ipynb
> 
> In this notebook, I have to use `tf.compat.v1.lite.TFLiteConverter` instead of `tf.lite.TFLiteConverter`.
> It seems that `tf.lite.TFLiteConverter` does not use inference_input/output_type property in quantization process.
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L395

@ohtaman Nice Work. It is working well. @ohtaman I have been compiling my own network. I also got an issue with UpSampling2D and I also used a custom one made out of basic units in TF. 

When I save the model after training and load the keras model, the prediction works fine. 

But when I convert it to a tflite model (which converts without giving any issues), when I use the tflite model to run the inference using the interpreter, it hangs in the line after the interpreter.invoke() and doesn't execute further. One other thing is the edgetpu_compiler also gives an unknown compilation issue when I used the generated tflite model. But the error is not clear. In my model, the only Non-quantized part is the Upsamling2D. But I replaced it. I am not sure what is wrong. Have any suggestions to debug this? > I build simple keras model and convert it to tflite file for Edge TPU. It seems works well.
> 
> https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb

@ohtaman Thanks for sharing your code, but I run into errors. The error differs depending on which docker image I use. Could you share your setup, please?

With the nightly-gpu image (Python 3.6.8, tf 1.15.0-dev20190715), I get an error when trying to convert the model:

```
Traceback (most recent call last):
  File "/usr/local/bin/tflite_convert", line 10, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/tflite_convert.py", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/tflite_convert.py", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/tflite_convert.py", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py", line 983, in convert
    **converter_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py", line 437, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py", line 155, in toco_convert_protos
    fp_debug.write(debug_info_str)
  File "/usr/lib/python3.6/tempfile.py", line 624, in func_wrapper
    return func(*args, **kwargs)
TypeError: a bytes-like object is required, not 'str'
```

On the other hand, with the latest-gpu-py3 image (Python 3.6.8, tf 1.14.0), I get an error when trying to test the tflite model with the interpreter of the Python API. It happens when it tries to allocate memory with the `allocate_tensors()` call.

```
INFO: Initialized TensorFlow Lite runtime.
Aborted (core dumped)
```

Have you run into such issues and if you did, how did you resolve them?@DocDriven 
I think one of your image arrays maybe like ['1.0', '2.0', '3.0'] not [1.0,2.0,3.0]. The string types are not supported I guess. Could you check that? @vibhatha 
I double checked it, but all entries are numpy arrays of type np.uint8. Also, the stable versions seems to have no problem with these inputs, so I assume something went wrong with the nightly build.@DocDriven 
I used Python 3.6.9 with the latest pip and tested this with
```
Python 3.6.9 (default, Jul  3 2019, 15:36:16) 
[GCC 5.4.0 20160609] on linux
>>> import tensorflow as tf
>>> print(tf.__version__)
2.0.0-beta1
>>> 
```

What is your setting? It feels like the 1.14 is the stable release for now. I assume you're using it. 
And also did you try using 

`converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)`

I think you have used toco converter. What I did was saved the keras model and then load it to convert using the above tflite converter. 
I followed most of the steps in @ohtaman code and guide in the google coral page as well. 

Could you try this one? > @ohtaman I have been compiling my own network. I also got an issue with UpSampling2D and I also used a custom one made out of basic units in TF.
> 
> When I save the model after training and load the keras model, the prediction works fine.
> 
> But when I convert it to a tflite model (which converts without giving any issues), when I use the tflite model to run the inference using the interpreter, it hangs in the line after the interpreter.invoke() and doesn't execute further. One other thing is the edgetpu_compiler also gives an unknown compilation issue when I used the generated tflite model. But the error is not clear. In my model, the only Non-quantized part is the Upsamling2D. But I replaced it. I am not sure what is wrong. Have any suggestions to debug this?

I changed the image sizes from 1024x1024 to smaller image size, then it worked pretty well. 

@suharshs  Is there a constraint on the data size in the inference level?

For instance flat 28x28 which is lesser size compared to 1024 x 1024. The last layer is not softmax, I can remember there is a 16K limit on softmax. Any idea on this? 
> @DocDriven
> I used Python 3.6.9 with the latest pip and tested this with
> 
> ```
> Python 3.6.9 (default, Jul  3 2019, 15:36:16) 
> [GCC 5.4.0 20160609] on linux
> >>> import tensorflow as tf
> >>> print(tf.__version__)
> 2.0.0-beta1
> >>> 
> ```
> 
> What is your setting? It feels like the 1.14 is the stable release for now. I assume you're using it.
> And also did you try using
> 
> `converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)`
> 
> I think you have used toco converter. What I did was saved the keras model and then load it to convert using the above tflite converter.
> I followed most of the steps in @ohtaman code and guide in the google coral page as well.
> 
> Could you try this one?

@vibhatha 
You are right, I am using the latest stable version of tf 1.14.0, however with Python 3.6.8. I also copy-pasted the code in combination with the official docker image latest-gpu-py3.

I have also tried using the Python API of the converter, but it worked equally well as the CLI, e.g. it produces a file, but fails when allocating memory.

I think the difference lies in the input format, as you are using a h5 file, not a protocol buffer. I guess this might be the reason why it is working out for you. Were you able to convert your tflite file with the edgetpu_compiler? My issue was that if I tried to do it this way, the compiler would not recognize that I have a quantized model.@ohtaman 
Good Custom Upsampling model 

[Custom Upsampling vs Original Upsampling](https://colab.research.google.com/drive/1oyWyijDUGwEP5sseaGDSBKiIqz8oiBPB)@DocDriven 
Yes, I was able to compile it. 
I first took a look at what @ohtaman has done. Then read all the docs that I can find on quantization with TF. 

Then I created 3 or 4 models custom builts using previous examples and my own examples. 
I was able to get done 100% TPU conversion at the edgetpu_compiler and the inference also worked well. 

My main issue is I am having troubles when handling larger image sizes. That's where I am currently stuck. 

Do you need a custom code? I can provide a simple one?
Make sure at every time when you load the data astype(np.float32) or any data type okay for quantization is put there. 

And also the 

`converter.representative_dataset = representative_dataset_gen`

this sample data must have the same data type as the input data type. Just pay attention to those parts. 

Then put all the right quantization params for the tflite converter. Then before testing on edge, for the sake of the argument, just load the tflite converted file and test on CPU machine using interpreter API. 
If it works well and fast, it may be okay. But there are certain issues shown in compiling due to Non-quantized model. What I used to get was a model is not quantized then I got an error when the model is too big (meaning the data size in the last layer is very big) saying compiling error. 

I guess this could help. @vibhatha 
Thank you for this walkthrough, I will investigate this the next few days. If you could provide a simple example, I would be very grateful. Maybe my bug is hiding in plain sight :/
Here is what I did using an existing code by another user. The license part is from him. 

I added a comment on what I did change. 
[Google Collab Link](https://colab.research.google.com/drive/1hQbm3gzOqEycFYj8woAUaYGPDRcHKXKb)

Hope this would help. 
@ohtaman  Example is a very neat code which would help you. @DocDriven 
The above link has a quantized code. Please check and tell me if there is an issue. Hello all, first of all, thanks for this helpful discussion I learned alot from the last two weeks. I wish that I was good enough to contribute to the community like you guys did. Honestly, I don't really understand the stuff behind the conversion. I am stuck at the last step: **convert frozen graph to tflite model** with the error:
```
F tensorflow/lite/toco/graph_transformations/quantize.cc:491] Unimplemented: this graph contains an operator of type Sqrt for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
```
So it complains about Sqrt opt, which has not implemented.

Below is the process I used:

**Train with** 

```     
tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)
train_sess.run(tf.global_variables_initializer()) 
```

**Save checkpoint after the training:**

```
saver = tf.train.Saver()
saver.save(train_sess, path_checkpoint)`
```

**Evaluate phase or whatever to get a frozen graph** (I don't understand what it does):
```
tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)
eval_graph_def = eval_graph.as_graph_def()
saver = tf.train.Saver()
saver.restore(eval_sess, path_checkpoint)

frozen_graph_def = tf.graph_util.convert_variables_to_constants(
    eval_sess,
    eval_graph_def,
    [model.output.op.name]
)

open(path_frozen_graph, 'wb').write(frozen_graph_def.SerializeToString())     
```

Everything seems fine until this point,  `tflite_model = converter.convert()` gives me the mentioned error:
```
converter = tf.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file=path_frozen_graph, 
    input_arrays=[x.input.name.split(':')[0] for x in model.layers[0:2] ], 
    output_arrays=[model.layers[-1].output.name.split(':')[0]]
)
converter.representative_dataset = representative_dataset
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {
    x : (0., 255) for x in input_arrays[0:2]
} 
# converter.default_ranges_stats = (-100, +100)
tflite_model = converter.convert()
open(path_lite_model, 'wb').write(tflite_model)
```
I know the problem because I am using lambda layer at the output, which is not in the compatibility table:
```
def euclidean_distance(vects):
    x, y = vects
    return tf.norm(x-y, ord='euclidean',  axis=1, keepdims=True)
#    I also replace those two line by below lines, but not work          
#    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)
#    return K.sqrt(K.maximum(sum_square, K.epsilon()))
...
layers.Lambda(euclidean_distance)([processed_a, processed_b])
...
```
If I replace the `euclidean_distance(vects)` function by something simpler like `return 2x - y` and add another layer: `layers.Dense(1)` to produce a same shape, my conversion process works just fine. 
Ok I know most of the api here are currently in experiment phase. But I really want to convert my customized model to tflite model. It looks like tensorflow doesn't support the operations I want to use. I don't know if there is any other way or not. Please let me know if there is, I appreciate that.
**Another interesting approach is to split the model**, here is the part output from my model summary:
```
...
lambda (Lambda)                 (None, 1)            0           sequential[1][0] 
...
```
which shows it has zero parametter. If you look at my lambda layer, it is just basic math. I also tried to split my model at Keras level into 2 models. Lets say model.layers[0:1] and model.layers[2:3] (for lambda). When I use it, I can feed the output of the first one to the input a second one. It works just fine.  So I can somehow get rid of the complexity of the last layer (lambda). However, when I train the whole model, they have to come with each other. I don't know if it is possible to  save the checkpoint that contains just the information of model.layers[0:1]. And convert to tflite from that. At the end, I expect to have only model.layers[0:1]  quantized and converted to tflite. I belive this is possible in theory but still I don't know how to make it. Manual modify the checkpoint or frozengraph may work?!, but it really scary to look at those files. Again, if you know a way, please let me know. 
Sorry for not sharing the code, it is not my properties. I sometimes feel bad when I am asking from the community but I can't contribute. But if you want to know anything, feel free to ask, I really want the solution for this problem@jimmyvo2410 
I am also no expert on the topic. But I what I can see is the operator shown in the error message is not yet supported for quantization. 
If you take a look at @ohtaman solution, the upsampling2D is not yet supported for quantization. So he has written a custom one using already quantization supported ops. This is something similar you might have to do. What I can see is this solution. But I am sure an expert may be able to give you a better answer. 
I also needed some of these ops quantized and I had to write custom ones from already existing components, for instance, I also needed Upsampling2D. Then I did a validation with a single layer of Upsamling2D and a custom model written by @ohtaman and seems like it is a very good match for the original op. 
So you may want to double-check how good is your custom model with respect to the original one, just do it with a single layer of whatever the component you want to write. I am sure someone else might have the same problem and there can be a solution already in an existing thread. 

Hope this helps. 
> Here is what I did using an existing code by another user. The license part is from him.
> 
> I added a comment on what I did change.
> [Google Collab Link](https://colab.research.google.com/drive/1hQbm3gzOqEycFYj8woAUaYGPDRcHKXKb)
> 
> Hope this would help.
> @ohtaman Example is a very neat code which would help you.

@vibhatha 
Thank you for your help. I have tested your code over the last few days, with mixed results.

- When using the docker image with the TF 2.0.0-beta1, your code runs just fine. But as soon as I switch back to TF 1.14.0, I run into an error:

```
... (error-free output)

Traceback (most recent call last):
  File "test_main.py", line 205, in <module>
    tflite_fname = gen_tflite()
  File "test_main.py", line 164, in gen_tflite
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 783, in __getattribute__
    return object.__getattribute__(self, name)
AttributeError: 'TFLiteConverter' object has no attribute 'target_spec'
```

Is there an equivalent of this attribute in v1? Also, you do not seem to prepare the graph for usage with quantized params as one had to do before. This seems to be new and happens under the hood somehow. The magic has to happen within these few lines of code:

```
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
```

It would be nice if you could tell me where you found the documentation for these options. Thanks again!@vibhatha thanks for the hint, I saw that code but never thougth it was about the same issue> > Here is what I did using an existing code by another user. The license part is from him.
> > I added a comment on what I did change.
> > [Google Collab Link](https://colab.research.google.com/drive/1hQbm3gzOqEycFYj8woAUaYGPDRcHKXKb)
> > Hope this would help.
> > @ohtaman Example is a very neat code which would help you.
> 
> @vibhatha
> Thank you for your help. I have tested your code over the last few days, with mixed results.
> 
> * When using the docker image with the TF 2.0.0-beta1, your code runs just fine. But as soon as I switch back to TF 1.14.0, I run into an error:
> 
> ```
> ... (error-free output)
> 
> Traceback (most recent call last):
>   File "test_main.py", line 205, in <module>
>     tflite_fname = gen_tflite()
>   File "test_main.py", line 164, in gen_tflite
>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
>   File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 783, in __getattribute__
>     return object.__getattribute__(self, name)
> AttributeError: 'TFLiteConverter' object has no attribute 'target_spec'
> ```
> 
> Is there an equivalent of this attribute in v1? Also, you do not seem to prepare the graph for usage with quantized params as one had to do before. This seems to be new and happens under the hood somehow. The magic has to happen within these few lines of code:
> 
> ```
> converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
> converter.representative_dataset = representative_dataset_gen
> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
> converter.inference_input_type = tf.uint8
> converter.inference_output_type = tf.uint8
> converter.optimizations = [tf.lite.Optimize.DEFAULT]
> ```
> 
> It would be nice if you could tell me where you found the documentation for these options. Thanks again!

@DocDriven 

In 1.14 there is no field called 

`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`

Instead please use 

`converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`

What I do is, I keep a generic converter which is written with a factory design pattern to support compiling for multiple TF versions. 
Always take the support of the IntelliSense in the IDE, it will show you that that API extension is not in 1.14. 

This should resolve the issue. 
@vibhatha 
Thanks to your efforts, I was able to create a custom model in tf 2.0.0 and load it locally with an interpreter via the Python API. The effects of quantization is recognizable, but insignificant.

I just wanted you to ask if you have any experience with deploying tflite models on the Coral Edge TPU stick. I tried to do this with my successfully generated model, but are getting Segmentation faults all the time. If you can contribute, take a look at my corresponding GitHub issue: #30714 (even though this is for tf 1.14, I get the same error with tf 2.0.0)

Thanks again!@DocDriven 
Can you provide me the log file created after edgetpu_compiler was used?
In that one, please check how much of your operators were successfully converted with TPU support. 
It is a detailed report. 

Possible issues, first check the expected input from that loaded "edgetpu compiled model"? 
Make sure you provide input with a similar shape?
When the model is partially compiled for TPU support? I also got a segmentation fault? But I am no expert to tell why is that?

Debug Steps
------------------

1. After generating tflite model from your python tflite conerter api reload the model (tflite model) and do inference using the python API with the interpreter? (Examples are in plenty for this). 

If this works, it could mean that your h5 -> tflite conversion is good. 

2. Then convert the tflite with edgetpu_compiler and look at the logs. 

If everything is converted with TPU support, I think you're good. 

3. Then doing inference in TPU device, make sure the provided input array is correct?

If any of your ops seems to be not quantizable, edgetpu compiler will exit with an unknown error (they are continously working on this so there will be detailed info in future [My assumption]). 

Can you try these steps? I will also look at the code. 
Hi @ohtaman

Thanks for the great notebook.

I followed your notebook but I am getting following error with the edgetpu compiler. Everything else works fine. Any idea?

Edge TPU Compiler version 2.0.258810407
Invalid model: converted_model_simple_1_2.0.0-beta1.tflite
Model could not be parsed
INFO: Initialized TensorFlow Lite runtime.
ERROR: quantized_dimension must be in range [0, 1). Was 3.

@ankitmaurya001 
Only converting from frozen graph works for me with tf1.14.0. Try this:
```
# aware training
tf.keras.backend.clear_session()
train_graph = tf.Graph()
train_sess = tf.Session(graph=train_graph)
tf.keras.backend.set_session(train_sess)

with tf.device('/gpu:0'): 
    with train_graph.as_default():

        model = build_keras_model()

        tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)
        train_sess.run(tf.global_variables_initializer())    

        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        model.fit(train_images, train_labels, epochs=2)
        save_model(model, path_model)

        # save graph and checkpoints
        saver = tf.train.Saver()
        saver.save(train_sess, path_checkpoint)    

# evaluate ?! I am not sure
tf.keras.backend.clear_session()
eval_graph = tf.Graph()
eval_sess = tf.Session(graph=eval_graph)
tf.keras.backend.set_session(eval_sess)

with eval_graph.as_default():
    tf.keras.backend.set_learning_phase(0)
    model = build_keras_model()
    tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)
    eval_graph_def = eval_graph.as_graph_def()
    saver = tf.train.Saver()
    saver.restore(eval_sess, path_checkpoint)

    frozen_graph_def = tf.graph_util.convert_variables_to_constants(
        eval_sess,
        eval_graph_def,
        [model.output.op.name]
    )

    open(path_frozen_graph, 'wb').write(frozen_graph_def.SerializeToString())        

# convert to tflite from frozen graph
converter = tf.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file=path_frozen_graph, 
    input_arrays=[model.layers[0].input.name.split(':')[0]], 
    output_arrays=[model.layers[-1].output.name.split(':')[0]]
)

converter.representative_dataset = representative_dataset
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0] : (0., 255)}  # mean, std_dev
converter.default_ranges_stats = (-100, +100)
tflite_model = converter.convert()
open(path_lite_model, 'wb').write(tflite_model)

# convert to tpu lite version
cmd = "edgetpu_compiler %s -o %s" % (path_lite_model, path_export)
print(os.popen(cmd).read())

```I am trying to train a neural network that is done with convolutional 1d layers. I can train it without problems but when it comes to tflite_convert I have the following issue:

`2019-07-31 16:51:40.890861: F tensorflow/lite/toco/tooling_util.cc:1709] Array conv1d/Relu, which is an input to the Conv operator producing the output array conv1d_1/Relu, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.`

Note that if I use the same identical code without the convolutional1d (only dense layers) I have no problem and I can convert to tflite and compile for TPU. What is the problem? In my model I absolutely need the convolutional1dYes, the issue clearly mentions what you need to do. It is better to add a representative_dataset from your dataset. If you can follow @ohtaman notebook provided in this thread.

`def representative_dataset_gen():
    for i in range(1000):
        yield [train_images[i: i + 1]]`

converter.representative_dataset = representative_dataset_gen

add this to your converter object which is an instance of 

`converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)`

Hope this would resolve your issue. 

Or you can check the default ranges min-max param in the converter API and set them. 
But if you add this representative dataset, it might go away. Actually I am not using the api converter but the command line program. Here you can inspect my code: https://colab.research.google.com/drive/1vY1Y-mxQBmrH-YzNgh3nLP2hkOPV8-1K
I forgot to mention that I am using quantization aware training, not post training quantization. So how should I use representetive dataset gen? Isn't this for post training quantization?
Thanks
`!tflite_convert --help`

check the necessary param. 

In your case, I think 

`--default_ranges_min DEFAULT_RANGES_MIN
                        Default value for min bound of min/max range values
                        used for all arrays without a specified range,
                        Intended for experimenting with quantization via
                        "dummy quantization". (default None)
  --default_ranges_max DEFAULT_RANGES_MAX
                        Default value for max bound of min/max range values
                        used for all arrays without a specified range,
                        Intended for experimenting with quantization via
                        "dummy quantization". (default None)`

Try to set these values. @jimmyvo2410 
Using quantization aware training worked for me for edgeTPU. Thanks to @1n1n1t3 

I am facing issues in using post training integer quantization.

Hi @vibhatha ,
I tried your notebook . Was able to compile the model for EdgeTPU. 
But when I added a CONV2d layer at the top. I am facing an issue with the edge TPU complier .

```
Edge TPU Compiler version 2.0.258810407
Invalid model: converted_model_simple_1_2.0.0-beta1.tflite
Model could not be parsed
INFO: Initialized TensorFlow Lite runtime.
ERROR: quantized_dimension must be in range [0, 1). Was 3.
```

Here is the modified link.
https://colab.research.google.com/drive/13wH9eyjQ_YkBBiQhEoo6lDxKIKfRaboA

Any idea ?

Where did you add the layer?
I cant see the modification?

On Thu, Aug 1, 2019 at 6:07 AM truthSeeker <notifications@github.com> wrote:

> @jimmyvo2410 <https://github.com/jimmyvo2410>
> Using quantization aware training worked for me for edgeTPU. Thanks to
> @1n1n1t3 <https://github.com/1n1n1t3>
>
> I am facing issues in using post integer quantization.
>
> Hi @vibhatha <https://github.com/vibhatha> ,
> I tried your notebook . Was able to compile the model for EdgeTPU.
> But when I added a CONV2d layer at the top. I am facing an issue with the
> edge TPU complier .
>
> Edge TPU Compiler version 2.0.258810407
> Invalid model: converted_model_simple_1_2.0.0-beta1.tflite
> Model could not be parsed
> INFO: Initialized TensorFlow Lite runtime.
> ERROR: quantized_dimension must be in range [0, 1). Was 3.
>
> Here is the modified link.
> https://colab.research.google.com/drive/13wH9eyjQ_YkBBiQhEoo6lDxKIKfRaboA
>
> Any idea ?
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorflow/tensorflow/issues/27880?email_source=notifications&email_token=AC45OE3RSENQKKS55BDOEATQCK7YHA5CNFSM4HGE2DLKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3KGFZQ#issuecomment-517235430>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AC45OEYEEBVMKDVPMISBZSLQCK7YHANCNFSM4HGE2DLA>
> .
>
-- 
Vibhatha Abeykoon
@vibhatha just at the bottom.


```
# Now change the input shape to (28,28,1)

#train_images = train_images.astype(np.float32)
#test_images = test_images.astype(np.float32)

train_images = train_images[:, :, :, np.newaxis].astype(np.float32)
test_images = test_images[:, :, :, np.newaxis].astype(np.float32)



#change the input to (28,28,1)
#Add CONV2D and Activation layer
def build_model_2():
    inputs = keras.layers.Input(shape=(28, 28,1))
    x = keras.layers.Conv2D(16, (3, 3), padding='same')(inputs)
    x = keras.layers.Activation('relu')(x)
    x = keras.layers.Flatten()(x)
    encoded = keras.layers.Dense(128, activation='relu')(x)
    x = keras.layers.Dense(10)(encoded)
    decoded = keras.layers.Dense(10, activation='softmax')(x)

    model = keras.models.Model(inputs, decoded)
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model
  
  
model = build_model_2()
train_model()
predictions = do_prediction()
#plot_stuff1()
#plot_stuff2()
#plot_stuff3()
summary1()
tflite_fname = gen_tflite()
inference()

print(train_images.shape)
print(test_images.shape)
print(model.summary())

```@ankitmaurya001 
make sure the representative data also provides the expected input layer's shape. Because I think 28,28 is the previous input shape. I will check this as well. 

Btw, which edge device are you using to test the tflite model after compiling it to edgetpu_compileable model? @vibhatha 
I think representative data takes the correct input shape as train_images.shape is changed to (28,28,1). 

I am using google coral dev board.

Thanks for the help!!

@ankitmaurya001 
Can you compile the code you shared? I cannot compile it. 
Are you sure you used 2.0.0-beta?@vibhatha 
No, with (28,28,1) shape and conv2d it does not even complie with edge TPU compiler .

I am facing the same error with @ohtaman notebook.
https://colab.research.google.com/gist/ohtaman/7101e90f1f71a1cb054364d179845866/tf2-0-edgetpu-keras-cnn-autoencoder.ipynb

It throws the same error. Only difference I can think of is the change in compiler version.

**@ohtaman  notebook uses 1.0.249710469
While my version is 2.0.258810407.** 

I am not sure where to get the old complier. Might be interesting to check.

Any thoughts?No I mean the training part doesn't happen. Can you unit test your code after adding this new layer? 

Step 1, check model.train and then model.predict

If these two checks out, we can unit test the other components. I had an issue in running the training model. @ankitmaurya001 
I tested @ohtaman notebook previously and it did work fine. 
Let me check it again. I developed models in both 1.14 and 2.0.0-beta and they work fine. 
Also, I recently updated my dev board with the July update with 2.11 runtime. 
But I tested with the default runtime which was there (I guess it was a setup a while ago) and it worked just fine. I have both post quantized and qt aware models up and running in tf 1.14 but they are also working with tf 2.0.0-beta as well. I doubt runtime is the issue or tf vesion. Could you do the unit testing? @vibhatha 

I cleaned up the code a little. Please check you should be able to run now. Yes I am using 2.0.0-beta .

I am thinking it has to do with the edge TPU compiler version not the tf version.

```
%%bash
edgetpu_compiler -v
Edge TPU Compiler version 2.0.258810407
```

Which compiler version you used ?Ah yes, when I got the latest updates on the dev board, it was updated with the latest. Now the runtime is v.12 and my compiler version is Edge TPU Compiler version 2.0.258810407 [same as yours]

Are you able to compile with the latest version (with CONV2D + 3D input shape)?I cannot compile yours, but I definitely have a set of conv2d layers in the design, but I didn't get such issue. I also use 3D inputs. I will look into this. Till now I couldn't figure out why.
There is a stack-overflow question raising a similar concern. Did you check this with existing issues in TF Github issues? Did you compile your design with the latest compiler without any issues?

Yeah, I saw the stack overflow concern, but still no answers :(.
Checking the TF Github issues now !!

Yes, I was able to get it compiled no issues. Yes, the main reason is the error message is not clear. If it points out to a specific cause. I think an API expert will definitely understand why this is happening. I am not sure why it is happening. It has to be something with the PQ related conversion params. Nothing to do with the model. Some param is missing or some param is inserted incorrectly. I am sorry, I cannot give a 100% correct answer. @vibhatha 
No issues, thanks for the help anyways. Will explore further !!I am using post training quantization with a function similar to the @ohtaman one, mine is:
`def representative_dataset_gen():
    for i in range(num_calibration_steps):
        yield [x_train_scaled[i:i+1]]`
but I always get the following error: 
`ValueError: Cannot set tensor: Got tensor of type NOTYPE but expected type FLOAT32 for input 16, name: dense_input`
The variable `x_train_scaled` is a numpy array containing the dataset (each row is one example of the dataset). What's wrong?`diff = num_calibration_steps - x_trained_scaled.shape[0]  `

What is this value?Its -20475, since the dimension of the `x_train_scaled` (is 20575) it's much bigger than the `num_calibration_step` (I used 100)Can u iterate through the yielding values can make sure all values are of type FLOAT32?
Yes, this is a generic output of what is yielding: `[array([[ 0.04949207, -0.75925859, -0.30664617, ..., -0.38399959,
         2.03520339, -0.77253633]])]`so all of the values are dtype FLOAT32. 
Do you have a code snippet? It is hard to tell otherwise. Sure, the neural network is the following very simple:


def create_model():
    model = Sequential()
    model.add(Dense(500, input_shape=(time_periods,), activation="relu"))
    model.add(Dropout(0.4))
    model.add(Dense(500, activation="relu"))
    model.add(Dropout(0.4))
    model.add(Dense(500, activation="relu"))
    model.add(Dropout(0.4))
    model.add(Dense(500, activation="relu"))
    model.add(Dropout(0.4))
    model.add(Dense(1, activation="sigmoid"))
    return model


Then it's just compiled with `model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])`. 
At the end It's saved with `model.save(filepath="/content/drive/My Drive/post_quantum.h5", include_optimizer=True)`. 

The code of the generator is:
``num_calibration_steps=100
def representative_dataset_gen():
    for i in range(num_calibration_steps):
        yield [x_train_scaled[i]]``
where `x_train_scaled` has been used during training, and has shape (20575, 4492).
The code for quantization and conversion is:
``converter = tf.lite.TFLiteConverter.from_keras_model_file("DenseModel.h5")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()``why haven't you set the input and output type?

`converter.representative_dataset = representative_dataset_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()`

Try something like this, but make sure you correctly add the input and output types based on your model. I get always the same error, nothing changesIf you have a google collab link, please share if it is okay with you. It is hard to tell without looking at the code. Sure I can, here's the link: https://colab.research.google.com/drive/1vFSEvu4mudfDb2vLMXtgGmQEMBqLwqnN
Let me know if you need the dataset too.
Thank you very much!No need, just mention the shape and types of X and YX has shape (20575, 4492); Y has shape 20575 (1D vector, is a classification problem with two classes, so elements of Y are or 1 or 0).@ulorentz 
I see that you are using 
`converter = tf.lite.TFLiteConverter.from_keras_model_file("/content/drive/My Drive/model_quant_aware.h5")`

Can you try this instead?

tf version : 2.0.0-beta1
```
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file("/content/drive/My Drive/model_quant_aware.h5")
converter.representative_dataset = representative_dataset_gen
converter.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]

```> @ulorentz
> I see that you are using
> `converter = tf.lite.TFLiteConverter.from_keras_model_file("/content/drive/My Drive/model_quant_aware.h5")`
> 
> Can you try this instead?
> 
> tf version : 2.0.0-beta1
> 
> ```
> converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file("/content/drive/My Drive/model_quant_aware.h5")
> converter.representative_dataset = representative_dataset_gen
> converter.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
> converter.inference_input_type = tf.uint8
> converter.inference_output_type = tf.uint8
> converter.optimizations = [tf.lite.Optimize.DEFAULT]
> ```

Good suggestion. If you look at the runtime warnings, it definitely shows that the depreciated APIs and suggest using this API. > Yes, I was able to get it compiled no issues. Yes, the main reason is the error message is not clear. If it points out to a specific cause. I think an API expert will definitely understand why this is happening. I am not sure why it is happening. It has to be something with the PQ related conversion params. Nothing to do with the model. Some param is missing or some param is inserted incorrectly. I am sorry, I cannot give a 100% correct answer.

@vibhatha 
If it is not much of a problem, can you share a simple notebook with 3D input and CONV2D layers 
which you are able to compile with the latest edge TPU compiler using post-training quantization?

Will be of great help!!@ankitmaurya001 
I will try to compile one snippet. Finally I am able to convert to tflite with post integer quantization, but now I am facing the same problem of @ankitmaurya001, that is `ERROR: quantized_dimension must be in range [0, 1). Was 3.`

You can inspect my code at https://colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7Actually I got the same error even with the @ohtaman code (I little modified it in order to work with tflite 2.0): https://colab.research.google.com/gist/ulorentz/4375fea7099c15a47204819b92a85fcd/tf2-0-edgetpu-keras-cnn-autoencoder.ipynbHello,
I am seeing the same problem using the code in:
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb 

as an example to get the tflite quantized models.  When using the compiler I get:

...coral/compiler$ edgetpu_compiler mnist_model_quant.tflite
Edge TPU Compiler version 2.0.258810407
INFO: Initialized TensorFlow Lite runtime.
ERROR: quantized_dimension must be in range [0, 1). Was 3.
Invalid model: mnist_model_quant.tflite
Model could not be parsed

I am wondering if anybody has found a way forward ? Thanks,
> Hello,
> I am seeing the same problem using the code in:
> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb
> 
> as an example to get the tflite quantized models. When using the compiler I get:
> 
> ...coral/compiler$ edgetpu_compiler mnist_model_quant.tflite
> Edge TPU Compiler version 2.0.258810407
> INFO: Initialized TensorFlow Lite runtime.
> ERROR: quantized_dimension must be in range [0, 1). Was 3.
> Invalid model: mnist_model_quant.tflite
> Model could not be parsed
> 
> I am wondering if anybody has found a way forward ? Thanks,

I have the same problem here! Does anyone have a clue?I got a reply from Google saying that there was a problem with the edgetpu compiler and keras based models such as the mnist conv model used in the: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb 

tutorial.

They were looking into it but did not know about timing. 

Anybody has a post training quantization notebook example that is compatible with the edge tpu compiler and is willing to share ?  Please.  I just need something simple to get started. 

The same here: 

```
Edge TPU Compiler version 2.0.258810407
INFO: Initialized TensorFlow Lite runtime.
ERROR: quantized_dimension must be in range [0, 1). Was 3.
Invalid model: /data/model.tflite
Model could not be parsed
```

TF2 Beta1 Keras model with post-training quantization:
```
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_data_gen
```

TFLite model inference in notebook works without issues. Problem with `edge_compiler` being cryptic. 

A side note: I have my simple image classification model. Nothing weird.Same here...
"ERROR: quantized_dimension must be in range [0, 1). Was 3."

Noticeably, the batch normalization layers are conveniently missing from the EdgeTPU sample model here: https://coral.withgoogle.com/docs/edgetpu/retrain-classification/

and other examples that use EdgeTPU quantization like:
https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html
also use their own form of batch normalization layers, apparently to get around this problem.
Not sure if it's just a problem when fuse=True or if it's a more general problem with batch normalization layers.Apparently (as stated in their website - they just added it-) using TensorFlow 1.15 "nightly" works. I can confirm that trying to compile a Resnet50 works with that build (with plain tensorflow I had the "quantized dimension range error").@ulorentz Grazzi mille! Despite trying a lot of workarounds such as using EfficientNet or changing batchnormalization layer's `fuse` parameter to `False`, I spent a couple weeks without getting a single model to compile on the EdgeTPU. I can confirm that only by using the unreleased TF nightly 1.15, that I was finally able to get models to consistently compile on the EdgeTPU device. Any models that include Keras BatchNormalization layers such as transfer learned models built off of MobileNetV2 or ResNet50 seem to fail automatically when compiled with any other version of TensorFlow.Hello, @ulorentz, @Alekxos 
I have the same problem with TF2.0rc1 (use MobileNet v2). As a result of visualizing the model with [visualize.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/visualize.py), Input / Output is Float, not Full intger quantization model. I think this is the reason why EdgeTPUCompiler fails.

I have found that using "**from_keras_model_file**" instead of "**from_keras_model**" works well for model conversion.
(When "from_keras_model_file" is used, Input / Output becomes UINT8 and becomes Full intger quantization model. )
```
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(path_to_model_file)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_data_gen
tflite_full_integer_quant_model = converter.convert()
tflite_full_integer_model_quant_file = models_dir/'mobilenet_v2_full_integer_quant.tflite'
tflite_full_integer_model_quant_file.write_bytes(tflite_full_integer_quant_model)
```I am able to do post training full integer quantization, with mobilenet V1. Compiled with edge TPU , worked fine.

The conversion from floating point to INT needs to be done only with tensorflow nightly 1.15, other wise this "ERROR: quantized_dimension must be in range [0, 1). Was 3." error occurs. Regards@ohtaman I just want to know how do you decide the output layer name as : dense_1/Softmax?
What is the rule of the naming? How can I check the name of last layer?
Also, can I name it with my customized name for it? thanks.@jk78346, you can open your model with netron, to check the name of the last layer.> Finally I am able to convert to tflite with post integer quantization, but now I am facing the same problem of @ankitmaurya001, that is `ERROR: quantized_dimension must be in range [0, 1). Was 3.`
> 
> You can inspect my code at https://colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7

Can you share a dummy_data so that we can run and reproduce the issue? Thanks!> > > @1n1n1t3 did you test the runtime of your generated tf-lite files?
> > > training seems to go correctly, the input and output details of my .tflite file seem consistent
> > > ```
> > > >>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')
> > > >>> interpreter.allocate_tensors()
> > > >>> input_details = interpreter.get_input_details()
> > > >>> output_details = interpreter.get_output_details()
> > > >>> print(input_details)
> > > [{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]
> > > >>> print(output_details)
> > > [{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]
> > > ```
> > > 
> > > 
> > > The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).
> > > However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.
> > > Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.
> > > This should not be the case.
> > 
> > 
> > Are you using the Coral USB Accelerator ? I tried it on the Coral Dev Board and the model I posted above is very fast. It goes something like 1500 fps. I'm used the MNIST Fashion dataset (input: 28,28,1) though. Not sure if that is fast, normal or slow for that, tbh, but the MobilenetSSD models run at about 300 fps.
> > For sure you are not supposed to get slower performance on quantized models though. Maybe the problem is something specific to the Raspberry PI and the environment.
> 
> Nope, I'm running on native Raspberry PI (it's for my master thesis, I just use the Raspberry PI as a benchmarck which is why I don't use USB accelerators). I did not precise it but I'm using a MobileNetv1 adapted to CIFAR-10's input size.
> 
> Yeah that extremely weird, at least it's not specific to the Raspberry Pi since it happens on my desktop computer as well. It might be worthwile to test the floating point 32 model in your use case as well (I'm not asking for anything, I'm already extremely grateful for your help with the BatchNorm).

Is this problem solved now?> > Finally I am able to convert to tflite with post integer quantization, but now I am facing the same problem of @ankitmaurya001, that is `ERROR: quantized_dimension must be in range [0, 1). Was 3.`
> > You can inspect my code at [colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7](https://colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7)
> 
> Can you share a dummy_data so that we can run and reproduce the issue? Thanks!

I am experiencing the same problem. My code is here: https://pastebin.com/LfmPANwV. This is my error: https://pastebin.com/pW1rchLj. Here is all the data you should need to reproduce and debug locally: [train_and_convert_for_coral.zip](https://github.com/tensorflow/tensorflow/files/3913277/train_and_convert_for_coral.zip). You should also know that I run the notebook (in the zip I attached) in a Docker container here: https://hub.docker.com/r/wpilib/axon-playground (just run it and it will start the notebook server).@tensorflowbutler Yes, I guess it still isThis is still an issue, no way to do quantization-aware training, even in TF2.1.

Was expected Q2 2019, now it's Q1 2020.Hi there @gilescoope. I'm one of the individuals developing the Keras quantization-aware training API.

Stay tuned on https://www.tensorflow.org/model_optimization and its roadmap. We're currently in the midst of creating user-facing documentation and tutorials, together with addressing some outstanding usability issues (e.g. removing the need for quantized_input_stats, as previously needed) and doing some final testing. With our successful training experiments, we are also in a good shape. There is a decent chance that initial usage will have to depend on tf-nightly, after which we'll move onto a stable TF release - we'll see what happens there.

I'll update this issue once it has been launched. I apologize for prior miscommunication on timelines. Following the Keras quantization API launch, we'll be making the progress (or non-progress) on major features a bit more transparent.An update since my last comment:

We did a launch review and one of the suggestions (to the benefit of most users) is to get people to dogfood the tool. This is currently in the process and we have a number of teams actively testing it. As developers, we will also be training on additional models ourselves (while expanding model coverage) to improve upon things. 

Given this and the work I previously mentioned, things will take longer (but be more polished at launch). Expect it take around a month (end of March) (and hopefully not longer) for the initial release with docs, a blog post, and the pip package. For expectations on what would be available at launch, see what we have for [pruning](https://www.tensorflow.org/model_optimization/guide/pruning) on the [API Compatibility Matrix](https://www.tensorflow.org/model_optimization/guide/pruning#api_compatibility_matrix) as a reference point. Notably in contrast to pruning, we would only be supporting TF 2.X to start at launch and tf.distribute integration may not quite be there yet. For fundamental reasons on the way quantization works, subclassed models will have even less support.

Post launch, we'd work on tf.distribute and increased model coverage, prioritizing essentially based on what users request. If we don't get enough user feedback, we'd prioritize based on our intuition and past experience.Also new quantization infra will come in MLIR https://llvm.discourse.group/t/rfc-a-proposal-for-implementing-quantization-transformations-in-mlir/655It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?> It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?

yes we are awaiting updates slated for end March. Thanks to everyone working on this!Update from my last comment:

- Tutorials and examples are in-review but almost complete with the general structure approved.
- Initial LGTM to move through launch process based on good experiments results and feedback.
- Making some final minor tweaks to improve usability (e.g. error messages, pydocs, etc.) and clarifying what's available at launch

End of March is reasonable and if not ~ a week later given some processes. 

I found this page indicating how to use tfmot.quantization to do quantization-aware training: https://www.tensorflow.org/model_optimization/guide/quantization/api_usage
But it seems this api is not released yet?Update given that it's past the end of March. We've just gotten all approvals and are just finishing up the final steps. 

A few documents need to be submitted and a couple more PRs. Then we'll do the release at some scheduled date. 

@kazenokizi : the examples in that document don't fully work and the API is not released yet. We were incrementally working on the documents. Hi everyone. The Keras quantization aware training API has now launched!

Edit: please reference the below. I will no longer respond to comments on this thread. 

Documentation on what versions of TensorFlow and models are supported, as well as general API 
compatibility and support matrices: https://www.tensorflow.org/model_optimization/guide/quantization/training. This page also links to other end-to-end tutorials.

File feature requests and bugs: after reading the above, you can go to https://github.com/tensorflow/model-optimization for filing anything. In general,
if it's an error you run into from using "import tensorflow_model_optimization", file it there.
If it's an error you run into from using "import tensorflow as tf" after the correct versions of TensorFlow, file it in the main https://github.com/tensorflow/tensorflow.

Blog post: https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html

Twitter: https://twitter.com/TensorFlow/status/1247933575183233024

Edit:added content from below up here for ease 
Thanks for the update @alanchiao ! Which version of TF are the changes in the video in?  I didn't see the changes mentioned in these release notes: https://github.com/tensorflow/tensorflow/releasesFor general documentation, refer to the TFMOT documentation and Github since the tool is installed via the TFMOT pip package, not the TensorFlow pip package.

See the [overview page](https://www.tensorflow.org/model_optimization/guide/quantization/training#api_compatibility) for what versions of TF can be used. This information is also mentioned in the [release notes for TFMOT](https://github.com/tensorflow/model-optimization/releases).

Generally, it's best to reference the documentation and code examples on tensorflow.org/model_optimization, since that will stay updated, as opposed to the blog post and videos.



In general for documentation, if you think certain aspects could be more visible/easier to find, we welcome feedback. You'll see from the documentation that there are different types of users, and we tried to make the documentation accommodate all of them.Hi @alanchiao,
i built an NN with BatchNormalization layer  and i have tried to quantize the whole model for EdgeTPU application. But it doesn't work. I'm using tfmot 0.3.0 and tensorflow 2.1.

This is the error

```
RuntimeError: Layer batch_normalization:<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'> is not supported. You can quantize 
this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.
```
and the code

```
model = create_model()
quant_aware_model = tfmot.quantization.keras.quantize_model(model)
quant_aware_model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

quantized_tflite_model = converter.convert()
```






@lravano and others.

Please reference the [overview page](https://www.tensorflow.org/model_optimization/guide/quantization/training#api_compatibility) to see what is supported currently.

Then if something is supported but you're facing an issue, file a bug request. If it's not supported, file a feature request, ideally with the motivation (e.g. what model you are trying to apply quantization to, which may be relevant to others in the community also). You can do that at https://github.com/tensorflow/model-optimization.

For the case of BatchNormalization, you can see in the overview page that currently BatchNormalization is currently just supported only when it's after a convolutional or Dense layer. You should then file a feature request. Hello! 
For example, I made this command:

model = create_model()
quant_aware_model =tfmot.quantization.keras.quantize_model(model)
quant_aware_model.summary()

But how will I able to convert this model with uint8 format?
This look as float format:
converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_tflite_model = converter.convert()

These commands do not work:
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)@alex283h
```
converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_tflite_model = converter.convert()
```
When I tried this example code with `tf2.1`, it returns all weights with float32. 
However, when I tried this with `tf-nightly`, all the weights are converted to int8 except input and output layers.

I still wonder whether there is an option to convert the input/output layers to `uint8`.
Also for me, the below code doesn't work.

```
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)
```

Please now look at https://github.com/tensorflow/tensorflow/issues/27880#issuecomment-611079359. I'll now stop commenting on this thread and only comment on other support channels.

@alex283h, @hyungui: Thanks Hyungui for helping Alex. Officially, TF 2.1 is not supported, only tf-nightly. See our docs [here](https://www.tensorflow.org/model_optimization/guide/quantization/training#api_compatibility). When it works on a stable TF release, we'll update this page.

@hyungui : regarding your question, see [here](https://www.tensorflow.org/model_optimization/guide/quantization/training#general_support_matrix), which points to a Github issue. It's a known issue that what you're trying doesn't work. **@hyungui**, thank you for explain! I get the same results:
>>tf2.1 returns all weights with float32;
>>tf-nightly, all the weights are converted to int8 except input and output layers. 

In fact it would be greate to have some way to get uint8 for the next converting in edge_tpu model.

**@alanchiao** Thank you. I will study your links, maybe I will found the solution about uint8 there. If somebody know how to do it after learning model with quantization.keras.quantize_mode I will ve much appresiate.is tf.quantization.fake_quant_with_min_max_vars still useful now that quantization aware training is out?

Is quantization aware training solely for getting TFLite models? and we would still have to fake_quant_with_min_max_vars to get the best INT8 TensorRT models? Or is there something I am missing out?> Hi @alanchiao,
> i built an NN with BatchNormalization layer and i have tried to quantize the whole model for EdgeTPU application. But it doesn't work. I'm using tfmot 0.3.0 and tensorflow 2.1.
> 
> This is the error
> 
> ```
> RuntimeError: Layer batch_normalization:<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'> is not supported. You can quantize 
> this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.
> ```
> 
> and the code
> 
> ```
> model = create_model()
> quant_aware_model = tfmot.quantization.keras.quantize_model(model)
> quant_aware_model.summary()
> 
> converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)
> converter.optimizations = [tf.lite.Optimize.DEFAULT]
> 
> quantized_tflite_model = converter.convert()
> ```
Did you find a solution to this problem?
I am also stuck at exactly the same place. There a large number of image segmentation models that use batch_normalization and running them on the edge TPU would be awesome.