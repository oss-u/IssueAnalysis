This discussion bring some insights about adding a multivariate imputer in scikit-learn. Because of release time constraint, the development was moved into a specific branch (FIXME: give a specific name) see #11600.

- [ ] Decide good default for the IterativeImputer/ChaindedImputer (#11350).
- [ ] Modify the example for imputation to show a compelling example (#11350).
- [ ] Add an example to illustrate how to make multiple imputation (#11370).
- [ ] Add a meta-estimator which does multiple imputations?

<details>

From the discussion in #8478, we have to deal with the following issues in `MICEImputer`:

We have the following things to do:

* Determine the most appropriate way to use individual imputation samples in predictive modelling, clustering, etc, which are Scikit-learn's focus.
      a. is using a single draw acceptable?
      b. is averaging over multiple draws from the final fit appropriate?
      c. is ensembling multiple predictive estimators each trained on a different imputation most appropriate?
* Perhaps determine if, in a predictive modelling context, it is necessary to have the sophistication of MICE in sampling each imputation value rather than just using point predictions.
* Provide an example illustrating the inferential capabilities due to multiple imputation. I don't think there's anything limiting about our current interface, but it deserves an example.
* Rename MICEImputer to de-emphasise multiple imputation because it only performs a single one at a time.

Minor things:

* The documentation refer to `Imputer` instead of `SimpleImputer`.
* `imputation_sequences_` should be improved (length of the list mainly).
@glemaitre can you clarify what you mean by your point about `imputation_sequences_`?Currently the documentation is not explicit regarding the size of the list. I find that it should be `n_burn_in * n_features_with_missing + n_imputations * n_features_with_missing`.Ah, thanks!One comment about the to-do list. I don't think this is necessary to do:

"Perhaps determine if, in a predictive modelling context, it is necessary to have the sophistication of MICE in sampling each imputation value rather than just using point predictions."

I think stats people already know that they want to understand uncertainty due to missing values. Why do we need to if it is "necessary"? The answer will inevitably be "sometimes" and depend on what dataset is chosen to run some basic experiments on.

I'll try to carve some time out next week to tackle at least some of these, but it would be great to have someone else contribute as it's a busy work season for me.I am willing to help, but I don't totally get what's been decided on the n_imputations and m?   We keep as it is for n_imputations. For m, it will be an example to show how to make inference.                                                                                                                                                                         Sent from my phone - sorry to be brief and potential misspell.My understanding is that concrete action about `n_imputations` and `m` are not being taken now, but instead we can demonstrate how to perform MICE as the stats world intended with something like this:

```
n_burn_in = 50
n_imputations = 1
m = 10
multiple_imputations = []
for i in range(m):
    imputer = MICEImputer(n_burn_in=n_burn_in, n_imputations=n_imputations, random_state=i)
    X_imputed = imputer.fit_transform(X_missing)
    multiple_imputations.append(X_imputed)

# insert some other downstream tasks that are done to each element so as to demonstrate the variability of the missing values
```
This can be accompanied by comments that discuss why the various values are set as they are.

In addition, we will also need a ML example that looks like this:
```
n_burn_in = 50
n_imputations = 50
# m = 1
# etc etc
```

I think we may have something like this already actually here: https://github.com/scikit-learn/scikit-learn/blob/master/examples/plot_missing_values.py

@glemaitre @jnothman I ran a quick experiment with Boston to demonstrate that averaging a longer set of the last `n_imputations` is helpful for downstream ML tasks.

See this gist for the code: https://gist.github.com/sergeyf/08e5af7674b4d2c6d36dcb7872745c40

The result:

![image](https://user-images.githubusercontent.com/1874668/41428275-89919e7e-6fd7-11e8-9a22-6e73b7b24e24.png)

Here Boston is missing data in 75% of the rows. I kept `n_burn_in + n_imputations` fixed to `100` and swept over `n_imputations`. At the very left of the plot is the average held-out MSE of a `RandomForestRegressor` after the data is imputed with MICE with `n_burn_in = 99` and `n_imputations = 1`. It's the highest MSE and the most variable.

As we go to the right, the mean/MSE goes down until about 50 and then flattens out. 

This entire experiment was re-run 100 times to get the bars.


I think this provides at least a partial answer to:
```
Determine the most appropriate way to use individual imputation samples in predictive modelling, clustering, etc, which are Scikit-learn's focus.
a. is using a single draw acceptable?  NO
b. is averaging over multiple draws from the final fit appropriate?  YES
```Ah great! I can spend some time making a draft for the first example.

2018-06-14 19:39 GMT+02:00 Sergey Feldman <notifications@github.com>:

> @glemaitre <https://github.com/glemaitre> @jnothman
> <https://github.com/jnothman> I ran a quick experiment with Boston to
> demonstrate that averaging the a longer and longer set of the last
> n_imputations is helpful for downstream tasks.
>
> See this gist for the code: https://gist.github.com/sergeyf/
> 08e5af7674b4d2c6d36dcb7872745c40
>
> The result:
>
> [image: image]
> <https://user-images.githubusercontent.com/1874668/41428275-89919e7e-6fd7-11e8-9a22-6e73b7b24e24.png>
>
> What's happening here is I kept n_burn_in + n_imputations fixed to 100
> and swept over n_imputations. At the very left of the plot is the average
> held-out MSE of a RandomForestRegressor on the Boston dataset after being
> filled in with MICE with n_burn_in = 99 and n_imputations = 1. It's the
> highest MSE and the most variable.
>
> This entire experiment was re-run 100 times to get the bars.
>
> As we go to the right, the mean/MSE goes down until about 50 and then
> flattens out.
>
> I think this provides at least a partial answer to:
>
> Determine the most appropriate way to use individual imputation samples in predictive modelling, clustering, etc, which are Scikit-learn's focus.
> a. is using a single draw acceptable?  NO
> b. is averaging over multiple draws from the final fit appropriate?  YES
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/scikit-learn/scikit-learn/issues/11259#issuecomment-397378308>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AVQqe4orruOt8qcCWqI31LM9gdKYjl8qks5t8p_mgaJpZM4Un-On>
> .
>
I am also thinking that we should remove ``initial_strategy`` to ``initial_imputer`` since that it will be a pain to update the docstring, each time that the ``SimpleImputer`` get updated. Also it allows to accept several imputer and the future ``RandomImputer`` for instance.

@jnothman WDYT?@glemaitre I'll try to get this one done today: "Rename MICEImputer to de-emphasise multiple imputation because it only performs a single one at a time."

I'm going to use `ChainedImputer`, unless anyone has any objections.Here is the renaming PR: #11314@sergeyf possibly naive question of somebody who has no statistics background: When the goal is to have a single best imputation (and not multiple imputations to do inference on the results), is it then really needed to do the many iterations and take the average? 
In your experiment above you showed that with the current code it ensures a more stable result (which is indeed good if the goal is the single best imputation, and you are not interested in the variation on this). But, the current implementation also introduces random noise (based on the sigma of the model) in each iteration:

https://github.com/scikit-learn/scikit-learn/blob/007aa710bd45cd0df8c56ad581a4e59886128771/sklearn/impute.py#L624-L632

So naively I would think doing this many times and averaging in the end will lead to almost the same results as just doing it once and not adding this random noise (after the initial burn in, so relying on the mean of the model) , but the latter would be much more efficient (1 instead of 100 imputations). 

@jorisvandenbossche I'm not a MICE expert, really. I just happened to meet Stef in real life, tried MICE out on some problems I had, and noticed it worked very well in ML contexts. So I wanted to make it available to ML people. That is to say: I'm not the best person to answer. @stefvanbuuren would know better, however.

But, my intuition is that it would not be the same. It's basically a MCMC sampling process:
![image](https://user-images.githubusercontent.com/1874668/41725799-8bbb5eee-7525-11e8-8a4d-dc1c3f9bbfa6.png)
I think of the individual imputed values as jumping around as in the upper left image. If we freeze the process at any point, it may not be near the mean of that plot (around 1.0). By taking the average across the last `n_imputations`, we are hedging against this and getting closer to the mean. I don't think simply not sampling at the very end would get you the mean because refitting at each iteration is also part of the sampling process.

I could be wrong though. Maybe you could make a fork, quickly modify MICE and check your hypothesis, using the gist I posted as a base?> I could be wrong though. Maybe you could make a fork, quickly modify MICE and check your hypothesis, using the gist I posted as a base?

Yes, I was planning, and took now the time to do it, see figure below. The figure is the same as the one above (using the same seeds as you), but the added red line gives the experiment of using the imputed values of the last iteration + using the model mean (without adding noise) in each iteration. I did this for a `n_burn_in`of [10, 20, 50] (and with `n_imputations=1`). So you don't really need to look at the x values, as the number of iterations do not match  with the green line (the green line always has a total of 100 iterations divided between n_burn_in and n_imputations). 
I also added error bars for the results with the original data without missing values (by running this also 100 times like the other experiments).

![image](https://user-images.githubusercontent.com/1020496/41746044-484d0296-75a9-11e8-814f-d8b2d1ea61e9.png)

The mean value is slightly lower (whether this is good or bad I don't know), the variation is clearly lower, and already stable after 10 burn in iterations (for this example).

> I think of the individual imputed values as jumping around as in the upper left image. If we freeze the process at any point, it may not be near the mean of that plot (around 1.0).

Yes, but, when we don't add noise to the iterations but only use the mean of the model it will not be jumping around but converge to a stable (single best) imputation. So that is the reason that doing it like this gives a lower variance (and with much less iterations, so more efficient). 
Of course, directly taking the output (prediction) of the imputation model without adding noise in each iteration will affect the next model in the chain, but whether this is a problem I don't know.



<details>
<summary>The quick and dirty patch to MICEImputer (click to expand)</summary>

```patch
--- a/sklearn/impute.py
+++ b/sklearn/impute.py
@@ -625,11 +625,12 @@ class MICEImputer(BaseEstimator, TransformerMixin):
         X_test = safe_indexing(X_filled[:, neighbor_feat_idx],
                                missing_row_mask)
         mus, sigmas = predictor.predict(X_test, return_std=True)
-        good_sigmas = sigmas > 0
-        imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
-        imputed_values[~good_sigmas] = mus[~good_sigmas]
-        imputed_values[good_sigmas] = self.random_state_.normal(
-            loc=mus[good_sigmas], scale=sigmas[good_sigmas])
+        imputed_values = mus
 
         # clip the values
         imputed_values = np.clip(imputed_values,
@@ -883,7 +884,8 @@ class MICEImputer(BaseEstimator, TransformerMixin):
                       '%d/%d, elapsed time %0.2f'
                       % (i_rnd + 1, n_rounds, time() - start_t))
 
-        Xt /= self.n_imputations
+        Xt = X_filled # last filled values
         Xt[~mask_missing_values] = X[~mask_missing_values]
         return Xt
```
</details>


Thanks for the experiment. I think I had the wrong interpretation of what you were suggesting. It looks like not sampling during the chained process helps the downstream model quite a bit (and reduces variance), which is a cool finding! 

Have you by chance looked at whether the MICE tests that have to do with empirical correctness still pass? They're in `test_impute.py`: `test_chained_imputer_additive_matrix`, `test_chained_imputer_transform_recovery`.

Without any kind of sampling, this is somewhat far from the original MICE algorithm. The empirical evidence you've provided is positive, but it's limited to this example and has not been thoroughly explored like MICE has in the literature. This makes me a bit hesitant about adding it to sklearn.

But maybe we can put the sampling behind a flag and set it to True by default? This would at least allow an end-user to try to version you're suggesting. We would probably also need an example of how to use it with the flag set to False and why one might want to.

Anyone else have thoughts here?This is what I meant by "Perhaps determine if, in a predictive modelling
context, it is necessary to have the sophistication of MICE in sampling
each imputation value rather than just using point predictions."

I think there is literature on chaining without sampling from a
distribution around the candidate imputation, but I've not explored the
references in the MICE paper.

Before seeing this conversation, I was thinking it would be nice to support
regressors (or indeed classifiers) that do not give a predictive
distribution. Yes, I think we could control this with a parameter
('sample'?). The question is: which behaviour should we offer by default?
Also I believe that this is the right way to go if we're to make it a more
generic "ChainingImputer". But we can still support, and illustrate, the
inferences possible with Multiple Imputation and sampling.

On 22 June 2018 at 14:13, Joel Nothman <joel.nothman@gmail.com> wrote:

> This is what I meant by "Perhaps determine if, in a predictive modelling
> context, it is necessary to have the sophistication of MICE in sampling
> each imputation value rather than just using point predictions."
>
> I think there is literature on chaining without sampling from a
> distribution around the candidate imputation, but I've not explored the
> references in the MICE paper.
>
> Before seeing this conversation, I was thinking it would be nice to
> support regressors (or indeed classifiers) that do not give a predictive
> distribution. Yes, I think we could control this with a parameter
> ('sample'?). The question is: which behaviour should we offer by default?
>
In my opinion, the default one should be the one that is empirically shown to work best with a variety of examples based on already available ones. > Have you by chance looked at whether the MICE tests that have to do with empirical correctness still pass? They're in test_impute.py: test_chained_imputer_additive_matrix, test_chained_imputer_transform_recovery.

Those two still pass (only `test_chained_imputer_transform_stochasticity` is failing, for good reason as `transform` no longer adds the stochastic noise)Great, thank you for checking. Definitely would be good to have a flag to enable this. It would also mean we can easily toss in other regressors, It would be great to have a reference for the non-sampled version of MICE if anyone happens to know a good one.Hi, I'm very new here but after talking with @jorisvandenbossche, I would like to support him on the fact that generating a variable's missing data in the conditional distribution is perhaps not necessary with an aim of prediction - we can just take the regression instead. I'm not sure about such an implementation of MICE, but missForest does just that with random forests. 
@julierennes would know more than me on that.The example to show how ChainedImputer can be used as a MICE Imputer would include something like this: 

```
def calculate_data_variance(X):

    means = np.mean(X, axis = 0)
    X_dif = X - means
    X_dif_squared = X_dif ** 2
    SSxx = np.sum(X_dif_squared, axis = 0)

    return(SSxx)

def calculate_variance_of_beta_estimates(y_true, y_pred, SSxx):

    SSe = (np.sum((y_true - y_pred)**2) / (len(y_true) - 2))
    vars = SSe / SSxx

    return vars

 # Impute incomplete data using the ChainedImputer as a MICEImputer
 m = 5
 multiple_imputations = []

 for i in range(m):

     imputer = ChainedImputer(n_burn_in=100, n_imputations=1)
     imputer.fit(X_incomplete)
     X_imputed = imputer.transform(X_incomplete)
     multiple_imputations.append(X_imputed)
     del imputer

 # Perform a model on each of the m imputed datasets
 # Estimate the estimates for each model/dataset
 m_coefs = []
 m_vars = []
 for i in range(m):

     estimator = LinearRegression()
     estimator.fit(multiple_imputations[i], y)
     y_predict = estimator.predict(multiple_imputations[i])
     SSxx = calculate_data_variance(multiple_imputations[i])

     m_coefs.append(estimator.coef_)
     m_vars.append(calculate_variance_of_beta_estimates(y, y_predict, SSxx))

     del estimator

 # Calculate the end estimates by applying Rubin's rules
 # Rubin's rules can be slightly different for different types of estimates
 # In case of linear regression, these are the rules:
 # The value of every estimate is the mean of estimates in each of the m datasets
 Qbar = np.mean(m_coefs, axis = 0)
 # The variance of these estimates is a combination of the variance of each of the m estimates (Ubar)
 # And the variance between the m estimates (B)
 Ubar = np.mean(m_vars, axis = 0)
 B = (1 / (m-1)) * np.mean((Qbar - m_coefs) ** 2, axis = 0)
 T = Ubar + B + (B/m)

# The beta estimates are stored in Qbar, the variance of these estimates are stored in T
```

However, I have done some simulations testing the procedure and comparing the ChainedImputer with m = 1 and m = 5 and with n = 1 and n= 100 and the results are not as I would expect. I will post the script in a github repo later, but need some more time to think about these results.

![simulationschainedimputer](https://user-images.githubusercontent.com/22293115/41796921-6e0a8868-7668-11e8-90f6-1c5481b49ebb.png)
@RianneSchouten I haven't had time to think about this yet, but you probably want to set a different seed in this line for each of the imputations in this line:

`ChainedImputer(n_burn_in=100, n_imputations=1)`

It should instead be:

 `ChainedImputer(n_burn_in=100, n_imputations=1, random_state=i)`@jorisvandenbossche @nprost 

Indeed, if your aim is to impute and predict as well as possible the missing entries then using single imputation is enough and using multiple imputation for that is not required and you could impute by taking the conditional expectation and not by drawing from the conditional distribution. As far as prediction is concerned, there are not yet many results on this problem. Common practice and few papers tend more to suggest the following approach: Perform multiple imputation and on each imputed data set, apply your predictive algorithm to estimate the response say Y. Then aggregate the different predictions. 

Best,
JJ@sergeyf 
If the default random_state is None, and that means it picks a random number, than the imputer will be different for every `i in range(m)`, don't you think? Because I delete the imputer after round i is finished with `del imputer`. Maybe changing it to `random_state = i` means you don't have to delete the imputer? You shouldn't need to delete the imputer in any case. fitting again will
reset things. Using random_state=None is fine for this. random_state=i for
i=0,1,2,... will only make sure the result is reproducible.

@julierennes thank you for confirming the suspicion that "you could impute
by taking the conditional expectation and not by drawing from the
conditional distribution". When you suggest that we nonetheless predict
with an ensemble of predictive models fitted to different imputations, is
this common practice with something like missForest? In that case do
multiple draws come from different randomisation of the forest
construction, rather than samples from the gaussian posterior of
BayesianRidge used here in MICE?
​
I also suspect that the current n_imputations is not a feature we should be
providing...​
You're probably right @jnothman.  I can make a PR that:

(a) removes `n_imputations`
(b) add a `sampling` flag that requires the imputation model to have `return_std`. On by default?

With those two we should be able to replicate `missForest` trivially as well as still do `MICE` as designed.

Let me know how that sounds and I can get started next week.Yes, thanks! I'm tempted to say sampling=False by default given the results
cited above. But not sure.​
I should probably rename `n_burn_in` to `n_imputations`, eh?n_iter would be wonderful, actually.PR is here: #11350. It's nearly done, just waiting for some input.> n_iter would be wonderful, actually.

Or, a `max_iter` might also be an option? (although it might be more work to change the current code) 
And then have a convergence criterion. This is similar to what missForest does (https://arxiv.org/pdf/1105.0828.pdf).I suppose it should be possible to test convergence...​ We can call it
max_iter, but not have early stopping for now.
The work in this thread is heading towards a method that does single imputation. This is very different from the philosophy and theory that underlies the MICE/chained equations approach. I would like to ask the developers to document and advertise their new method as doing *single imputation*, so as to avoid any confusion with multiple imputation methodology.@stefvanbuuren, the work in this thread is heading towards a tool that is capable of either single or multiple imputation, i.e. to engineer a solution that provides a common basis for multivariate imputation with chaining. We hope to include an example of multiple imputation and its application. However, in the context of a Scikit-learn pipeline, and the needs of our users, single imputation is both most compatible, and appears to be at least as useful for predictive performance.

At the moment we do not mention "single imputation" explicitly in the documentation (and for many of our users this is not an especially meaningful term except in contrast to a definition of multiple imputation; otherwise it is mere "imputation").

The only place either term is used is [here](http://scikit-learn.org/dev/modules/impute.html#multiple-vs-single-imputation). To quote:

> In the statistics community, it is common practice to perform multiple imputations, generating, for example, 10 separate imputations for a single feature matrix. Each of these 10 imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The 10 final analysis results (e.g. held-out validation error) allow the data scientist to obtain understanding of the uncertainty inherent in the missing values. The above practice is called multiple imputation. As implemented, the ChainedImputer class generates a single (averaged) imputation for each missing value because this is the most common use case for machine learning applications. However, it can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds with the n_imputations parameter set to 1.

> Note that a call to the transform method of ChainedImputer is not allowed to change the number of samples. Therefore multiple imputations cannot be achieved by a single call to transform.

Does this seem acceptable?If you make single imputation the default, then this creates an association between the name MICE and single imputation. Users will say that they are doing MICE imputation, while in fact they are not properly accounting for the uncertainty of the synthetic values. So either break the association with MICE (and  advertise the method as single), or set the default to multiple (and users to need to explicitly set m = 1 when they want to do single). it is no longer called MICEImputer for this reason
In the section just above the one you quoted, we read:
    
    A more sophisticated approach is to use the ChainedImputer class, which implements 
    the imputation technique from MICE (Multivariate Imputation by Chained Equations). MICE 
    models each feature with missing values as a function of other features...

It's fairly easy for the user to mix up "multivariate" and "multiple", and think they're doing MICE, which they do not under the proposed default. agreed that it's easy to confuse. let's reword it to:

* Focus on describing the algorithm

* explain the parameters and application setting in which it is equivalent
to MICE, referring to the example we have not yet coded up

* perhaps also note that the implementation was inspired by your R package

* reference other literature on chained multivariate imputation. could you
please recommend seminal work in this space?

thanks
OK, thanks. Here’s what I would do:

- Choose your default algorithm, and describe that as clearly as possible;
- Buck (1960) was to first to suggest mutual regressions to find replacement values in multivariate missing data. MICE uses an iterated version of Buck’s method, and extended it to multiple imputation;
- Refer to Rubin (1987, Ch 1) or Little and Rubin (2002, Ch. 4) for the limitations of single imputation;
- If your default is single imputation, indicate that it is fairly easy to make it multiple: take m draws from the posterior for the missing value (instead of finding one “best” value), analyze in parallel, and pool the results;
- Create an example of the full multiple imputation cycle, including the pooling, for a classification/prediction problem;
- Indicate that it is still an open problem how useful single versus multiple imputation is in context of prediction/classification. I haven’t seen any research that properly evaluates out-of-sample predictions from incomplete data;
- Say that your method does single imputation if that's the default.
Thanks @stefvanbuuren and @jnothman. I'll integrate the fruits of this discussion into the most recent open PR.Some additional questions, given the discussed re-purposing of the imputer:

- Is `ChainedImputer` a good name?  
  We still do the "chained equations" part of MICE, but if I quickly google for it (no thorough search though), I only find that term in context of MICE. For example, the missForest package/paper (which also does this iterative version, and is actually much closer to the future implementation in scikit-learn) does not speak at all about chained equations. One description of itself in the paper is "iterative imputation method". Raghunathan et al. (2001) ([pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.405.4540&rep=rep1&type=pdf)) describes a similar idea as "multivariate imputation using a sequence of regression models" / "sequential regression multivariate imputation". 
  It's mainly that the name "ChainedImputer" would not directly give many people a clue what it is doing I think. But "MultivariateImputer" is probably also too generic?

- Should `BayesianRidge` be the default estimator?  
  If the default settings are optimized for the single prediction setting, it may make sense to choose a different default estimator?I am OK changing name to `SequentialImputer` or `IterativeImputer`

In terms of the default: `BayesianRidge` is best in my opinion:

- it's really fast
- it supports `return_std` if we want to do MICE 
- it has no params to tune.

I tried to swap in `RandomForestRegressor` into the predictor in the current tests and both `test_chained_imputer_additive_matrix` and `test_chained_imputer_transform_recovery` failed. > In terms of the default: BayesianRidge is best in my opinion:

I don't really agree:

> it's really fast

```python
In [1]: from sklearn.datasets import load_boston
In [2]: X, y = load_boston(return_X_y=True)
In [3]: from sklearn.linear_model import BayesianRidge, RidgeCV, Ridge
In [4]: reg = BayesianRidge()
In [5]: %timeit reg.fit(X, y)
1.01 ms ± 1.44 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
In [6]: reg = RidgeCV()
In [7]: %timeit reg.fit(X, y)
857 µs ± 16.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
In [9]: reg = Ridge()
In [10]: %timeit reg.fit(X, y)
391 µs ± 1.82 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
> it supports return_std if we want to do MICE

True but this is not the primary use case, isn't it. However, we should support it and offer an option to return the std when needing it.

> it has no params to tune.

There is actually 4 parameters (alpha_1, alpha_2, lambda_1, lambda_2)
That's why I would go for a `RidgeCV` which has a single parameter, optimized using a CV, under consideration that a linear model is enough as default.@sergeyf `IterativeImputer` sounds good and neutral. `SequentialImputer` might be too close to IVEware.@glemaitre Thanks for the timings. I was thinking "fast" compared to RF, which I was fooling around with just a few minutes prior to writing that. It's true that RidgeCV is faster, but I still feel that supporting more use-cases with less parameter fiddling out-of-the-box is more important here. I'm open to being convinced however.

@stefvanbuuren Thanks. If @glemaitre and @jnothman agree, I'll change the name to `IterativeImputer`.The naming is fine with me :)

On 25 June 2018 at 17:59, Sergey Feldman <notifications@github.com> wrote:

> @glemaitre <https://github.com/glemaitre> Thanks for the timings. I was
> thinking "fast" compared to RF, which I was fooling around with just a few
> minutes prior to writing that. It's true that RidgeCV is faster, but I
> still feel that supporting more use-cases with less parameter fiddling
> out-of-the-box is more important here. I'm open to being convinced however.
>
> @stefvanbuuren <https://github.com/stefvanbuuren> Thanks. If @glemaitre
> <https://github.com/glemaitre> and @jnothman <https://github.com/jnothman>
> agree, I'll change the name to IterativeImputer.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/scikit-learn/scikit-learn/issues/11259#issuecomment-400003869>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AHG9P8f--7FeJ_sdIgR7Rg2SH3Lj67ryks5uAQjogaJpZM4Un-On>
> .
>



-- 
Guillaume Lemaitre
INRIA Saclay - Parietal team
Center for Data Science Paris-Saclay
https://glemaitre.github.io/
Iterative imputer does sound reasonably clear, and is probably clearer. In
comparison to ClassifierChain, this is much less chain like, since it uses
round robin rather than adding a "link" in each step.

I would be happy with the default regression being dependent on whether or
not we are in sampling mode

Which could be fill='mean'/'sample'... Or mode='single'/'multiple'? (My
main problem with that is that it might be possible to do multiple
imputation by randomising the regressor rather than using return_std.
I like the idea of the default being dependent on the mode.

I think keeping the flag more function-oriented (`posterior_sample`) rather than usage-oriented (`fill` or `mode`) is probably better. I'm fine with posterior_sample=True/False.​
OK, I made the changes as discussed. And I last-minute went with `sample_posterior` =)I didn't actually look at Guillaume's benchmarks until now. I'd be
comfortable using BayesianRidge as the default in both cases...​ but I
might be under-informed on that.

There is one thing that is not mentioned in this discussion yet and it is
important:

- the methodology of mice implies using the whole dataset including the
output variable in the imputation iterations. In other words: x1 is imputed
with x2, x3, ... and y. Then x2 is imputed with x1, x3, ... and y,
etcetera! This is because the imputation model should include at least the
analysis model.

@stefvanbuuren am I saying this correctly?

As said earlier, it is yet unknown what the difference is between the
outcome of a prediction model (when the output variable will not be used)
and a model built for statistical inference.

Would it be possible to include the possibility to add y in the imputation
process?
Something like ` include_y = False ` by default and ` X = np.column_stack((X,
y))` when `include_y = True`?

I will show the difference between the two in the example then. This won't work because during validation/test time we don't have `y`. Your option would only work in transductive settings.Good point Rianne. Potentially you could add `y` as a predictor and impute. Whether that's OK to do depends on how you want to impute. If you want to impute a single best value, then DO NOT include `y` (as now implemented). If you draw from the posterior you must include `y`. See Little 1992 for more details.so... should we refuse to draw from the posterior and refuse to do multiple
imputation? surely in a clustering context needing to have y as a predictor
is meaningless
Sure, in a clustering context there is no observed `y`, so then there is no issue whether you should include it or not. But for prediction, there is an observed `y`, and we could go down two routes. It's then up to the software designer to decide which routes to support.So maybe we can find a way to make this an option, or to illustrate it in
an example... eventually. I don't see it as a priority here.
My interest is mostly in making the API as stable, useful, well-informed
and well-documented as possible.​
Before a release in coming weeks, I should add.​
I just used this and noticed that a) the model is called ``predictor`` not ``estimator`` (or ``base_estimator``) as is usual for a meta-estimator, and b) that it's not the first argument.
Ideally I'd prohibit positional arguments but since we don't for now, I'd rather have   the base estimator be the first argument.
If you do `` IterativeImputer(RandomForestRegressor(n_estimators=100))`` you get a hard-to-debug error about nans in training data.Thanks for the feedback. I'm happy to rename to `estimator`.

Regarding position: this is tricky because `SimpleImputer` doesn't have a
meta estimator so it will have a different first parameter from
`IterariveImputer` if we make the ordering change. Is that acceptable?
That's the reason we haven't made `estimator` first already.

On Mon, Feb 11, 2019, 8:35 AM Andreas Mueller <notifications@github.com
wrote:

> I just used this and noticed that a) the model is called predictor not
> estimator (or base_estimator) as is usual for a meta-estimator, and b)
> that it's not the first argument.
> Ideally I'd prohibit positional arguments but since we don't for now, I'd
> rather have the base estimator be the first argument.
> If you do IterativeImputer(RandomForestRegressor(n_estimators=100)) you
> get a hard-to-debug error about nans in training data.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/scikit-learn/scikit-learn/issues/11259#issuecomment-462395988>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABya7H-gzQJhhIeBTOpGiqeX61bUI2foks5vMZvFgaJpZM4Un-On>
> .
>
I'm okay with renaming predictor to estimator. I think the point of
"predictor" was that there might be other underlying estimators, e.g. an
initial imputer. Thanks for the feedback, @amueller.

Most of the discussion is happening at #11977 rather than here.
Should we consider making `predictor` the first input parameter?It is the most important parameter... but I don't mind the current
consistency with SimpleImputer
I'll defer to sklearn full-timers. Let me know if you reach a consensus to move it. I'll change the name to `estimator` in the most recent PR.