#6372 adds `get_feature_names` to `PolynomialFeatures`. It accepts a list of names of `input_features` (or substitutes with defaults) and constructs feature name strings that are human-readable and informative. Similar support should be available for other transformers, including feature selectors, feature agglomeration, `FunctionTransformer`, and perhaps even PCA (giving the top contributors to each component). `FeatureUnion` should be modified to handle the case where an argument is supplied. A proposal for support in `Pipeline` is given in #6424.

Modelled on #6372, each enhancement can be contributed as a separate PR. Note that default names for features are `[x0, x1, ...]` 
- [x] `PolynomialFeatures` @amueller #6372
- [ ] feature selection and randomized L1 @yenchenlin1994 
- [ ] feature agglomeration @yenchenlin1994 
- [x] `FunctionTransformer` @nelson-liu #6431
- [x] scalers, normalizers and imputers: should be trivial like `FunctionTransformer` #6431
- [x] `Binarizer` #6431
- [x] `OneHotEncoder` #6441
- [ ] `FeatureUnion` @yenchenlin1994 
- [ ] PCA?
- [ ] `SparseRandomProjection`?
- [ ] `GaussianRandomProjection`??
- other transformers?
@jnothman May I try this?
On which (family of) estimator?
 Hi @jnothman, I am interested in taking this issue. Could you please suggest how I can get started on this issue?
I'll handle implementing this for `FunctionTransformer` for now, and we'll see if there's more classes to implement this in after I'm done :)
@jnothman I'll modify the `FeatureUnion`.

> including feature selectors, feature agglomeration, FunctionTransformer, and perhaps even PCA

Is feature agglomeration here refering to `cluster.FeatureAgglomeration`?
@yenchenlin1994 I assume so?
@nelson-liu Thx!

If so, I would also love to implement it for `cluster.FeatureAgglomeration`.
I have added an extended list of transformers where this may apply and noted the default feature naming convention (though maybe its generation belongs in `utils`)
Hello @jnothman ,

What should `preprocessing.Normalizer` do when input_features passed into `get_feature_names` is None? 

`PolynomialFeatures` doesn't suffer from this since it set both `self.n_input_features_` and `self.n_output_features_` during `fit()`.

Maybe `preprocessing.Normalizer` should set `self.n_input_features_` too during `fit()`?
Fair question, which I don't currently have an answer for. One option is for it to just return `feature_names` even if that means returning `None`.
Oh and even if `input_features`  passed into `get_feature_names` of `preprocessing.Normalizer` is not None,
I guess what it can do is to return `feature_names`, which is the same with `input_features` in this case?
yes, trivial, as noted in the issue description

On 24 February 2016 at 00:01, Yen notifications@github.com wrote:

> Oh and even if input_features passed into get_feature_names of
> preprocessing.Normalizer is not None,
> I guess what it can only do is to return feature_names, which is the same
> with input_features in this case?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/6425#issuecomment-187689663
> .
Oh okay!
I will also do scalars, normalizers and imputers and Binarizer.
Will send a PR right away.

Thanks for your clarification.
Hello @jnothman ,
about 

> feature selection and randomized L1

Do you mean all classes listed here:
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection

It seems that all these classes may be put into `Pipeline` and therefore need `get_feature_names` too.
Please correct me if I'm wrong. Thanks!
Yes, I mean those.

On 24 February 2016 at 17:44, Yen notifications@github.com wrote:

> Hello @jnothman https://github.com/jnothman ,
> about
> 
> feature selection and randomized L1
> 
> Do you mean all classes listed here:
> 
> http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection
> 
> It seems that all these classes may be put into Pipeline and therefore
> need get_feature_names too.
> Please correct me if I'm wrong. Thanks!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/6425#issuecomment-188105506
> .
Hi everyone, if it is fine I too would like to work on this issue. Would be helpful if the estimators which are currently worked on could be mentioned, so that I can try something which does not overlap. Thanks !
I think I can also work on 

> feature selection and randomized L1

@maniteja123 from `PCA` to the end of the issue description is not yet done
@yenchenlin1994, thanks for letting me know. 
@jnothman It would be of great help if you could confirm if the output for PCA needs to have shape `n_components` where each element is the input feature having the maximum contribution. Should the case of multiple features having high contribution along one component be handled ? Thank you !
I'm really not sure about PCA. Try make something useful. If you think it
will be helpful to users to have names for projection-style features,
submit a PR. There is definitely a component of art to this.

On 25 February 2016 at 01:12, Maniteja Nandana notifications@github.com
wrote:

> @jnothman https://github.com/jnothman It would be of great help if you
> could confirm if the output for PCA needs to have shape n_components
> where each element is the input feature having the maximum variance. Should
> the case of multiple features having high variance along one component be
> handled ? Thank you !
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/6425#issuecomment-188271445
> .
Thanks for the reply. My doubt is mainly about choosing dominant features and also that all the components are not equally significant. Since multiple features can have almost same contribution along a component, there might be need for some threshold to figure out the number of input features to be considered. Anyway I will create a initial PR with just the most dominant feature along the component and continue the discussion there. Hope it is fine.
@yenchenlin1994 one more question. I am not sure how to handle this for `SparseRandomProjection` and `GaussianRandomProjection`. Have you worked already on all of these ?

>  feature selection and randomized L1 
>  feature agglomeration
>  FeatureUnion 

If you have already started working, will be waiting for your PRs :) Thanks !
I've started, thanks!
for PCA I'm not sure if we want `pca0, pca1, ...` or if we want `0.1 * x1 + 0.7 * x2  - 1.0 * x3, ....` I can see both being useful, but the second one can become very verbose very quickly.
pca currently implements both and switches using a function parameter. However, that will be hard to do in a pipeline, right? Or do we have one argument for the whole pipeline and propagate it through?
Should we have this for all transformers in the decomposition module? It's kinda weird to have it for PCA but nothing else...
(I think this will be most helpful for feature extraction and feature selection, but possibly useful beyond)
> Should we have this for all transformers in the decomposition module? It's
> kinda weird to have it for PCA but nothing else...

I'd actually would like to put an embargo on any of the
"get_feature_names" that are poping up everywhere, and have a design
discussion (enhancement proposal), rather than fixing things here and
there.

I am having a hard time seeing the big picture across the entirety of
scikit-learn. For instance, last week, I was struggling with feature
names and the Imputer (which was removing some features due to too many
missing data). While these are not directly related problems, it would be
good to think about how we design a consistent API.

I think that the right way to tackle that is to list a set of example
problems, and possible solutions, and then break this into atomic tasks,
which all get a PR. WDYT?

I'm sorry, I didn't reply, because I felt that I needed to free time to
think about this to be able to have a constructive comment, but it didn't
happen, and I am drowning, so it won't happen before this week end.
We did discuss the design in another place before. I think it's pretty straight-forward for feature extraction and feature selection. I'm unsure about the rest, but don't think that's a blocker.
There was discussion here: #5172 there seems to be decent consensus.

I think the tasks are:
- add the functionality to all transformers
- add the functionality to pipelines

the questions are:
- what do transformers that create linear combinations do?
- what do transformers that create arbitrary functions (RBMs,  kernel pca, spectral embedding) do?
- how does that fit with the RFE model? For some reason, RFE, in contrast to other modle-based feature selection methods, does not have a transform, and assumes that the model that is used for feature selection is the same that is used for predicting. Maybe that's a design problem with RFE?

The open questions seems to be relatively unrelated to adding the functionality to the places where it's obvious what to do.

I think there is basically one use-case:
- having a pipeline with a model at the end, find out what the coefficients / feature importances mean.

example:
- `make_pipeline(CountVectorizer(), LogisticRegression())`
- `make_pipeline(DictVectorizer(), SelectFdr(), RandomForest)`
- `make_pipeline(OneHotEncoder(), NMF(), BernoulliNB())`

btw, I gave the use-cases already in #5172
actionable input: do the "obvious" `get_feature_names` ones, and think about the linear / non-linear features ones.
@GaelVaroquaux: Admittedly I was being a bit sneaky by putting together this issue. I suspected that embargo was inevitable, but wanted to see what contributors and review came up with rather than design without implementations to critique. I think we should avoid merging these PRs until we're ready, or perhaps merge them into a branch where we can continue to refine, but use the PRs to try to get a consensus on each family of transformers of what we think would be useful. With @amueller and @jakevdp starting the ball rolling, I was hoping to approximate agility with this approach! :P

Along those lines, and replying to @amueller's "Should we have this for all transformers in the decomposition module? It's kinda weird to have it for PCA but nothing else..." I suggested PCA as a testbed before someone got carried away with code duplication.

I think there is no doubt that in simple cases this is something that users have sought and will make model inspection somewhat more accessible. I might go as far as to say we need something like this. The details will be disputed, but I also think it's reasonable to allow `get_feature_names` to have parameters that tweak the output (e.g. for decomposition), and which can be underscore-prefixed in `Pipeline.get_feature_names` (`get_feature_names` is unlike `fit`, `predict`, `transform` in that it does not involve sample-correlated input, and has only parameters we don't care to set with  @model search).
the underscore-prefixed version of the parameters sounds verbose but sensible. Better than anything I could come up with.
So not merge #6372?
I think this is a something we urgently need.

It looks to me all linear decomposition methods can have a common `get_feature_names` if they all use `coef_` correctly.

The discussion has been open in #5172 since 9 month, with the only disagreement on whether the function should be public or private, as far as I can see. I realized public would indeed be better, so I went ahead ;)
Not sure you actually mean #5172 there...

By "urgently" you mean you need it merged before the book goes to press?? :p
Maybe ;) I tried to write a book chapter that explains feature extraction without it, but I was too embarrassed by the current interface of `PolynomialFeatures`. I actually use pandas for one-hot-encoding currently in the book because OneHotEncoder is soo bad (I put @vighneshbirodkar on a mission to fix that).

But also something I'm asked for basically every time I give a talk. The two most common questions are "why are categorical variables so hard" and "why is using column names so hard". Well maybe additionally "why is inspecting models so hard".

I meant these comments: https://github.com/scikit-learn/scikit-learn/pull/5172#issuecomment-135624231 https://github.com/scikit-learn/scikit-learn/pull/5172#issuecomment-135624952
Sorry for nitpicking but what is wrong with RFE? It has a transform that is inherited from `SelectorMixin` right?
@MechCoder you're right, I overlooked that.
@GaelVaroquaux did you have time to think about the issue?
I keep finding that I need something like this. I'm working with heterogeneous data like electronic medical records, with patient attributes, clinical notes, medical terminology entries... I need to be able to inspect a model consisting of nested pipelines and feature unions. I am sure that for other users, lacking feature name information leads to breaches of best practice: either giving up on model inspection, or not using pipelines and hence perhaps leaking data from test into train.

I very strongly believe we should go ahead and implement this, in lieu of a straightforward alternative. The main problem I see with it is the arbitrary string-based representation, and nesting of transformers annotating those string representations, and any backwards compatibility concerns with that.

@GaelVaroquaux, will you consent to merging the enhanced `get_feature_names` with an "Experimental: the API and output of this method may change in future versions" tag?(And if we really want to hedge our bets, we can make this contingent on the user importing `sklearn.experimental.feature_names`.)I'd also be most interested in @kmike's opinion on what better feature name support for pipelines should look like, whether aiming for perfection or aiming for agility.@kmike's comment at https://github.com/scikit-learn/scikit-learn/pull/6431/files#r88185069 might suggest that in practice an interface like `get_output_feature_names(indices, input_feature_names=None)` saves computation and memory costs:

```python
importances = pipeline.steps[-1][1].feature_importances_
truncated_pipeline = Pipeline(pipeline.steps[:-1])
top_features = np.argsort(importances)[-10:]
print(dict(zip(truncated_pipeline.get_output_feature_names(top_features,
                                                           X.columns.values),
               importances[top_features])))  # for dataframe X
```

Then a `FeatureUnion` can (with enough information stored) only query the necessary constituent transformers.

But I'm sure this involves more scaffolding work than `get_feature_names` as proposed here does.Hey,

Here are the bits of structured information we missed recently. Maybe they are a bit too specific, but anyways:

1. For CountVectorizer, HashingVectorizer and TfIdfVectorizer we needed (start, end) spans which map feature names back to the input text. It required quite a lot of copy-paste to implement: https://github.com/TeamHG-Memex/eli5/blob/master/eli5/sklearn/_span_analyzers.py, and then you need to pass this information using a side channel. This allows to implement highlighting like that: 
![default](https://cloud.githubusercontent.com/assets/107893/22055607/26ad81d6-dd86-11e6-9d98-a89acd9df9ca.png)

2. For FeatureHasher and HashingVectorizer there can be several "sub-feature names" for a single dimension - when recovering possible features from a corpus there can be collisions. Each "sub-feature name" also has a sign. For each dimension we're showing such "sub-feature names" sorted by their frequency in the corpus; if the top first "sub-feature name" is negative then the whole feature is treated as negative, and signs of all other "sub-feature names" are inverted. Collisions are also truncated for display (with an option to expand) if there is too may of them.
3. For FeatureUnion it'd be nice to map feature names back to transformers, i.e. to know which transformer is responsible for a feature; currently this is done with `.startswith()`. Ideally, it'd be nice to have the whole chain, along with meta-information, available in a structured form.
4. For pipelines it'd be nice to preserve meta-information about feature names, e.g. spans from (1) if CountVectorizer is followed by TfIdfTransformer.

All of the above is unconvenient and hacky to implement with feature names as strings.

Also, as @jnothman said, performance can be a problem sometimes - e.g. if you're building feature names for HashingVectorizer then the dimension is huge, and most feature names (but not all) are empty or auto-generated. Another use case is FeatureUnion of several transformers where some of them don't have get_feature_names defined; that's nice to still be able to inspect feature names which are defined, and have some kind of auto-generated names for undefined ones. This It is not a super-big deal, but it may cause several seconds delays in interactive usage; it could add up quickly if scikit-learn start to use auto-generated feature names everywhere.

That said, I see the appeal of plain string feature names; they are much easier to understand and implement.Somehow this turned into an eli5 wishlist to scikit-learn?

> For CountVectorizer, HashingVectorizer and TfIdfVectorizer we needed (start, end) spans

Hmm. Can we leave that out of the picture for now? Or do you just mean that this is part of a structured representation you would appreciate in eli5?

> Ideally, it'd be nice to have the whole chain, along with meta-information, available in a structured form.

I'm not sure what exactly you mean by this. I proposed an age ago having an attribute on a `FeatureUnion` to describe which output features come from which constituent transformer. It's easy to get that information when `fit_transform` is called, but not when `fit` is called without `transform`; hence its design is trickier than it looks.

> For pipelines it'd be nice to preserve meta-information about feature names, e.g. spans from (1) if CountVectorizer is followed by TfIdfTransformer.

So you mean a structured feature description? I can see the need for this, but designing it would be a big effort, and still best if it remains marked "experimental".

I would rather have something that provides the user with information for basic cases, but could be expanded later.> Somehow this turned into an eli5 wishlist to scikit-learn?

Heh, right. I don't have a good API feedback, so I just enumerated related problems we had.

> Hmm. Can we leave that out of the picture for now? Or do you just mean that this is part of a structured representation you would appreciate in eli5?

Sure, this is just an example of structured feature name representation. Actually it is no longer 'feature names' - that's some structured object which describes where the feature came from, like a link to the transformer in case of FeatureUnion. It could be a naming issue; structured representation could be irrelevant if we're talking only about feature names.

> I would rather have something that provides the user with information for basic cases, but could be expanded later.

All of this can be expanded later by adding new methods which return information e.g. in lists of the same length as `get_feature_names()`. So yeah, structured representation is not a blocker for get_feature_names improvements.For my work, I've created a module that monkey-patches scikit-learn transformers with a `transform_feature_names` method that can generate feature names for a pipeline/featureunion construction: https://gist.github.com/jnothman/bb1608e6ffea3109ff2ce7c926b0e0cbNever mind that; I should have used `singledispatch` there instead of monkey-patching and have updated it accordingly.

More importantly, I've submitted a patch to @eli5 which should handle explaining feature importances in pipelines (https://github.com/TeamHG-Memex/eli5/pull/158). I hope @eli5, particularly with its prolific use of singledispatch, is able to adopt this with greater agility than scikit-learn.A flaw with this design: I would like to build a transformer which selects (or excludes) features by name. It can be designed as a meta-transformer which gets the transformed feature names from the base transformer. However, doing this properly requires that the input feature names are known at fit time:

```python
class SelectByName(BaseEstimator, TransformerMixin):
    def __init__(self, transformer, names, exclude=False):
        self.transformer = self.transformer
        self.names = names
        self.exclude = exclude

    def fit(self, X, y=None, **kwargs):
        self.transformer_ = clone(self.transformer)
        self.transformer_.fit(X, y, **kwargs)
        # XXX: how do we get in_names here?
        feature_names = self.transformer_.transform_feature_names()
        self.support_mask_ = ...
```

For this application, it is necessary to pass the feature names alongside `X`.> I would like to build a transformer which selects (or excludes) features by name. 

Can you get rid of that with a ``ColumnTransformer``? I guess the question is a bit whether it's always possible to have the ``ColumnTransformer`` be right at the beginning of the pipeline, where we still know the names / positions of the columns.@kmike Are the structured annotations mostly needed because of the ranges?

Also, @GaelVaroquaux any more opinions on this? I doing the easy cases like feature selection and imputation (which might drop columns), in addition to having some support in FeatureUnion and Pipeline (and ColumnTransformer) will be very useful.@amueller once feature names get more complex (e.g. pca on top of tf*idf), showing them as a text  gets more and more opinionated, and maybe problem-specific. How concise should be a feature name, e.g. should we show only top PCA components (how many?), or all of them? Note the amount of bikeshedding @jnothman got from me at https://github.com/TeamHG-Memex/eli5/pull/208. 

It seems the root of the problem is that formatting a feature name is not the same as figuring out where feature comes from. 

This is where structured representation helps. Full information - all PCA components, or (start, end) ranges in case of text vectorizers - can be excessive for a default feature name, but it allows richer display: highlighting features in text, showing the rest of the components on mouse hover / click. @kmike thanks for the explanation :) Maybe doing strings first would still work. For PCA I would just basically do ``pca1``, ``pca2`` etc for now (aka punt)@jnothman @GaelVaroquaux should we include this in the "Townhall" meeting?feature names / DataFrames come together to some extent, so yes.
> @jnothman @GaelVaroquaux should we include this in the "Townhall" meeting?

Yes
Any news ?@kiros32 #13307 i think `OrdinalEncoder` can be added to this list